@inproceedings{gebeleFaceValueImpact2022,
	title = {Face {{Value}}: {{On}} the {{Impact}} of {{Annotation}} ({{In-}}){{Consistencies}} and {{Label Ambiguity}} in {{Facial Data}} on {{Emotion Recognition}}},
	shorttitle = {Face {{Value}}},
	booktitle = {2022 26th {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
	author = {Gebele, Jens and Brune, Philipp and Faußer, Stefan},
	date = {2022-08},
	pages = {2597--2604},
	issn = {2831-7475},
	doi = {10.1109/ICPR56361.2022.9956230},
	abstract = {Artificial Intelligence (AI)-based emotion recognition using various kinds of data has attracted vast attention in recent years. Impressive results have been achieved, but only recently the influence of the training data with its potential biases and variations in annotation quality are discussed. Still, the majority of the research literature focuses on improving machine learning techniques and model performance using single data sets. Literature on the impact of training data remains scarce. Therefore, in this paper we investigate the influence of the training data on the accuracy of recognizing emotional states in facial expressions by a comparative evaluation, using multiple established facial image databases. Results reveal inconsistencies in the data annotations as well as ambiguities in the emotional states expressed. Thus, they allow to critically discuss data quality of the training data, contributing to a more in-depth understanding of previous emotion recognition approaches, and improving the design of more transparent AI solutions.},
	eventtitle = {2022 26th {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
	keywords = {Annotations,Data integrity,Data Quality,Emotion recognition,Emotion Recognition,Emotional Artificial Intelligence,Face recognition,Facial Expression Recognition,Image databases,Image recognition,Image resolution},
	file = {C\:\\Users\\dreher\\LRZ Sync+Share\\Zotero\\Gebele et al_2022_Face Value.pdf;C\:\\Users\\dreher\\Zotero\\storage\\29UIXDCC\\9956230.html}
}

@inproceedings{abadiDeepLearningDifferential2016,
  title = {Deep Learning with Differential Privacy},
  booktitle = {Proceedings of the 2016 {{ACM SIGSAC}} Conference on Computer and Communications Security},
  author = {Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H. Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
  year = {2016},
  pages = {308--318},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\STVMVUYU\\Abadi et al. - 2016 - Deep learning with differential privacy.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\793H9G4K\\2976749.html}
}

@misc{abbeLearningReasonNeural2022,
  title = {Learning to {{Reason}} with {{Neural Networks}}: {{Generalization}}, {{Unseen Data}} and {{Boolean Measures}}},
  shorttitle = {Learning to {{Reason}} with {{Neural Networks}}},
  author = {Abbe, Emmanuel and Bengio, Samy and Cornacchia, Elisabetta and Kleinberg, Jon and Lotfi, Aryo and Raghu, Maithra and Zhang, Chiyuan},
  year = {2022},
  month = may,
  number = {arXiv:2205.13647},
  eprint = {2205.13647},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  institution = {{arXiv}},
  abstract = {This paper considers the Pointer Value Retrieval (PVR) benchmark introduced in [ZRKB21], where a 'reasoning' function acts on a string of digits to produce the label. More generally, the paper considers the learning of logical functions with gradient descent (GD) on neural networks. It is first shown that in order to learn logical functions with gradient descent on symmetric neural networks, the generalization error can be lower-bounded in terms of the noise-stability of the target function, supporting a conjecture made in [ZRKB21]. It is then shown that in the distribution shift setting, when the data withholding corresponds to freezing a single feature (referred to as canonical holdout), the generalization error of gradient descent admits a tight characterization in terms of the Boolean influence for several relevant architectures. This is shown on linear models and supported experimentally on other models such as MLPs and Transformers. In particular, this puts forward the hypothesis that for such architectures and for learning logical functions such as PVR functions, GD tends to have an implicit bias towards low-degree representations, which in turn gives the Boolean influence for the generalization error under quadratic loss.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: 28 pages, 8 figures},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\Y6B8UGUG\\Abbe et al. - 2022 - Learning to Reason with Neural Networks Generaliz.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\5SYSQ6DV\\2205.html}
}

@article{abdelhamidRobustSpeechEmotion2022,
  title = {Robust {{Speech Emotion Recognition Using CNN}}+{{LSTM Based}} on {{Stochastic Fractal Search Optimization Algorithm}}},
  author = {Abdelhamid, Abdelaziz A. and {El-Kenawy}, El-Sayed M. and Alotaibi, Bandar and Amer, Ghada M. and Abdelkader, Mahmoud Y. and Ibrahim, Abdelhameed and Eid, Marwa Metwally},
  year = {2022},
  journal = {IEEE Access},
  volume = {10},
  pages = {49265--49284},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2022.3172954},
  abstract = {One of the main challenges facing the current approaches of speech emotion recognition is the lack of a dataset large enough to train the currently available deep learning models properly. Therefore, this paper proposes a new data augmentation algorithm to enrich the speech emotions dataset with more sam Department, College of Computing and ples through a careful addition of noise fractions. In addition, the hyperparameters of the currently available deep learning models are either handcrafted or adjusted during the training process. However, this approach does not guarantee finding the best settings for these parameters. Therefore, we propose an optimized deep learning model in which the hyperparameters are optimized to find their best settings and thus achieve more recognition results. This deep learning model consists of a convolutional neural network (CNN) composed of four local feature-learning blocks and a long short-term memory (LSTM) layer for learning local and long-term correlations in the log Mel-spectrogram of the input speech samples. To improve the performance of this deep network, the learning rate and label smoothing regularization factor are optimized using the recently emerged stochastic fractal search (SFS)-guided whale optimization algorithm (WOA). The strength of this algorithm is the ability to balance between the exploration and exploitation of the search agents' positions to guarantee to reach the optimal global solution. To prove the effectiveness of the proposed approach, four speech emotion datasets, namely, IEMOCAP, Emo-DB, RAVDESS, and SAVEE, are incorporated in the conducted experiments. Experimental results confirmed the superiority of the proposed approach when compared with state-of-the-art approaches. Based on the four datasets, the achieved recognition accuracies are 98.13\%, 99.76\%, 99.47\%, and 99.50\%, respectively. Moreover, a statistical analysis of the achieved results is provided to emphasize the stability of the proposed approach.},
  keywords = {Convolutional neural networks,deep learning,Deep learning,Emotion recognition,Feature extraction,guided whale optimization algorithm,Optimization,Speech emotions,Speech recognition,stochastic fractal search optimization,Training},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\T6AMPF2V\\Abdelhamid et al. - 2022 - Robust Speech Emotion Recognition Using CNN+LSTM B.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\GFNV2955\\9770097.html}
}

@article{abdelnasserSemanticSLAMUsingEnvironment2015,
  title = {{{SemanticSLAM}}: {{Using}} Environment Landmarks for Unsupervised Indoor Localization},
  shorttitle = {{{SemanticSLAM}}},
  author = {Abdelnasser, Heba and Mohamed, Reham and Elgohary, Ahmed and Alzantot, Moustafa Farid and Wang, He and Sen, Souvik and Choudhury, Romit Roy and Youssef, Moustafa},
  year = {2015},
  journal = {IEEE Transactions on Mobile Computing},
  volume = {15},
  number = {7},
  pages = {1770--1782},
  publisher = {{IEEE}},
  keywords = {Acceleration,Elevators,indoor localization,Legged locomotion,Mobile computing,semantic SLAM,Simultaneous localization and mapping,Unconventional localization,unsupervised localization},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\QXLEKQST\\Abdelnasser et al. - 2015 - SemanticSLAM Using environment landmarks for unsu.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\4MSCK48U\\7265092.html;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\M9ZFFF8I\\7265092.html}
}

@article{adlerFernsehenDurchAugen2018,
  title = {{Fernsehen: durch die Augen direkt in den Bauch}},
  author = {Adler, Dorothea and Schwab, Frank},
  year = {2018},
  journal = {tv diskurs},
  pages = {32--37},
  langid = {german},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\NX9RZFC7\\Adler und Schwab - Fernsehen durch die Augen direkt in den Bauch.pdf}
}

@misc{AffectivaHumanizingTechnology2021,
  title = {Affectiva - {{Humanizing Technology}}},
  year = {2021},
  month = nov,
  journal = {Affectiva},
  abstract = {Affectiva's AI analyzes complex human states in context \textemdash{} emotions, cognitive states, activities and objects people use.},
  howpublished = {https://www.affectiva.com/},
  langid = {american},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\FWY8R7UI\\www.affectiva.com.html}
}

@article{agarwalEndtoEndNeuroSymbolicArchitecture2021,
  title = {End-to-{{End Neuro-Symbolic Architecture}} for {{Image-to-Image Reasoning Tasks}}},
  author = {Agarwal, Ananye and Shenoy, Pradeep and Mausam},
  year = {2021},
  month = jun,
  journal = {arXiv:2106.03121 [cs]},
  eprint = {2106.03121},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Neural models and symbolic algorithms have recently been combined for tasks requiring both perception and reasoning. Neural models ground perceptual input into a conceptual vocabulary, on which a classical reasoning algorithm is applied to generate output. A key limitation is that such neural-to-symbolic models can only be trained end-to-end for tasks where the output space is symbolic. In this paper, we study neural-symbolic-neural models for reasoning tasks that require a conversion from an image input (e.g., a partially filled sudoku) to an image output (e.g., the image of the completed sudoku). While designing such a three-step hybrid architecture may be straightforward, the key technical challenge is end-to-end training -- how to backpropagate without intermediate supervision through the symbolic component. We propose NSNnet, an architecture that combines an image reconstruction loss with a novel output encoder to generate a supervisory signal, develops update algorithms that leverage policy gradient methods for supervision, and optimizes loss using a novel subsampling heuristic. We experiment on problem settings where symbolic algorithms are easily specified: a visual maze solving task and a visual Sudoku solver where the supervision is in image form. Experiments show high accuracy with significantly less data compared to purely neural approaches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  note = {Very Important
\par
Image-to-Image Reasoning},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\4AD83IBF\\Agarwal et al. - 2021 - End-to-End Neuro-Symbolic Architecture for Image-t.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\ZC5DN9GW\\2106.html}
}

@inproceedings{ahmadCurrentTechnologiesLocation2017,
  title = {Current Technologies and Location Based Services},
  booktitle = {2017 {{Internet Technologies}} and {{Applications}} ({{ITA}})},
  author = {Ahmad, I. and Asif, R. and {Abd-Alhameed}, R.A. and Alhassan, H. and Elmegri, F. and Noras, J.M. and See, C H and Obidat, H. and Shuaieb, W. and Riberio, J.C. and Hameed, K. and Alabdullah, A. and Ali, A. and Melha, M Bin and Khambashi, M. Al and Migdadi, H. and Child, M.B. and {Al-Sadoon}, M.A.G. and Abdussalam, F.M. and Dunjuma, I.M. and Saleh, A. and Oguntala, G. and Eya, N. and Buhari, M. and Abduljabbar, N. and Sanousi, Geili T.A. El and Kosha, J. and Shepherd, S J and Mirza, A. and Jones, S.M.R. and Elfergani, I T E and Roderguez, J and Hussaini, A S and Qahwaji, R. and Excell, P S and McEwan, N J},
  year = {2017},
  month = sep,
  pages = {299--304},
  doi = {10.1109/ITECHA.2017.8101958},
  abstract = {This paper examines the benefits and drawbacks of these services, reviewing differences in infrastructure, power requirements, sensing devices, and other factors. Technologies covered include Radio Frequency Identification, GSM, GPS, A-GPS, Smart Antennas, Distributed Antenna Systems, Localization by Cell-ID, Localization by Prediction (Dead Reckoning method), Angle of Arrival (AOA), Localization by Finger Printing, Localization by Time of Arrival (TOA), Localization by Observed Time Difference of Arrival (TDOA), and Hybrid Localization-based AOA-TOA.},
  keywords = {Advantages of LBS,Benefits of LBS,Computer architecture,Disadvantages of LBS,Drawbacks of LBS,Global Positioning System,GPS,GSM,Localization Based Services LBS,Mobile communication,Radiofrequency identification,RFID Systems,Satellite broadcasting,Servers},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\NZ5T5Y75\\Ahmad et al. - 2017 - Current technologies and location based services.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\5H52E5Y2\\8101958.html}
}

@article{akbariNeuroSymbolicRepresentationsVideo2020,
  title = {Neuro-{{Symbolic Representations}} for {{Video Captioning}}: {{A Case}} for {{Leveraging Inductive Biases}} for {{Vision}} and {{Language}}},
  shorttitle = {Neuro-{{Symbolic Representations}} for {{Video Captioning}}},
  author = {Akbari, Hassan and Palangi, Hamid and Yang, Jianwei and Rao, Sudha and Celikyilmaz, Asli and Fernandez, Roland and Smolensky, Paul and Gao, Jianfeng and Chang, Shih-Fu},
  year = {2020},
  month = nov,
  journal = {arXiv:2011.09530 [cs, eess]},
  eprint = {2011.09530},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {Neuro-symbolic representations have proved effective in learning structure information in vision and language. In this paper, we propose a new model architecture for learning multi-modal neuro-symbolic representations for video captioning. Our approach uses a dictionary learning-based method of learning relations between videos and their paired text descriptions. We refer to these relations as relative roles and leverage them to make each token role-aware using attention. This results in a more structured and interpretable architecture that incorporates modality-specific inductive biases for the captioning task. Intuitively, the model is able to learn spatial, temporal, and cross-modal relations in a given pair of video and text. The disentanglement achieved by our proposal gives the model more capacity to capture multi-modal structures which result in captions with higher quality for videos. Our experiments on two established video captioning datasets verifies the effectiveness of the proposed approach based on automatic metrics. We further conduct a human evaluation to measure the grounding and relevance of the generated captions and observe consistent improvement for the proposed model. The codes and trained models can be found at https://github.com/hassanhub/R3Transformer},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  note = {Less Important},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\G3HNGS9R\\Akbari et al. - 2020 - Neuro-Symbolic Representations for Video Captionin.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\HYSJQFTG\\2011.html}
}

@inproceedings{alexeevskayaRecognizingHumanEmotions2022,
  title = {Recognizing {{Human Emotions Using}} a {{Convolutional Neural Network}}},
  booktitle = {2022 4th {{International Youth Conference}} on {{Radio Electronics}}, {{Electrical}} and {{Power Engineering}} ({{REEPE}})},
  author = {Alexeevskaya, Yulia A. and Skvortsova, Maria and Alexeevsky, Roman A.},
  year = {2022},
  month = mar,
  pages = {1--6},
  doi = {10.1109/REEPE53907.2022.9731391},
  abstract = {Improvement of developments in recognizing human emotions is one of the tasks in the field of Internet behavior. Such developments can be used to determine a person's mood, workplace fatigue, and many other factors. This paper aims to study the performance of the convolutional neural network (CNN) in human emotion recognition and define the quality of recognition in relation to similar studies. In the theoretical part of the study, was analyzed the basic algorithms used in determining the emotions on the face of a person, as well as compare approaches to the application of different methods that could be applied to the data set used in solving the problem, where subsequently proposed options for improving accuracy. Algorithms and functions such as convolution, pooling, activation, batch normalization, and optimization are taken as methods of research. The results show that the implemented model has an average accuracy relative to similar models.},
  keywords = {artificial intelligence,convolutional neural network (CNN),Data models,deep learning,Emotion recognition,Face recognition,human emotion recognition,Internet,machine learning,Neural networks,Power engineering,Training},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\VRV8BHPQ\\Alexeevskaya et al. - 2022 - Recognizing Human Emotions Using a Convolutional N.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\WISZM4CJ\\9731391.html}
}

@article{almasawaSurveyDeepLearningBased2019,
  title = {A {{Survey}} on {{Deep Learning-Based Person Re-Identification Systems}}},
  author = {Almasawa, Muna O. and Elrefaei, Lamiaa A. and Moria, Kawthar},
  year = {2019},
  journal = {IEEE Access},
  volume = {7},
  pages = {175228--175247},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2957336},
  abstract = {Person re-identification systems (person Re-ID) have recently gained more attention between computer vision researchers. They are playing a key role in intelligent visual surveillance systems and have widespread applications like applications for public security. The person Re-ID systems can identify if a person has been seen by a non-overlapping camera over large camera network in an unconstrained environment. It is a challenging issue since a person appears differently under different camera views and faces many challenges such as pose variation, occlusion and illumination changes. Many methods had been introduced for generating handcrafted features aimed to handle the person Re-ID problem. In recent years, many studies have started to apply deep learning methods to enhance the person Re-ID performance due the deep learning yielded significant results in computer vision issues. Therefore, this paper is a survey of the recent studies that proposed to improve the person Re-ID systems using deep learning. The public datasets that are used for evaluating these systems are discussed. Finally, the paper addresses future directions and current issues that must be considered toward improving the person Re-ID systems.},
  keywords = {Cameras,Deep learning,Feature extraction,Machine learning,Measurement,person re-identification,Probes,Robustness,Task analysis,video surveillance},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\NMDBD6V4\\Almasawa et al. - 2019 - A Survey on Deep Learning-Based Person Re-Identifi.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\TC8C7WFJ\\8920082.html}
}

@article{almazanReidDoneRight2018,
  title = {Re-Id Done Right: Towards Good Practices for Person Re-Identification},
  shorttitle = {Re-Id Done Right},
  author = {Almazan, Jon and Gajic, Bojana and Murray, Naila and Larlus, Diane},
  year = {2018},
  journal = {arXiv preprint arXiv:1801.05339},
  eprint = {1801.05339},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\XQHPFFCV\\Almazan et al. - 2018 - Re-id done right towards good practices for perso.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\M75HESK6\\1801.html}
}

@article{alsadikSimultaneousLocalizationMapping2021,
  title = {The {{Simultaneous Localization}} and {{Mapping}} ({{SLAM}})-{{An Overview}}},
  author = {Alsadik, Bashar and Karam, Samer},
  year = {2021},
  month = nov,
  journal = {Journal of Applied Science and Technology Trends},
  volume = {2},
  number = {04},
  pages = {120--131},
  issn = {2708-0757},
  doi = {10.38094/sgej1027},
  abstract = {Positioning is a need for many applications related to mapping and navigation either in civilian or military domains. The significant developments in satellite-based techniques, sensors, telecommunications, computer hardware and software, image processing, etc. positively influenced to solve the positioning problem efficiently and instantaneously. Accordingly, the mentioned development empowered the applications and advancement of autonomous navigation. One of the most interesting developed positioning techniques is what is called in robotics as the Simultaneous Localization and Mapping SLAM. The SLAM problem solution has witnessed a quick improvement in the last decades either using active sensors like the RAdio Detection And Ranging (Radar) and Light Detection and Ranging (LiDAR) or passive sensors like cameras. Definitely, positioning and mapping is one of the main tasks for Geomatics engineers, and therefore it's of high importance for them to understand the SLAM topic which is not easy because of the huge documentation and algorithms available and the various SLAM solutions in terms of the mathematical models, complexity, the sensors used, and the type of applications. In this paper, a clear and simplified explanation is introduced about SLAM from a Geomatical viewpoint avoiding going into the complicated algorithmic details behind the presented techniques. In this way, a general overview of SLAM is presented showing the relationship between its different components and stages like the core part of the front-end and back-end and their relation to the SLAM paradigm. Furthermore, we explain the major mathematical techniques of filtering and pose graph optimization either using visual or LiDAR SLAM and introduce a summary of the deep learning efficient contribution to the SLAM problem. Finally, we address examples of some existing practical applications of SLAM in our reality.},
  langid = {english},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\ANDS443I\\Alsadik und Karam - 2021 - The Simultaneous Localization and Mapping (SLAM)-A.pdf}
}

@article{alvarez-patoMultisensorDataFusion2020,
  title = {A Multisensor Data Fusion Approach for Predicting Consumer Acceptance of Food Products},
  author = {{\'A}lvarez-Pato, V{\'i}ctor M. and S{\'a}nchez, Claudia N. and {Dom{\'i}nguez-Soberanes}, Julieta and {M{\'e}ndoza-P{\'e}rez}, David E. and Vel{\'a}zquez, Ramiro},
  year = {2020},
  journal = {Foods},
  volume = {9},
  number = {6},
  pages = {774},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\RRNMZZSL\\foods-09-00774-v2.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\AW3BADBA\\htm.html;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\WGK68JY9\\774.html}
}

@inproceedings{alvarezMethodFacialEmotion2018,
  title = {A {{Method}} for {{Facial Emotion Recognition Based}} on {{Interest Points}}},
  booktitle = {2018 {{International Conference}} on {{Research}} in {{Intelligent}} and {{Computing}} in {{Engineering}} ({{RICE}})},
  author = {Alvarez, Victor M. and Vel{\'a}zquez, Ramiro and Gutierrez, Sebasti{\'a}n and {Enriquez-Zarate}, Josu{\'e}},
  year = {2018},
  month = aug,
  pages = {1--4},
  doi = {10.1109/RICE.2018.8509055},
  abstract = {Current facial recognition techniques allow to automatically determine human emotions through a digital image of the face. The present study employs interest points as landmarks in facial images affected by some emotion and compares their positions with those of a neutral expression. We seek to establish a relationship between the obtained results and the ones proposed by Paul Ekman's FACS (Facial Action Coding System) tool to determine the viability of an emotion recognition algorithm as well as some possible guidelines for its implementation.},
  keywords = {Conferences,Emotion recognition,Encoding,Eyebrows,Face,Face recognition,facial action coding system (F A CS),facial analysis,interest points,Muscles},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\FUUTXXWQ\\Alvarez et al. - 2018 - A Method for Facial Emotion Recognition Based on I.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\VQBVVDUD\\8509055.html}
}

@misc{amirVerifyingLearningBasedRobotic2022,
  title = {Verifying {{Learning-Based Robotic Navigation Systems}}},
  author = {Amir, Guy and Corsi, Davide and Yerushalmi, Raz and Marzari, Luca and Harel, David and Farinelli, Alessandro and Katz, Guy},
  year = {2022},
  month = may,
  number = {arXiv:2205.13536},
  eprint = {2205.13536},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  institution = {{arXiv}},
  abstract = {Deep reinforcement learning (DRL) has become a dominant deep-learning paradigm for various tasks in which complex policies are learned within reactive systems. In parallel, there has recently been significant research on verifying deep neural networks. However, to date, there has been little work demonstrating the use of modern verification tools on real, DRL-controlled systems. In this case-study paper, we attempt to begin bridging this gap, and focus on the important task of mapless robotic navigation -- a classic robotics problem, in which a robot, usually controlled by a DRL agent, needs to efficiently and safely navigate through an unknown arena towards a desired target. We demonstrate how modern verification engines can be used for effective model selection, i.e., the process of selecting the best available policy for the robot in question from a pool of candidate policies. Specifically, we use verification to detect and rule out policies that may demonstrate suboptimal behavior, such as collisions and infinite loops. We also apply verification to identify models with overly conservative behavior, thus allowing users to choose superior policies that are better at finding an optimal, shorter path to a target. To validate our work, we conducted extensive experiments on an actual robot, and confirmed that the suboptimal policies detected by our method were indeed flawed. We also compared our verification-driven approach to state-of-the-art gradient attacks, and our results demonstrate that gradient-based methods are inadequate in this setting. Our work is the first to demonstrate the use of DNN verification backends for recognizing suboptimal DRL policies in real-world robots, and for filtering out unwanted policies. We believe that the methods presented in this work can be applied to a large range of application domains that incorporate deep-learning-based agents.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics,Mathematics - Optimization and Control},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\LLNR22JE\\Amir et al. - 2022 - Verifying Learning-Based Robotic Navigation System.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\NMQXY9SF\\2205.html}
}

@misc{AnzahlPatentanmeldungenDeutschland,
  title = {{Anzahl der Patentanmeldungen in Deutschland nach Unternehmen 2020}},
  journal = {Statista},
  abstract = {Die Statistik zeigt die wichtigsten Unternehmen nach Anzahl der eingereichten Patentanmeldungen beim Deutschen Patent- und Markenamt im Jahr 2020.},
  howpublished = {https://de-statista-com.ezproxy.hs-neu-ulm.de/statistik/daten/studie/258128/umfrage/anzahl-der-patentanmeldungen-in-deutschland-nach-unternehmen/},
  langid = {german},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\QQB2Y6X7\\anzahl-der-patentanmeldungen-in-deutschland-nach-unternehmen.html}
}

@article{apolloniGeneralFrameworkLearning2004,
  title = {A General Framework for Learning Rules from Data},
  author = {Apolloni, B. and Esposito, A. and Malchiodi, D. and Orovas, C. and Palmas, G. and Taylor, J.G.},
  year = {2004},
  month = nov,
  journal = {IEEE Transactions on Neural Networks},
  volume = {15},
  number = {6},
  pages = {1333--1349},
  issn = {1941-0093},
  doi = {10.1109/TNN.2004.836249},
  abstract = {With the aim of getting understandable symbolic rules to explain a given phenomenon, we split the task of learning these rules from sensory data in two phases: a multilayer perceptron maps features into propositional variables and a set of subsequent layers operated by a PAC-like algorithm learns Boolean expressions on these variables. The special features of this procedure are that: i) the neural network is trained to produce a Boolean output having the principal task of discriminating between classes of inputs; ii) the symbolic part is directed to compute rules within a family that is not known a priori; iii) the welding point between the two learning systems is represented by a feedback based on a suitability evaluation of the computed rules. The procedure we propose is based on a computational learning paradigm set up recently in some papers in the fields of theoretical computer science, artificial intelligence and cognitive systems. The present article focuses on information management aspects of the procedure. We deal with the lack of prior information about the rules through learning strategies that affect both the meaning of the variables and the description length of the rules into which they combine. The paper uses the task of learning to formally discriminate among several emotional states as both a working example and a test bench for a comparison with previous symbolic and subsymbolic methods in the field.},
  keywords = {Artificial intelligence,Computer networks,Computer science,Edge pulling functions,fuzzy relaxations,hybrid systems,Information management,learning fitness,Learning systems,Multilayer perceptrons,Neural networks,Neurofeedback,Ockham razor,Output feedback,PAC-learning,symbolic feedbacks,understandable learning,Welding},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\I2K25PXG\\Apolloni et al. - 2004 - A general framework for learning rules from data.pdf}
}

@inproceedings{apolloniGeneralFrameworkSymbol2000,
  title = {A General Framework for Symbol and Rule Extraction in Neural Networks},
  booktitle = {Proceedings of the {{IEEE-INNS-ENNS International Joint Conference}} on {{Neural Networks}}. {{IJCNN}} 2000. {{Neural Computing}}: {{New Challenges}} and {{Perspectives}} for the {{New Millennium}}},
  author = {Apolloni, B. and Orovas, C. and Taylor, J. and Fellenz, W. and Gielen, S. and Westerdijk, M.},
  year = {2000},
  month = jul,
  volume = {2},
  pages = {87-92 vol.2},
  issn = {1098-7576},
  doi = {10.1109/IJCNN.2000.857879},
  abstract = {We split the rule extraction task into a subsymbolic and a symbolic phase and present a set of neural networks for filling the former. Under the two general commitments of: (i) having a learning algorithm that is sensitive to feedback signals coming from the latter phase, and (ii) extracting Boolean variables whose meaning is determined by the further symbolic processing, we introduce three unsupervised learning algorithms and show related numerical examples for a multilayer perceptron, recurrent neural networks, and a specially devised vector quantizer.},
  keywords = {Artificial intelligence,Artificial neural networks,Biological neural networks,Computer science,Data mining,Educational institutions,Filling,Intelligent networks,Mathematics,Neural networks},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\RHNVQ328\\Apolloni et al. - 2000 - A general framework for symbol and rule extraction.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\MKAYP4I3\\857879.html}
}

@inproceedings{ariastapiaContributionMethodAutomatic2014,
  title = {A Contribution to the Method of Automatic Identification of Human Emotions by Using Semantic Structures},
  booktitle = {2014 {{International Conference}} on {{Interactive Collaborative Learning}} ({{ICL}})},
  author = {Arias Tapia, Susana Alexandra and G{\'o}mez, A H{\'e}ctor F and Corbacho, Jos{\'e} Barbosa and Ratt{\'e}, Sylvie and {Torres-Diaz}, Juan and {Torres-Carrion}, Pablo Vicente and Garc{\'i}a, Juan Manuel},
  year = {2014},
  month = dec,
  pages = {60--70},
  doi = {10.1109/ICL.2014.7017748},
  abstract = {The study of human emotions has been significantly important in recent years, mainly due to its incidence in human behavior. Additionally, having semantic tools that infer emotions from multisensory sources is a crucial aspect, especially because the feelings or actions of a person might be identified through these semantic tools. In the present research, a methodology that uses semantic structures is proposed in order to identify complex emotions on the basis of simple emotions. For this purpose, the SHEO ontology was used. This ontology is designed to conceptualize simple emotions, combine them, and work with axioms and rules that infer complex emotions. SHEO takes simple emotions as instances. These emotions can be identified using computer algorithms. This is demonstrated in the testing phase in which the authors of this research designed the software called DetectionEmotion, which is used to identify simple emotions in video and text. The result of the authors' proposal proved the easiness to infer complex emotions by using SHEO. SHEO is not a final solution in this research, but rather a contribution to the semantic management of emotions.},
  keywords = {Complex Emotions,Context,Emotion recognition,Hidden Markov models,Human Emotions,Ontologies,Proposals,Semantic Structures,Semantics,Software},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\TJW8RKAT\\Arias Tapia et al. - 2014 - A contribution to the method of automatic identifi.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\5R6UHXM2\\7017748.html}
}

@article{aristodemouStateoftheartIntellectualProperty2018,
  title = {The State-of-the-Art on {{Intellectual Property Analytics}} ({{IPA}}): {{A}} Literature Review on Artificial Intelligence, Machine Learning and Deep Learning Methods for Analysing Intellectual Property ({{IP}}) Data},
  shorttitle = {The State-of-the-Art on {{Intellectual Property Analytics}} ({{IPA}})},
  author = {Aristodemou, Leonidas and Tietze, Frank},
  year = {2018},
  month = dec,
  journal = {World Patent Information},
  series = {Advanced {{Analytics}} of {{Intellectual Property Information}} for {{TechMining}}},
  volume = {55},
  pages = {37--51},
  issn = {0172-2190},
  doi = {10.1016/j.wpi.2018.07.002},
  abstract = {Big data is increasingly available in all areas of manufacturing and operations, which presents an opportunity for better decision making and discovery of the next generation of innovative technologies. Recently, there have been substantial developments in the field of patent analytics, which describes the science of analysing large amounts of patent information to discover trends. We define Intellectual Property Analytics (IPA) as the data science of analysing large amount of IP information, to discover relationships, trends and patterns for decision making. In this paper, we contribute to the ongoing discussion on the use of intellectual property analytics methods, i.e artificial intelligence methods, machine learning and deep learning approaches, to analyse intellectual property data. This literature review follows a narrative approach with search strategy, where we present the state-of-the-art in intellectual property analytics by reviewing 57 recent articles. The bibliographic information of the articles are analysed, followed by a discussion of the articles divided in four main categories: knowledge management, technology management, economic value, and extraction and effective management of information. We hope research scholars and industrial users, may find this review helpful when searching for the latest research efforts pertaining to intellectual property analytics.},
  langid = {english},
  keywords = {Artificial intelligence,Deep learning,Intellectual property analytics,Machine learning,Patent analytics},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\RCH6V2KK\\Aristodemou und Tietze - 2018 - The state-of-the-art on Intellectual Property Anal.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\65MBU485\\S0172219018300103.html}
}

@inproceedings{aroraFacialEmotionalIdentification2022,
  title = {Facial and {{Emotional Identification}} Using {{Artificial Intelligence}}},
  booktitle = {2022 6th {{International Conference}} on {{Trends}} in {{Electronics}} and {{Informatics}} ({{ICOEI}})},
  author = {Arora, Himanshu and Kumar, Manish and Rasool, Tawsai and Panchal, Pooja},
  year = {2022},
  month = apr,
  pages = {1025--1030},
  doi = {10.1109/ICOEI53556.2022.9776862},
  abstract = {Facial identification has been an easy task for humans for a very long interval of time but facial expressions with emotions are quite challenging nowadays. With the great success of machine learning, researchers are working day and night to interpret facial expressions having emotions like sadness, anger, happiness, etc. Although, in the topical research on facial identification, there are numerous issues such as low lustiness and indigent abstraction of recognition system. This paper depicts how people use their facial emotions to describe their situation and at the same time use the same emotions that imply whether they are happy or sad, upset or normal, etc. that is quite challenging to predict one's mood as multiple emotions are taken into account for distinguished problems which have impacted the performance of many existing systems proposed so far. In addition, this paper also analyzesa few machine learning algorithms along with feature extraction techniques that would benefit us in the unerring identification of human emotion.},
  keywords = {Artificial Intelligence,Convolutional Neural Networks(CNN),Emotion recognition,Emotional Identification,Face recognition,Face Recognition,Facial Identification,Feature extraction,Machine learning,Machine learning algorithms,Market research,Mental Representation,Mood},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\SRUUS5PL\\Arora et al. - 2022 - Facial and Emotional Identification using Artifici.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\CH4NA2HT\\9776862.html}
}

@article{ayariHybridModelBasedEmotion2020,
  title = {Hybrid {{Model-Based Emotion Contextual Recognition}} for {{Cognitive Assistance Services}}},
  author = {Ayari, N. and Abdelkawy, H. and Chibani, A. and Amirat, Y.},
  year = {2020},
  journal = {IEEE Transactions on Cybernetics},
  pages = {1--10},
  issn = {2168-2275},
  doi = {10.1109/TCYB.2020.3013112},
  abstract = {Endowing ubiquitous robots with cognitive capabilities for recognizing emotions, sentiments, affects, and moods of humans in their context is an important challenge, which requires sophisticated and novel approaches of emotion recognition. Most studies explore data-driven pattern recognition techniques that are generally highly dependent on learning data and insufficiently effective for emotion contextual recognition. In this article, a hybrid model-based emotion contextual recognition approach for cognitive assistance services in ubiquitous environments is proposed. This model is based on: 1) a hybrid-level fusion exploiting a multilayer perceptron (MLP) neural-network model and the possibilistic logic and 2) an expressive emotional knowledge representation and reasoning model to recognize nondirectly observable emotions; this model exploits jointly the emotion upper ontology (EmUO) and the n-ary ontology of events HTemp supported by the NKRL language. For validation purposes of the proposed approach, experiments were carried out using a YouTube dataset, and in a real-world scenario dedicated to the cognitive assistance of visitors in a smart devices showroom. Results demonstrated that the proposed multimodal emotion recognition model outperforms all baseline models. The real-world scenario corroborates the effectiveness of the proposed approach in terms of emotion contextual recognition and management and in the creation of emotion-based assistance services.},
  keywords = {Cognition,Context modeling,Emotion recognition,Face recognition,Hidden Markov models,Intelligent assistance services,multimodal emotion/affect recognition,ontologies,Ontologies,Robots,symbolic modeling and reasoning,ubiquitous robotics},
  note = {hybrid model-based emotion contextual recognition approach, 
\par
(1) neural network including possibilistic logic (decision model under uncertainty), feature extraction, fusion neural network and possiblistic logic for decision making.
\par
(2) In combination with a semantic framework that makes use of a narrative knowledge representation language (NKRL)},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\GPHB3NPU\\Ayari et al. - 2020 - Hybrid Model-Based Emotion Contextual Recognition .pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\QIYC6EIJ\\9194302.html}
}

@article{bagozziRoleEmotionsMarketing1999,
  title = {The Role of Emotions in Marketing},
  author = {Bagozzi, Richard P. and Gopinath, Mahesh and Nyer, Prashanth U.},
  year = {1999},
  journal = {Journal of the Academy of Marketing Science},
  volume = {27},
  number = {2},
  pages = {184--206},
  issn = {0092-0703},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\XQKLTK4J\\The_Role_of_Emotions_in_Marketing.pdf}
}

@article{bailensonRealtimeClassificationEvoked2008,
  title = {Real-Time Classification of Evoked Emotions Using Facial Feature Tracking and Physiological Responses},
  author = {Bailenson, Jeremy N. and Pontikakis, Emmanuel D. and Mauss, Iris B. and Gross, James J. and Jabon, Maria E. and Hutcherson, Cendri AC and Nass, Clifford and John, Oliver},
  year = {2008},
  journal = {International journal of human-computer studies},
  volume = {66},
  number = {5},
  pages = {303--317},
  issn = {1071-5819},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\MC2KL6C2\\Real-time_Classification.pdf}
}

@article{balanEmotionClassificationBased2020,
  title = {Emotion {{Classification Based}} on {{Biophysical Signals}} and {{Machine Learning Techniques}}},
  author = {B{\u a}lan, Oana and Moise, Gabriela and Petrescu, Livia and Moldoveanu, Alin and Leordeanu, Marius and Moldoveanu, Florica},
  year = {2020},
  journal = {Symmetry},
  volume = {12},
  number = {1},
  pages = {21},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\3XZZDLB8\\Bălan-2020-Emotion Classification Based on Bio.pdf}
}

@inproceedings{balaRobustShapeAnalysis1992,
  title = {Robust Shape Analysis Using Multistrategy Learning},
  booktitle = {Proceedings., 11th {{IAPR International Conference}} on {{Pattern Recognition}}. {{Vol}}.{{II}}. {{Conference B}}: {{Pattern Recognition Methodology}} and {{Systems}}},
  author = {Bala, J. and Wechsler, H.},
  year = {1992},
  month = aug,
  pages = {162--165},
  doi = {10.1109/ICPR.1992.201745},
  abstract = {This paper describes how to integrate subsymbolic and symbolic processes in order to create high-performance shape analysis systems. The specific methodology introduced integrates morphological processing and machine learning techniques such as genetic algorithms (GAs) and empirical inductive generalization. The optimal operators (defined as variable morphological structuring elements) evolved by GAs are used to derive discriminant feature vectors, which are then used by empirical inductive learning to generate rule-based class description in disjunctive normal form. The rule-based descriptions are finally optimized by removing small disjuncts in order to enhance the robustness of the shape analysis system. Experimental results are presented to illustrate the feasibility of the methodology for discriminating among classes of arbitrarily shaped objects, for learning the concepts of convexity and concavity, and for building robust recognition methods.{$<>$}},
  keywords = {Computer science,Genetic algorithms,Image analysis,Information analysis,Machine learning,Pattern analysis,Pattern recognition,Performance analysis,Robustness,Shape},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\UZF6IY8C\\Bala und Wechsler - 1992 - Robust shape analysis using multistrategy learning.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\XT9ESSSM\\201745.html}
}

@misc{balestrieroContrastiveNonContrastiveSelfSupervised2022,
  title = {Contrastive and {{Non-Contrastive Self-Supervised Learning Recover Global}} and {{Local Spectral Embedding Methods}}},
  author = {Balestriero, Randall and LeCun, Yann},
  year = {2022},
  month = may,
  number = {arXiv:2205.11508},
  eprint = {2205.11508},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  institution = {{arXiv}},
  abstract = {Self-Supervised Learning (SSL) surmises that inputs and pairwise positive relationships are enough to learn meaningful representations. Although SSL has recently reached a milestone: outperforming supervised methods in many modalities\textbackslash dots the theoretical foundations are limited, method-specific, and fail to provide principled design guidelines to practitioners. In this paper, we propose a unifying framework under the helm of spectral manifold learning to address those limitations. Through the course of this study, we will rigorously demonstrate that VICReg, SimCLR, BarlowTwins et al. correspond to eponymous spectral methods such as Laplacian Eigenmaps, Multidimensional Scaling et al. This unification will then allow us to obtain (i) the closed-form optimal representation for each method, (ii) the closed-form optimal network parameters in the linear regime for each method, (iii) the impact of the pairwise relations used during training on each of those quantities and on downstream task performances, and most importantly, (iv) the first theoretical bridge between contrastive and non-contrastive methods towards global and local spectral embedding methods respectively, hinting at the benefits and limitations of each. For example, (i) if the pairwise relation is aligned with the downstream task, any SSL method can be employed successfully and will recover the supervised method, but in the low data regime, VICReg's invariance hyper-parameter should be high; (ii) if the pairwise relation is misaligned with the downstream task, VICReg with small invariance hyper-parameter should be preferred over SimCLR or BarlowTwins.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Mathematics - Spectral Theory,Statistics - Machine Learning},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\JKVT2AJ7\\Balestriero und LeCun - 2022 - Contrastive and Non-Contrastive Self-Supervised Le.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\83GKMAQY\\2205.html}
}

@inproceedings{banIndoorPositioningMethod2015,
  title = {Indoor Positioning Method Integrating Pedestrian {{Dead Reckoning}} with Magnetic Field and {{WiFi}} Fingerprints},
  booktitle = {2015 {{Eighth International Conference}} on {{Mobile Computing}} and {{Ubiquitous Networking}} ({{ICMU}})},
  author = {Ban, Ryoji and Kaji, Katsuhiko and Hiroi, Kei and Kawaguchi, Nobuo},
  year = {2015},
  month = jan,
  pages = {167--172},
  doi = {10.1109/ICMU.2015.7061061},
  abstract = {In this paper, we propose a high accuracy indoor positioning method that uses residual magnetism in addition to Pedestrian Dead Reckoning (PDR) and WiFi-based localization methods. Our proposed method needs WiFi and magnetic field fingerprints, which are created by measuring in advance the WiFi radio waves and the magnetic field in the target map. The fingerprints are represented by a Gaussian Mixture Models (GMMs) to reduce the amount of computation. Our proposed method estimates positions by comparing the pedestrian sensor and fingerprint values by particle filters. We evaluated this method in real environments and confirmed that it provides accurate indoor positioning with a mean error less than 8 m and more accurate position detection than existing techniques.},
  keywords = {Acceleration,Accuracy,Estimation,IEEE 802.11 Standards,Legged locomotion,Magnetic resonance imaging,Magnetic separation},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\X5HI6EDN\\Ban et al. - 2015 - Indoor positioning method integrating pedestrian D.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\DFGH9FRM\\7061061.html}
}

@misc{BarrettEmotionalExpressions,
  title = {Barrett: {{Emotional}} Expressions Reconsidered: {{Challenges}}... - {{Google Scholar}}},
  howpublished = {https://scholar.google.com/scholar\_lookup?title=\&journal=Psychol.\%20Sci.\%20Publ.\%20Interest\&volume=20\&pages=1-68\&publication\_year=2019\&author=Barrett\%2CL.\%20F.\&author=Adolphs\%2CR.\&author=Marsella\%2CS.\&author=Martinez\%2CA.\%20M.\&author=Pollak\%2CS.\%20D.},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\NEH3U9D2\\Barrett Emotional expressions reconsidered Chall.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\S6LQBBUP\\scholar_lookup.html}
}

@article{barrettEmotionalExpressionsReconsidered2019,
  title = {Emotional {{Expressions Reconsidered}}: {{Challenges}} to {{Inferring Emotion From Human Facial Movements}}:},
  shorttitle = {Emotional {{Expressions Reconsidered}}},
  author = {Barrett, Lisa Feldman and Adolphs, Ralph and Marsella, Stacy and Martinez, Aleix M. and Pollak, Seth D.},
  year = {2019},
  month = jul,
  journal = {Psychological Science in the Public Interest},
  publisher = {{SAGE PublicationsSage CA: Los Angeles, CA}},
  doi = {10.1177/1529100619832930},
  abstract = {It is commonly assumed that a person's emotional state can be readily inferred from his or her facial movements, typically called emotional expressions or facia...},
  copyright = {\textcopyright{} The Author(s) 2019},
  langid = {english},
  note = {Barrett and colleagues propose one can justify that a facial expression reveals something about a person's emotional state if four criteria are met:
\par
\begin{itemize}

\item reliability (e.g., a scowling face occurs whenever someone is angry), 

\item specificity (e.g., a scowling face rarely occurs when someone is not angry) 

\item generalizability (i.e., patterns of reliability and specificity must occur across different studies and for different populations), and 

\item validity (i.e., even when a facial expression is consistently associated with an emotion, it must be demonstrated that the person portraying the expression is really in the expected emotional state)

\end{itemize}},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\4M9MFTDP\\Barrett et al. - 2019 - Emotional Expressions Reconsidered Challenges to .pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\WJ8P5MU4\\full.html}
}

@article{bedagkar-galaSurveyApproachesTrends2014,
  title = {A Survey of Approaches and Trends in Person Re-Identification},
  author = {{Bedagkar-Gala}, Apurva and Shah, Shishir K.},
  year = {2014},
  journal = {Image and vision computing},
  volume = {32},
  number = {4},
  pages = {270--286},
  publisher = {{Elsevier}},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\D54TKDGT\\Bedagkar-Gala und Shah - 2014 - A survey of approaches and trends in person re-ide.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\MQHFULVN\\S0262885614000262.html}
}

@inproceedings{bekcanDesignImplementationVisual2021,
  title = {Design and {{Implementation}} of {{Visual Simultaneous Localization}} and {{Mapping}} ({{VSLAM}}) {{Navigation System}}},
  booktitle = {2021 29th {{Signal Processing}} and {{Communications Applications Conference}} ({{SIU}})},
  author = {Bekcan, Arda and Ergezer, Halit},
  year = {2021},
  month = jun,
  pages = {1--4},
  issn = {2165-0608},
  doi = {10.1109/SIU53274.2021.9477703},
  abstract = {It is very important to guess the location of the redetected objects and loop closures with the visual simultaneous localization and mapping system (VSLAM), one of the biggest problems of a mobile robot. VSLAM makes it possible to eliminate and/or reduce these applications' errors and realize or improve the robot's direction and position correctly by creating a map of the environment. This study aims to achieve an autonomous indoor/outdoor navigation of a ground robot using VSLAM algorithm in an unknown environment using a monocular camera. In this context, the theoretical information was tested in real-world conditions. Performance of localization and loop closing were compared based on the results obtained by experiments},
  keywords = {Computer Vision,Jacobian matrices,Localization,Mapping,Navigation,Presses,Robot sensing systems,Robots,Simultaneous localization and mapping,Visualization},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\P7K8R4NH\\Bekcan und Ergezer - 2021 - Design and Implementation of Visual Simultaneous L.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\5S3JIL8K\\9477703.html}
}

@article{bekingPrenatalPubertalTestosterone2018,
  title = {Prenatal and Pubertal Testosterone Affect Brain Lateralization},
  author = {Beking, Tess and Geuze, R. H. and Van Faassen, Martijn and Kema, I. P. and Kreukels, Baudewijntje PC and Groothuis, T. G. G.},
  year = {2018},
  journal = {Psychoneuroendocrinology},
  volume = {88},
  pages = {78--91},
  publisher = {{Elsevier}},
  note = {Belongs to KDEF-chimeric data set},
  file = {C\:\\Users\\gebele\\Desktop\\Beking et al., 2018.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\2JDARKDX\\S0306453017308739.html}
}

@inproceedings{bellaryHybridMachineLearning2010,
  title = {Hybrid {{Machine Learning Approach}} in {{Data Mining}}},
  booktitle = {2010 {{Second International Conference}} on {{Machine Learning}} and {{Computing}}},
  author = {Bellary, Jyothi and Peyakunta, Bhargavi and Konetigari, Sekhar},
  year = {2010},
  month = feb,
  pages = {305--308},
  doi = {10.1109/ICMLC.2010.57},
  abstract = {In this paper we discuss various machine learning approaches used in mining of data. Further we distinguish between symbolic and sub-symbolic data mining methods. We also attempt to propose a hybrid method with the combination of Artificial Neural Network (ANN) and Cased Based Reasoning (CBR) in mining of data.},
  keywords = {Artificial Neural Networ (ANN),Artificial neural networks,Case Based Reasoning( CBR),Classification tree analysis,Data analysis,Data mining,Data Mining,Data visualization,Databases,Hospitals,Hybrid Approach,Information systems,Learning systems,Machine learning,Machine Learning},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\355XCUFD\\Bellary et al. - 2010 - Hybrid Machine Learning Approach in Data Mining.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\ZXISYY6Q\\Bellary et al. - 2010 - Hybrid Machine Learning Approach in Data Mining.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\TA2AHMGR\\5460721.html}
}

@inproceedings{bellavista-parentNewTrendsIndoor2021,
  title = {New Trends in Indoor Positioning Based on {{WiFi}} and Machine Learning: {{A}} Systematic Review},
  shorttitle = {New Trends in Indoor Positioning Based on {{WiFi}} and Machine Learning},
  booktitle = {2021 {{International Conference}} on {{Indoor Positioning}} and {{Indoor Navigation}} ({{IPIN}})},
  author = {{Bellavista-Parent}, Vladimir and {Torres-Sospedra}, Joaqu{\'i}n and {Perez-Navarro}, Antoni},
  year = {2021},
  month = nov,
  eprint = {2107.14356},
  eprinttype = {arxiv},
  primaryclass = {eess},
  pages = {1--8},
  doi = {10.1109/IPIN51156.2021.9662521},
  abstract = {Currently there is no standard indoor positioning system, similar to outdoor GPS. However, WiFi signals have been used in a large number of proposals to achieve the above positioning, many of which use machine learning to do so. But what are the most commonly used techniques in machine learning? What accuracy do they achieve? Where have they been tested? This article presents a systematic review of works between 2019 and 2021 that use WiFi as the signal for positioning and machine learning models to estimate indoor position. 64 papers have been identified as relevant, which have been systematically analyzed for a better understanding of the current situation in different aspects. The results show that indoor positioning based on WiFi trends use neural network-based models, evaluated in empirical experiments. Despite this, many works still conduct an assessment in small areas, which can influence the goodness of the results presented.},
  archiveprefix = {arXiv},
  keywords = {Electrical Engineering and Systems Science - Signal Processing},
  note = {Comment: to appear in 2021 International Conference on Indoor Positioning and Indoor Navigation (IPIN), 29 Nov. - 2 Dec. 2021, Lloret de Mar, Spain},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\NSPAFQFP\\Bellavista-Parent et al. - 2021 - New trends in indoor positioning based on WiFi and.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\NV56W9TL\\2107.html}
}

@article{belleSymbolicLogicMeets2020,
  title = {Symbolic {{Logic}} Meets {{Machine Learning}}: {{A Brief Survey}} in {{Infinite Domains}}},
  shorttitle = {Symbolic {{Logic}} Meets {{Machine Learning}}},
  author = {Belle, Vaishak},
  year = {2020},
  month = jun,
  journal = {arXiv:2006.08480 [cs]},
  eprint = {2006.08480},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The tension between deduction and induction is perhaps the most fundamental issue in areas such as philosophy, cognition and artificial intelligence (AI). The deduction camp concerns itself with questions about the expressiveness of formal languages for capturing knowledge about the world, together with proof systems for reasoning from such knowledge bases. The learning camp attempts to generalize from examples about partial descriptions about the world. In AI, historically, these camps have loosely divided the development of the field, but advances in cross-over areas such as statistical relational learning, neuro-symbolic systems, and high-level control have illustrated that the dichotomy is not very constructive, and perhaps even ill-formed. In this article, we survey work that provides further evidence for the connections between logic and learning. Our narrative is structured in terms of three strands: logic versus learning, machine learning for logic, and logic for machine learning, but naturally, there is considerable overlap. We place an emphasis on the following "sore" point: there is a common misconception that logic is for discrete properties, whereas probability theory and machine learning, more generally, is for continuous properties. We report on results that challenge this view on the limitations of logic, and expose the role that logic can play for learning in infinite domains.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Logic in Computer Science,Computer Science - Machine Learning},
  note = {Important
\par
see in conclusion source [61] mentioned},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\WKHJVNZN\\Belle - 2020 - Symbolic Logic meets Machine Learning A Brief Sur.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\MQTA6D6R\\2006.html}
}

@article{belloLambdaNetworksModelingLongRange2021,
  title = {{{LambdaNetworks}}: {{Modeling Long-Range Interactions Without Attention}}},
  shorttitle = {{{LambdaNetworks}}},
  author = {Bello, Irwan},
  year = {2021},
  month = feb,
  journal = {arXiv:2102.08602 [cs]},
  eprint = {2102.08602},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We present lambda layers -- an alternative framework to self-attention -- for capturing long-range interactions between an input and structured contextual information (e.g. a pixel surrounded by other pixels). Lambda layers capture such interactions by transforming available contexts into linear functions, termed lambdas, and applying these linear functions to each input separately. Similar to linear attention, lambda layers bypass expensive attention maps, but in contrast, they model both content and position-based interactions which enables their application to large structured inputs such as images. The resulting neural network architectures, LambdaNetworks, significantly outperform their convolutional and attentional counterparts on ImageNet classification, COCO object detection and COCO instance segmentation, while being more computationally efficient. Additionally, we design LambdaResNets, a family of hybrid architectures across different scales, that considerably improves the speed-accuracy tradeoff of image classification models. LambdaResNets reach excellent accuracies on ImageNet while being 3.2 - 4.4x faster than the popular EfficientNets on modern machine learning accelerators. When training with an additional 130M pseudo-labeled images, LambdaResNets achieve up to a 9.5x speed-up over the corresponding EfficientNet checkpoints.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  note = {Comment: Accepted for publication at the International Conference in Learning Representations 2021 (Spotlight)},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\FHUWGZ5W\\Bello - 2021 - LambdaNetworks Modeling Long-Range Interactions W.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\SW7A3UFS\\2102.html}
}

@article{benitez-quirozFacialColorEfficient2018,
  title = {Facial Color Is an Efficient Mechanism to Visually Transmit Emotion},
  author = {{Benitez-Quiroz}, Carlos F. and Srinivasan, Ramprakash and Martinez, Aleix M.},
  year = {2018},
  month = apr,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {115},
  number = {14},
  pages = {3581--3586},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.1716084115},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\ALVV4PNB\\Benitez-Quiroz et al. - 2018 - Facial color is an efficient mechanism to visually.pdf}
}

@misc{BenitezQuirozFacialColor,
  title = {Benitez-{{Quiroz}}: {{Facial}} Color Is an Efficient Mechanism... - {{Google Scholar}}},
  howpublished = {https://scholar.google.com/scholar\_lookup?title=\&journal=Proc.\%20Natl\%20Acad.\%20Sci.\%20USA\&volume=115\&pages=3581-3586\&publication\_year=2018\&author=Benitez-Quiroz\%2CC.\%20F.\&author=Srinivasan\%2CR.\&author=Martinez\%2CA.\%20M.},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\DGK2A2PT\\Benitez-Quiroz Facial color is an efficient mecha.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\T46C69DJ\\scholar_lookup.html}
}

@article{berners-leeSemanticWeb2001,
  title = {The Semantic Web},
  author = {{Berners-Lee}, Tim and Hendler, James and Lassila, Ora},
  year = {2001},
  journal = {Scientific american},
  volume = {284},
  number = {5},
  pages = {34--43},
  publisher = {{JSTOR}},
  note = {Semantic Web},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\YKT6CWB5\\Berners-Lee et al. - 2001 - The semantic web.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\22FISL5H\\26059207.html}
}

@inproceedings{berthelonEmotionOntologyContext2013,
  title = {Emotion Ontology for Context Awareness},
  booktitle = {2013 {{IEEE}} 4th {{International Conference}} on {{Cognitive Infocommunications}} ({{CogInfoCom}})},
  author = {Berthelon, Franck and Sander, Peter},
  year = {2013},
  month = dec,
  pages = {59--64},
  doi = {10.1109/CogInfoCom.2013.6719313},
  abstract = {We present an emotion ontology for describing and reasoning on emotion context in order to improve emotion detection based on bodily expression. We incorporate context into the two-factor theory of emotion (bodily reaction plus cognitive input) and demonstrate the importance of context in the emotion experience. In attempting to determine emotion felt by another person, the bodily expresson of their emotion is the only evidence directly available, eg, ``John looks angry''. Our motivation in this paper is to bring context into the emotion-modulating cognitive input, eg, we know that John is a generally calm person, so we can conclude from expression (anger) plus context (calm) that John is not only angry, but that ``John must be furious''. We use a well known interoperable reasoning tool, an ontology, to bring context into the implementation of the emotion detection process. Our emotion ontology (EmOCA) allow us to describe and to reason about philia and phobia in order to modulate emotion determined from expression. We present an experiment suggesting that people use such a strategy to incorporate contextual information when determining what emotion another person may be feeling.},
  keywords = {Cognition,Context,Context modeling,Emotion recognition,Observers,Ontologies,Semantics},
  note = {Emotion ontology based on two-factor theory (expression + cognition), future research plan to extend with personality traits},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\PI22X9WS\\Berthelon und Sander - 2013 - Emotion ontology for context awareness.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\VYT9Y6QR\\6719313.html}
}

@inproceedings{blakowskiMayOntologiesBe2016,
  title = {May the {{Ontologies Be}} with {{You}}! {{Towards}} a User-Friendly Web-Based Editor for Semantic Web Service Description},
  booktitle = {2016 {{IEEE Symposium}} on {{Service-Oriented System Engineering}} ({{SOSE}})},
  author = {Blakowski, Steven and Brune, Philipp},
  year = {2016},
  pages = {203--210},
  publisher = {{IEEE}},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\JI2AZFA3\\7473027.html}
}

@article{brillMixedMethodsApproachUsing2019,
  title = {A {{Mixed-Methods Approach Using Self-Report}}, {{Observational Time Series Data}}, and {{Content Analysis}} for {{Process Analysis}} of a {{Media Reception Phenomenon}}},
  author = {Brill, Michael and Schwab, Frank},
  year = {2019},
  journal = {Frontiers in Psychology},
  volume = {10},
  issn = {1664-1078},
  abstract = {Due to the complexityof research objects, theoretical concepts, and stimuli in media research, researchers in psychology and communications presumably need sophisticated measures beyond self-report scales to answer research questions on media use processes. The present study evaluates stimulus-dependent structure in spontaneous eye-blink behavior as an objective, corroborative measure for the media use phenomenon of spatial presence. To this end, a mixed methods approach is used in an experimental setting to collect, combine, analyze, and interpret data from standardized participant self-report, observation of participant behavior, and content analysis of the media stimulus. T-pattern detection is used to analyze stimulus-dependent blinking behavior, and this structural data is then contrasted with self-report data. The combined results show that behavioral indicators yield the predicted results, while self-report data shows unpredicted results that are not predicted by the underlying theory. The use of a mixed methods approach offered insights that support further theory development and theory testing beyond a traditional, mono-method experimental approach.},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\XJ2BXSLN\\Brill und Schwab - 2019 - A Mixed-Methods Approach Using Self-Report, Observ.pdf}
}

@article{brillTpatternAnalysisSpike2020,
  title = {T-Pattern Analysis and Spike Train Dissimilarity for the Analysis of Structure in Blinking Behavior},
  author = {Brill, Michael and Schwab, Frank},
  year = {2020},
  month = dec,
  journal = {Physiology \& Behavior},
  volume = {227},
  pages = {113163},
  issn = {0031-9384},
  doi = {10.1016/j.physbeh.2020.113163},
  abstract = {Spontaneous eye-blinks are a ubiquitous behavior. However, blink timing is not random, nor does it always follow physiological demands. Research rather suggests that blink timing, and thus the structure of blinking behavior, is influenced by cognitive processes, such as attention. Since attention is regarded a necessary precursor of media use phenomena, the present study investigates the relation between the structure of blinking behavior and the media use phenomenon of spatial presence. To this end, spontaneous eye-blinks have been observed in an experiment during the reception of a video story. The methods of T-pattern analysis, ISI distance, and IBI variability have been used to quantify stimulus-dependent blink structure, which has then been related to self-reports of spatial presence experiences. While the T-pattern analysis and ISI distance showed converging results for behavior structure, a hypothesized relation between more stimulus-dependent blink structure and stronger presence experiences was not found. On the contrary, blink data suggested a difference in attention allocation, whereas self-report data indicated no difference in presence experiences. This demonstrates that beyond self-report and the analysis of event frequencies, the analysis of behavior structure offers insights into behavior synchronization between participants, allowing for new inferences on internal processing of media stimuli.},
  langid = {english},
  keywords = {Attention,Blinking,ISI distance,Spike trains,Synchronization,T-patterns},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\FFSSXEJV\\S0031938420304777.html}
}

@inproceedings{brockmannNonclassicalApproachNeural1994,
  title = {A Non-Classical Approach to Neural Networks},
  booktitle = {Proceedings of 1994 {{IEEE International Conference}} on {{Neural Networks}} ({{ICNN}}'94)},
  author = {Brockmann, W.},
  year = {1994},
  month = jun,
  volume = {3},
  pages = {1607-1612 vol.3},
  doi = {10.1109/ICNN.1994.374396},
  abstract = {Artificial neural networks have shown their usefulness in many applications. But they are hampered by some drawbacks such as long training times and limitations concerning implementation on low cost microcontrollers. As a possible solution, a non-classical network approach is presented, which is centered between symbolic and subsymbolic computation. It consists of nodes based on a lookup table. The node and network structures are discussed in more detail while a heuristic learning scheme is only outlined. Some examination results are presented using a closed loop control application.{$<>$}},
  keywords = {Artificial neural networks,Computer networks,Costs,Electronic mail,Fuzzy logic,Knowledge based systems,Microcontrollers,Neural networks,Neurons,Table lookup},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\R7JBGRPB\\Brockmann - 1994 - A non-classical approach to neural networks.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\99894BGX\\374396.html}
}

@inproceedings{bunteWhySymbolicAI2019,
  title = {Why {{Symbolic AI}} Is a {{Key Technology}} for {{Self-Adaption}} in the {{Context}} of {{CPPS}}},
  booktitle = {2019 24th {{IEEE International Conference}} on {{Emerging Technologies}} and {{Factory Automation}} ({{ETFA}})},
  author = {Bunte, Andreas and Wunderlich, Paul and Moriz, Natalia and Li, Peng and Mankowski, Andr{\'e} and Rogalla, Antje and Niggemann, Oliver},
  year = {2019},
  month = sep,
  pages = {1701--1704},
  issn = {1946-0759},
  doi = {10.1109/ETFA.2019.8869082},
  abstract = {The vision of smart factories are self-diagnosing, self-optimizing and self-adapting Cyber-Physical Production Systems (CPPS). Self-adaption, on which this paper focuses on, means that the CPPS can adapt itself to a changing environment, so that the downtime costs can be reduced by using the system modules most efficient. An architecture is introduced and demonstrated on a concrete use case to show how this capability can be achieved by using different Artificial Intelligence (AI) techniques. For each technique, we define challenges that have to be solved to use it in a real world environment. Additionally, we illustrate the symbolic and subsymbolic AI and argue why symbolic AI is an important aspect in the context of CPPS.},
  keywords = {Adaptation models,Artificial intelligence,Automation,Data analysis,Planning,Process planning},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\5M54XRC2\\Bunte et al. - 2019 - Why Symbolic AI is a Key Technology for Self-Adapt.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\TUIM8KZR\\8869082.html}
}

@article{calvoHumanObserversAutomated2018,
  title = {Human {{Observers}} and {{Automated Assessment}} of {{Dynamic Emotional Facial Expressions}}: {{KDEF-dyn Database Validation}}},
  shorttitle = {Human {{Observers}} and {{Automated Assessment}} of {{Dynamic Emotional Facial Expressions}}},
  author = {Calvo, Manuel G. and {Fern{\'a}ndez-Mart{\'i}n}, Andr{\'e}s and Recio, Guillermo and Lundqvist, Daniel},
  year = {2018},
  month = oct,
  journal = {Frontiers in Psychology},
  volume = {9},
  pages = {2052},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2018.02052},
  abstract = {Most experimental studies of facial expression processing have used static stimuli (photographs), yet facial expressions in daily life are generally dynamic. In its original photographic format, the Karolinska Directed Emotional Faces (KDEF) has been frequently utilized. In the current study, we validate a dynamic version of this database, the KDEF-dyn. To this end, we applied animation between neutral and emotional expressions (happy, sad, angry, fearful, disgusted, and surprised; 1,033-ms unfolding) to 40 KDEF models, with morphing software. Ninety-six human observers categorized the expressions of the resulting 240 video-clip stimuli, and automated face analysis assessed the evidence for 6 expressions and 20 facial action units (AUs) at 31 intensities. Low-level image properties (luminance, signal-to-noise ratio, etc.) and other purely perceptual factors (e.g., size, unfolding speed) were controlled. Human recognition performance (accuracy, efficiency, and confusions) patterns were consistent with prior research using static and other dynamic expressions. Automated assessment of expressions and AUs was sensitive to intensity manipulations. Significant correlations emerged between human observers' categorization and automated classification. The KDEF-dyn database aims to provide a balance between experimental control and ecological validity for research on emotional facial expression processing. The stimuli and the validation data are available to the scientific community.},
  langid = {english},
  note = {Belongs to KDEF-dyn-I Data Set},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\2EUQTAKH\\Calvo et al. - 2018 - Human Observers and Automated Assessment of Dynami.pdf}
}

@article{calvoSelectiveEyeFixations2018,
  title = {Selective Eye Fixations on Diagnostic Face Regions of Dynamic Emotional Expressions: {{KDEF-dyn}} Database},
  shorttitle = {Selective Eye Fixations on Diagnostic Face Regions of Dynamic Emotional Expressions},
  author = {Calvo, Manuel G. and {Fern{\'a}ndez-Mart{\'i}n}, Andr{\'e}s and {Guti{\'e}rrez-Garc{\'i}a}, Aida and Lundqvist, Daniel},
  year = {2018},
  month = dec,
  journal = {Scientific Reports},
  volume = {8},
  number = {1},
  pages = {17039},
  issn = {2045-2322},
  doi = {10.1038/s41598-018-35259-w},
  langid = {english},
  note = {Belongs to KDEF-dyn-II Data Set},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\DMV2AV8U\\Calvo et al. - 2018 - Selective eye fixations on diagnostic face regions.pdf}
}

@article{campbellAugeKunstlichenIntelligenz2022,
  title = {Das {{Auge}} Der K\"unstlichen {{Intelligenz}}},
  author = {Campbell, Martin HessePatrick BeuthZach},
  year = {2022},
  journal = {DER SPIEGEL},
  number = {7},
  pages = {72--73},
  file = {C\:\\Users\\gebele\\Desktop\\Das_Auge_der_künstlichen_Intelligenz.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\FBZS4PH2\\SPIE__PMG4SPIEGEL-Heftimport-SP20220212-58725_431a535f-6045-46d7-8ab1-36ceec620365.html}
}

@article{canalSurveyFacialEmotion2022,
  title = {A Survey on Facial Emotion Recognition Techniques: {{A}} State-of-the-Art Literature Review},
  shorttitle = {A Survey on Facial Emotion Recognition Techniques},
  author = {Canal, Felipe Zago and M{\"u}ller, Tobias Rossi and Matias, Jhennifer Cristine and Scotton, Gustavo Gino and {de Sa Junior}, Antonio Reis and Pozzebon, Eliane and Sobieranski, Antonio Carlos},
  year = {2022},
  month = jan,
  journal = {Information Sciences},
  volume = {582},
  pages = {593--617},
  issn = {0020-0255},
  doi = {10.1016/j.ins.2021.10.005},
  abstract = {In this survey, a systematic literature review of the state-of-the-art on emotion expression recognition from facial images is presented. The paper has as main objective arise the most commonly used strategies employed to interpret and recognize facial emotion expressions, published over the past few years. For this purpose, a total of 51 papers were analyzed over the literature totaling 94 distinct methods, collected from well-established scientific databases (ACM Digital Library, IEEE Xplore, Science Direct and Scopus), whose works were categorized according to its main construction concept. From the analyzed works, it was possible to categorize them into two main trends: classical and those approaches specifically designed by the use of neural networks. The obtained statistical analysis demonstrated a marginally better recognition precision for the classical approaches when faced to neural networks counterpart, but with a reduced capacity of generalization. Additionally, the present study verified the most popular datasets for facial expression and emotion recognition showing the pros and cons each and, thereby, demonstrating a real demand for reliable data-sources regarding artificial and natural experimental environments.},
  langid = {english},
  keywords = {Emotion Recognition,Facial emotion recognition,Pattern recognition,Systematic literature review},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\IU879ADM\\S0020025521010136.html}
}

@article{cannonJamesLangeTheoryEmotions1927,
  title = {The {{James-Lange}} Theory of Emotions: {{A}} Critical Examination and an Alternative Theory},
  shorttitle = {The {{James-Lange}} Theory of Emotions},
  author = {Cannon, Walter B.},
  year = {1927},
  journal = {The American journal of psychology},
  volume = {39},
  number = {1/4},
  pages = {106--124},
  publisher = {{JSTOR}},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\KAB3SB3H\\Cannon - 1927 - The James-Lange theory of emotions A critical exa.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\UQ58DY28\\1415404.html}
}

@article{caramihaleEmotionClassificationUsing2018,
  title = {Emotion {{Classification Using}} a {{Tensorflow Generative Adversarial Network Implementation}}},
  author = {Caramihale, Traian and Popescu, Dan and Ichim, Loretta},
  year = {2018},
  month = sep,
  journal = {Symmetry},
  volume = {10},
  number = {9},
  pages = {414},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2073-8994},
  doi = {10.3390/sym10090414},
  abstract = {The detection of human emotions has applicability in various domains such as assisted living, health monitoring, domestic appliance control, crowd behavior tracking real time, and emotional security. The paper proposes a new system for emotion classification based on a generative adversarial network (GAN) classifier. The generative adversarial networks have been widely used for generating realistic images, but the classification capabilities have been vaguely exploited. One of the main advantages is that by using the generator, we can extend our testing dataset and add more variety to each of the seven emotion classes we try to identify. Thus, the novelty of our study consists in increasing the number of classes from N to 2N (in the learning phase) by considering real and fake emotions. Facial key points are obtained from real and generated facial images, and vectors connecting them with the facial center of gravity are used by the discriminator to classify the image as one of the 14 classes of interest (real and fake for seven emotions). As another contribution, real images from different emotional classes are used in the generation process unlike the classical GAN approach which generates images from simple noise arrays. By using the proposed method, our system can classify emotions in facial images regardless of gender, race, ethnicity, age and face rotation. An accuracy of 75.2\% was obtained on 7000 real images (14,000, also considering the generated images) from multiple combined facial datasets.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {convolutional neural networks,emotion classification,facial images processing,facial key point detection,generative adversarial network},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\BK5TRVLS\\Caramihale et al. - 2018 - Emotion Classification Using a Tensorflow Generati.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\PKKBJRSP\\414.html}
}

@article{ceccacciToolMakeShopping2018,
  title = {Tool to Make Shopping Experience Responsive to Customer Emotions},
  author = {Ceccacci, Silvia and Generosi, Andrea and Giraldi, Luca and Mengoni, Maura},
  year = {2018},
  journal = {International Journal of Automation Technology},
  volume = {12},
  number = {3},
  pages = {319--326},
  issn = {1881-7629},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\EZSSWZCG\\ToolToMakeShoppingExperienceResponsivetoCustom.pdf}
}

@inproceedings{changPersonReidentificationBased2022,
  title = {Person {{Re-identification Based}} on {{Deep Learning Algorithms}} with {{Manual Extracted Colour}} and {{Texture Features}}},
  booktitle = {2022 7th {{International Conference}} on {{Big Data Analytics}} ({{ICBDA}})},
  author = {Chang, Xiangqian and Su, Zhihao and Ni, Li and Liu, Chenyang and Xue, Huimin and Nie, Yunxi},
  year = {2022},
  month = mar,
  pages = {243--247},
  doi = {10.1109/ICBDA55095.2022.9760316},
  abstract = {Person re-identification technology is an important research direction in the field of computer vision and intelligent transportation. Deep learning algorithms have better performance than traditional feature extraction algorithms in recent years. The paper proposes a hybrid deep learning algorithm with manual colour and texture features. The proposed hybrid model in the paper has 3-5 \% higher accuracy than the single deep learning algorithms in the two mainstream databases. The experiment proves that the combination between deep learning algorithms and traditional algorithms has great significance.},
  keywords = {artificial intelligence,Convolution,Databases,Deep learning,deep learning algorithms,Image color analysis,manual colour and texture features,Manuals,Neural networks,person re-identification,Transportation},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\8SD8WXY4\\Chang et al. - 2022 - Person Re-identification Based on Deep Learning Al.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\43D7UYR5\\9760316.html}
}

@inproceedings{chanRobust2DIndoor2018,
  title = {Robust {{2D Indoor Localization Through Laser SLAM}} and {{Visual SLAM Fusion}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Systems}}, {{Man}}, and {{Cybernetics}} ({{SMC}})},
  author = {Chan, Shao-Hung and Wu, Ping-Tsang and Fu, Li-Chen},
  year = {2018},
  month = oct,
  pages = {1263--1268},
  issn = {2577-1655},
  doi = {10.1109/SMC.2018.00221},
  abstract = {An approach of robust localization for mobile robot working in indoor is proposed in this paper. A novel method for laser SLAM and visual SLAM fusion is introduced to provide robust localization. This architecture can be applied to a situation where any two kinds of laser-based SLAM and monocular camera-based SLAM can be fused together instead of being limited to single specific SLAM algorithm. While laser-based SLAM and monocular camera-based SLAM have their own strengths and drawbacks, the integration of these two kinds of SLAM algorithm can then promote the algorithmic effectiveness. Instead of using feature matching methods to achieve fusion procedure, trajectories matching is proposed with an attempt to achieve the generalization over all different kinds of SLAM algorithms, since localization is a natural function associated with any SLAM algorithm. It turns out that the hereby proposed approach is very lightweight during the run time, and the calculation can run in real-time without unnecessary computation waste. The experimental results show the localization error in terms of the real distance can be less than 5\%. Furthermore, through the experiment the proposed system can be shown able to improve the localization when the sensors are not very powerful.},
  keywords = {Cameras,Laser fusion,laser-based SLAM,monocular camera-based SLAM,service robot,Simultaneous localization and mapping,SLAM fusion,Trajectory,Visualization},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\C27PK54N\\Chan et al. - 2018 - Robust 2D Indoor Localization Through Laser SLAM a.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\WEB5FPVA\\8616217.html}
}

@inproceedings{chauhanAutomatedMachineLearning2020,
  title = {Automated {{Machine Learning}}: {{The New Wave}} of {{Machine Learning}}},
  shorttitle = {Automated {{Machine Learning}}},
  booktitle = {2020 2nd {{International Conference}} on {{Innovative Mechanisms}} for {{Industry Applications}} ({{ICIMIA}})},
  author = {Chauhan, Karansingh and Jani, Shreena and Thakkar, Dhrumin and Dave, Riddham and Bhatia, Jitendra and Tanwar, Sudeep and Obaidat, Mohammad S.},
  year = {2020},
  month = mar,
  pages = {205--212},
  doi = {10.1109/ICIMIA48430.2020.9074859},
  abstract = {With the explosion in the use of machine learning in various domains, the need for an efficient pipeline for the development of machine learning models has never been more critical. However, the task of forming and training models largely remains traditional with a dependency on domain experts and time-consuming data manipulation operations, which impedes the development of machine learning models in both academia as well as industry. This demand advocates the new research era concerned with fitting machine learning models fully automatically i.e., AutoML. Automated Machine Learning(AutoML) is an end-to-end process that aims at automating this model development pipeline without any external assistance. First, we provide an insights of AutoML. Second, we delve into the individual segments in the AutoML pipeline and cover their approaches in brief. We also provide a case study on the industrial use and impact of AutoML with a focus on practical applicability in a business context. At last, we conclude with the open research issues, and future research directions.},
  keywords = {Artificial Intelligence Meta Learning,Automated Machine Learning,Bayes methods,Encoding,Hyperparameter Optimization,Machine learning,Optimization,Pipelines,Tools,Training},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\YGE3LJYG\\Chauhan et al. - 2020 - Automated Machine Learning The New Wave of Machin.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\7C8Z6RRW\\9074859.html}
}

@inproceedings{chauhanReviewConventionalMachine2018,
  title = {A {{Review}} on {{Conventional Machine Learning}} vs {{Deep Learning}}},
  booktitle = {2018 {{International Conference}} on {{Computing}}, {{Power}} and {{Communication Technologies}} ({{GUCON}})},
  author = {Chauhan, Nitin Kumar and Singh, Krishna},
  year = {2018},
  month = sep,
  pages = {347--352},
  doi = {10.1109/GUCON.2018.8675097},
  abstract = {In now days, deep learning has become a prominent and emerging research area in computer vision applications. Deep learning permits the multiple layers models for computation to learn representations of data by processing in their original form while it is not possible in conventional machine learning. These methods surprisingly improved the accuracy of various image processing domains such as speech recognition, face recognition, object detection and in biomedical applications. Deep neural networks (DNN) such as convolutional neural network (CNN) provide tremendous results in processing of images and videos, while another approach of deep network i.e. recurrent neural network (RNN) gives better performance with sequential data such as text and speech.},
  keywords = {ANN,Classification algorithms,CNN,Deep learning,DNN,DT,Fully connected layers,LDA,Machine learning algorithms,Neural networks,Neurons,PCA,Pooling layers,QDA,RBM,RNN,Support vector machines,SVM},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\49M8EHG6\\Chauhan und Singh - 2018 - A Review on Conventional Machine Learning vs Deep .pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\2F9GA25E\\8675097.html}
}

@article{chenContextualSemanticEmbeddings2022,
  title = {Contextual {{Semantic Embeddings}} for {{Ontology Subsumption Prediction}}},
  author = {Chen, Jiaoyan and He, Yuan and {Jimenez-Ruiz}, Ernesto and Dong, Hang and Horrocks, Ian},
  year = {2022},
  month = feb,
  journal = {arXiv:2202.09791 [cs]},
  eprint = {2202.09791},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Automating ontology curation is a crucial task in knowledge engineering. Prediction by machine learning techniques such as semantic embedding is a promising direction, but the relevant research is still preliminary. In this paper, we present a class subsumption prediction method named BERTSubs, which uses the pre-trained language model BERT to compute contextual embeddings of the class labels and customized input templates to incorporate contexts of surrounding classes. The evaluation on two large-scale real-world ontologies has shown its better performance than the state-of-the-art.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  note = {Comment: Short paper (5 pages)},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\WYKXCJN2\\Chen et al. - 2022 - Contextual Semantic Embeddings for Ontology Subsum.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\DY384VW2\\2202.html}
}

@misc{ChenDistinctFacial,
  title = {Chen: {{Distinct}} Facial Expressions Represent Pain... - {{Google Scholar}}},
  howpublished = {https://scholar.google.com/scholar\_lookup?title=\&journal=Proc.\%20Natl\%20Acad.\%20Sci.\%20USA\&volume=115\&pages=E10013-E10021\&publication\_year=2018\&author=Chen\%2CC.},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\9GDFLGWB\\Chen Distinct facial expressions represent pain...pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\BKP9BWJQ\\scholar_lookup.html}
}

@article{chenDistinctFacialExpressions2018,
  title = {Distinct Facial Expressions Represent Pain and Pleasure across Cultures},
  author = {Chen, Chaona and Crivelli, Carlos and Garrod, Oliver G. B. and Schyns, Philippe G. and {Fern{\'a}ndez-Dols}, Jos{\'e}-Miguel and Jack, Rachael E.},
  year = {2018},
  month = oct,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {115},
  number = {43},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1807862115},
  abstract = {Significance             Humans often use facial expressions to communicate social messages. However, observational studies report that people experiencing pain or orgasm produce facial expressions that are indistinguishable, which questions their role as an effective tool for communication. Here, we investigate this counterintuitive finding using a new data-driven approach to model the mental representations of facial expressions of pain and orgasm in individuals from two different cultures. Using complementary analyses, we show that representations of pain and orgasm are distinct in each culture. We also show that pain is represented with similar face movements across cultures, whereas orgasm shows differences. Our findings therefore inform understanding of the possible communicative role of facial expressions of pain and orgasm, and how culture could shape their representation.           ,              Real-world studies show that the facial expressions produced during pain and orgasm\textemdash two different and intense affective experiences\textemdash are virtually indistinguishable. However, this finding is counterintuitive, because facial expressions are widely considered to be a powerful tool for social interaction. Consequently, debate continues as to whether the facial expressions of these extreme positive and negative affective states serve a communicative function. Here, we address this debate from a novel angle by modeling the mental representations of dynamic facial expressions of pain and orgasm in 40 observers in each of two cultures (Western, East Asian) using a data-driven method. Using a complementary approach of machine learning, an information-theoretic analysis, and a human perceptual discrimination task, we show that mental representations of pain and orgasm are physically and perceptually distinct in each culture. Cross-cultural comparisons also revealed that pain is represented by similar face movements across cultures, whereas orgasm showed distinct cultural accents. Together, our data show that mental representations of the facial expressions of pain and orgasm are distinct, which questions their nondiagnosticity and instead suggests they could be used for communicative purposes. Our results also highlight the potential role of cultural and perceptual factors in shaping the mental representation of these facial expressions. We discuss new research directions to further explore their relationship to the production of facial expressions.},
  langid = {english},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\L2Q9JIJS\\Chen et al. - 2018 - Distinct facial expressions represent pain and ple.pdf}
}

@article{chenEmotionRecognitionAudio2022,
  title = {Emotion {{Recognition With Audio}}, {{Video}}, {{EEG}}, and {{EMG}}: {{A Dataset}} and {{Baseline Approaches}}},
  shorttitle = {Emotion {{Recognition With Audio}}, {{Video}}, {{EEG}}, and {{EMG}}},
  author = {Chen, Jin and Ro, Tony and Zhu, Zhigang},
  year = {2022},
  journal = {IEEE Access},
  volume = {10},
  pages = {13229--13242},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2022.3146729},
  abstract = {This paper describes a new posed multimodal emotional dataset and compares human emotion classification based on four different modalities - audio, video, electromyography (EMG), and electroencephalography (EEG). The results are reported with several baseline approaches using various feature extraction techniques and machine-learning algorithms. First, we collected a dataset from 11 human subjects expressing six basic emotions and one neutral emotion. We then extracted features from each modality using principal component analysis, autoencoder, convolution network, and mel-frequency cepstral coefficient (MFCC), some unique to individual modalities. A number of baseline models have been applied to compare the classification performance in emotion recognition, including k-nearest neighbors (KNN), support vector machines (SVM), random forest, multilayer perceptron (MLP), long short-term memory (LSTM) model, and convolutional neural network (CNN). Our results show that bootstrapping the biosensor signals (i.e., EMG and EEG) can greatly increase emotion classification performance by reducing noise. In contrast, the best classification results were obtained by a traditional KNN, whereas audio and image sequences of human emotions could be better classified using LSTM.},
  keywords = {data collection,electroencephalography,Electroencephalography,electromyography,Electromyography,Emotion recognition,Feature extraction,Physiology,Support vector machines,Videos},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\NG25MK64\\Chen et al. - 2022 - Emotion Recognition With Audio, Video, EEG, and EM.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\3EGMHQB3\\9693926.html}
}

@article{chenJointAttentiveSpatialTemporal2019,
  title = {Joint {{Attentive Spatial-Temporal Feature Aggregation}} for {{Video-Based Person Re-Identification}}},
  author = {Chen, Lin and Yang, Hua and Gao, Zhiyong},
  year = {2019},
  journal = {IEEE Access},
  volume = {7},
  pages = {41230--41240},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2907274},
  abstract = {Video-based person re-identification (Re-ID) remains to be a promising but challenging computer vision task, suffering from a lack of discriminative features that better aggregate both the spatial and temporal information. In this paper, we propose a joint attentive spatial-temporal feature aggregation network (JAFN) for the video-based person Re-ID, simultaneously learning the quality- and frame-aware model to obtain attention-based spatial-temporal feature aggregation. Specifically, we utilize CNN to learn the spatial features, while introducing the LSTM to separately learn the temporal features. For the feature aggregation, we introduce two attention mechanisms respectively for generating the quality and frame significance score, where the quality score measures the quality of the images for attentive spatial feature aggregation, and the frame score measures the significance of the image frames contributing to the temporal feature. Then, we utilize the set-pooling for both the quality-aware spatial feature and the frame-aware temporal feature aggregation based on the attentive scores. The residual learning is also introduced to play between the LSTM and the CNN for adaptive spatial-temporal feature fusion. Furthermore, we adopt the data balance to alleviate the data disproportions existing in datasets of the video-based Re-ID. The extensive experimental results conducted on the PRID2011, i-LIDS-VID, and MARS datasets demonstrate the effectiveness of the proposed JAFN. Furthermore, comparison results conducted on different modules and features in the JAFN show that our approach is of favorable generalization ability on attentively aggregating both the spatial and temporal features.},
  keywords = {Aggregates,aggregation,attention,Cameras,Clutter,Feature extraction,Image recognition,Person re-identification,Reliability,spatial-temporal,Task analysis},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\5GQYEEQX\\Chen et al. - 2019 - Joint Attentive Spatial-Temporal Feature Aggregati.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\R3CD5R6K\\8675282.html}
}

@misc{ChenTrackingAffective,
  title = {Chen: {{Tracking}} the Affective State of Unseen Persons - {{Google Scholar}}},
  howpublished = {https://scholar.google.com/scholar\_lookup?title=\&journal=Proc.\%20Natl\%20Acad.\%20Sci.\%20USA\&volume=116\&pages=7559-7564\&publication\_year=2019\&author=Chen\%2CZ.\&author=Whitney\%2CD.},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\9IZ3QCJC\\Chen Tracking the affective state of unseen perso.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\RXX3F2LG\\scholar_lookup.html}
}

@article{chenTrackingAffectiveState2019,
  title = {Tracking the Affective State of Unseen Persons},
  author = {Chen, Zhimin and Whitney, David},
  year = {2019},
  month = apr,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {116},
  number = {15},
  pages = {7559--7564},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.1812250116},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\ZD4RXGMN\\Chen und Whitney - 2019 - Tracking the affective state of unseen persons.pdf}
}

@article{chilsonTwoFacesFacial2021a,
  title = {The {{Two Faces}} of {{Facial Recognition Technology}}},
  author = {Chilson, Neil A. and Barkley, Taylor D.},
  year = {2021},
  month = dec,
  journal = {IEEE Technology and Society Magazine},
  volume = {40},
  number = {4},
  pages = {87--100},
  issn = {1937-416X},
  doi = {10.1109/MTS.2021.3123752},
  abstract = {The face is the most public part of human bodies. We usually recognize or identify someone by their face. Our brains are wired to quickly spot faces\textemdash even in arrangements of inanimate objects\textemdash and to instantly identify the emotions they convey.},
  keywords = {Artificial intelligence,Emotion recognition,Face recognition,Government,Law,Law enforcement,Object recognition,Privacy,Security},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\T6MIF5PT\\Chilson und Barkley - 2021 - The Two Faces of Facial Recognition Technology.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\GVYXFA42\\9639441.html}
}

@misc{ChinaReleases14th,
  title = {China {{Releases}} 14th {{Five-Year Plan Notice}} on {{Intellectual Property}} \textendash{} {{China}} to {{More}} than {{Double Number}} of {{Foreign Patents}} by 2025},
  journal = {Schwegman Lundberg \& Woessner},
  abstract = {Jump into the SLW Institute as we discuss China Releases 14th Five-Year Plan Notice on Intellectual Property \textendash{} China to More than Double Number of Foreign Patents by 2025. Learn more from the experts today.},
  howpublished = {https://www.slwip.com/resources/china-releases-14th-five-year-plan-notice-on-intellectual-property-china-to-more-than-double-number-of-foreign-patents-by-2025/},
  langid = {american},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\IWU5D4PB\\china-releases-14th-five-year-plan-notice-on-intellectual-property-china-to-more-than-double-nu.html}
}

@article{cholletMeasureIntelligence2019,
  title = {On the {{Measure}} of {{Intelligence}}},
  author = {Chollet, Fran{\c c}ois},
  year = {2019},
  month = nov,
  journal = {arXiv:1911.01547 [cs]},
  eprint = {1911.01547},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {To make deliberate progress towards more intelligent and more human-like artificial systems, we need to be following an appropriate feedback signal: we need to be able to define and evaluate intelligence in a way that enables comparisons between two systems, as well as comparisons with humans. Over the past hundred years, there has been an abundance of attempts to define and measure intelligence, across both the fields of psychology and AI. We summarize and critically assess these definitions and evaluation approaches, while making apparent the two historical conceptions of intelligence that have implicitly guided them. We note that in practice, the contemporary AI community still gravitates towards benchmarking intelligence by comparing the skill exhibited by AIs and humans at specific tasks such as board games and video games. We argue that solely measuring skill at any given task falls short of measuring intelligence, because skill is heavily modulated by prior knowledge and experience: unlimited priors or unlimited training data allow experimenters to "buy" arbitrary levels of skills for a system, in a way that masks the system's own generalization power. We then articulate a new formal definition of intelligence based on Algorithmic Information Theory, describing intelligence as skill-acquisition efficiency and highlighting the concepts of scope, generalization difficulty, priors, and experience. Using this definition, we propose a set of guidelines for what a general AI benchmark should look like. Finally, we present a benchmark closely following these guidelines, the Abstraction and Reasoning Corpus (ARC), built upon an explicit set of priors designed to be as close as possible to innate human priors. We argue that ARC can be used to measure a human-like form of general fluid intelligence and that it enables fair general intelligence comparisons between AI systems and humans.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\2FFYSLSS\\Chollet - 2019 - On the Measure of Intelligence.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\SDWWCBW8\\1911.html}
}

@inproceedings{cholletXceptionDeepLearning2017,
  title = {Xception: {{Deep}} Learning with Depthwise Separable Convolutions},
  shorttitle = {Xception},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Chollet, Fran{\c c}ois},
  year = {2017},
  pages = {1251--1258},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\Z7Z9SQRV\\Chollet - 2017 - Xception Deep learning with depthwise separable c.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\WL6C69UG\\Chollet_Xception_Deep_Learning_CVPR_2017_paper.html}
}

@inproceedings{chongDeepLearningBased2022,
  title = {Deep {{Learning}} Based {{Semantic Ontology Alignment Process}} and {{Predictive Analysis}} of {{Depressive Disorder}}},
  booktitle = {2022 {{International Conference}} on {{Information Networking}} ({{ICOIN}})},
  author = {Chong, Ilyoung and Lee, Sanghong},
  year = {2022},
  month = jan,
  pages = {164--167},
  issn = {1976-7684},
  doi = {10.1109/ICOIN53446.2022.9687251},
  abstract = {The paper proposes deep learning based semantic ontology alignment process and predictive analysis methodology of DD as a use case. In the development of a semantic ontology alignment process, a mapping procedure has performed to find a WoO based semantic interoperable ontology of DD with respected to base ontology and three reference ontologies. And the semantic interoperable ontology developed in the first step is applied to perform a predictive analysis for characterization of the strong features to affect depression. In the experimentation, this paper shows its reasonable applicability through deep learning based analysis of DD case.},
  keywords = {correlation,deep learning,Deep learning,Depression,depressive disorder,Ontologies,Predictive analytics,semantic interoperability,semantic ontology alignment,Semantics,Web of Objects},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\3SNTTWY3\\Chong und Lee - 2022 - Deep Learning based Semantic Ontology Alignment Pr.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\C6YZBEPZ\\9687251.html}
}

@misc{chunSelfsupervisedRegressionLearning2022a,
  title = {Self-Supervised Regression Learning Using Domain Knowledge: {{Applications}} to Improving Self-Supervised Denoising in Imaging},
  shorttitle = {Self-Supervised Regression Learning Using Domain Knowledge},
  author = {Chun, Il Yong and Park, Dongwon and Zheng, Xuehang and Chun, Se Young and Long, Yong},
  year = {2022},
  month = may,
  number = {arXiv:2205.04821},
  eprint = {2205.04821},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2205.04821},
  abstract = {Regression that predicts continuous quantity is a central part of applications using computational imaging and computer vision technologies. Yet, studying and understanding self-supervised learning for regression tasks - except for a particular regression task, image denoising - have lagged behind. This paper proposes a general self-supervised regression learning (SSRL) framework that enables learning regression neural networks with only input data (but without ground-truth target data), by using a designable pseudo-predictor that encapsulates domain knowledge of a specific application. The paper underlines the importance of using domain knowledge by showing that under different settings, the better pseudo-predictor can lead properties of SSRL closer to those of ordinary supervised learning. Numerical experiments for low-dose computational tomography denoising and camera image denoising demonstrate that proposed SSRL significantly improves the denoising quality over several existing self-supervised denoising methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  note = {Comment: 17 pages, 16 figures, 2 tables, submitted to IEEE T-IP},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\KP2M7KLH\\Chun et al. - 2022 - Self-supervised regression learning using domain k.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\UEYPZCRR\\Chun et al. - 2022 - Self-supervised regression learning using domain k.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\ENEXN4IL\\2205.html}
}

@misc{clareExplainableArtificialIntelligence2022,
  title = {Explainable {{Artificial Intelligence}} for {{Bayesian Neural Networks}}: {{Towards}} Trustworthy Predictions of Ocean Dynamics},
  shorttitle = {Explainable {{Artificial Intelligence}} for {{Bayesian Neural Networks}}},
  author = {Clare, Mariana C. A. and Sonnewald, Maike and Lguensat, Redouane and Deshayes, Julie and Balaji, Venkatramani},
  year = {2022},
  month = apr,
  number = {arXiv:2205.00202},
  eprint = {2205.00202},
  eprinttype = {arxiv},
  primaryclass = {physics},
  institution = {{arXiv}},
  abstract = {The trustworthiness of neural networks is often challenged because they lack the ability to express uncertainty and explain their skill. This can be problematic given the increasing use of neural networks in high stakes decision-making such as in climate change applications. We address both issues by successfully implementing a Bayesian Neural Network (BNN), where parameters are distributions rather than deterministic, and applying novel implementations of explainable AI (XAI) techniques. The uncertainty analysis from the BNN provides a comprehensive overview of the prediction more suited to practitioners' needs than predictions from a classical neural network. Using a BNN means we can calculate the entropy (i.e. uncertainty) of the predictions and determine if the probability of an outcome is statistically significant. To enhance trustworthiness, we also spatially apply the two XAI techniques of Layer-wise Relevance Propagation (LRP) and SHapley Additive exPlanation (SHAP) values. These XAI methods reveal the extent to which the BNN is suitable and/or trustworthy. Using two techniques gives a more holistic view of BNN skill and its uncertainty, as LRP considers neural network parameters, whereas SHAP considers changes to outputs. We verify these techniques using comparison with intuition from physical theory. The differences in explanation identify potential areas where new physical theory guided studies are needed.},
  archiveprefix = {arXiv},
  keywords = {68T07; 86A05; 86A08,Computer Science - Machine Learning,I.2.6,J.2,Physics - Atmospheric and Oceanic Physics},
  note = {Comment: 25 pages, 11 figures},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\MKSZGTZI\\Clare et al. - 2022 - Explainable Artificial Intelligence for Bayesian N.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\FIBAJB5M\\2205.html}
}

@article{clarkeBasicsPatentSearching2018,
  title = {The Basics of Patent Searching},
  author = {Clarke, Nigel S.},
  year = {2018},
  journal = {World Patent Information},
  volume = {54},
  pages = {S4--S10},
  publisher = {{Elsevier}},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\MPEU6V42\\Clarke - 2018 - The basics of patent searching.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\MJLXSC5S\\S017221901630103X.html}
}

@misc{CowenMappingPassions,
  title = {Cowen: {{Mapping}} the Passions: {{Toward}} a High-Dimensional... - {{Google Scholar}}},
  howpublished = {https://scholar.google.com/scholar\_lookup?title=\&journal=Psychol.\%20Sci.\%20Publ.\%20Interest\&volume=20\&pages=69-90\&publication\_year=2019\&author=Cowen\%2CA.\&author=Sauter\%2CD.\&author=Tracy\%2CJ.\%20L.\&author=Keltner\%2CD.},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\ZILYINUC\\Cowen Mapping the passions Toward a high-dimensi.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\YMFREHS8\\scholar_lookup.html}
}

@article{cowenMappingPassionsHighDimensional2019,
  title = {Mapping the {{Passions}}: {{Toward}} a {{High-Dimensional Taxonomy}} of {{Emotional Experience}} and {{Expression}}},
  shorttitle = {Mapping the {{Passions}}},
  author = {Cowen, Alan and Sauter, Disa and Tracy, Jessica L. and Keltner, Dacher},
  year = {2019},
  month = jul,
  journal = {Psychological Science in the Public Interest},
  volume = {20},
  number = {1},
  pages = {69--90},
  publisher = {{SAGE Publications Inc}},
  issn = {1529-1006},
  doi = {10.1177/1529100619850176},
  abstract = {What would a comprehensive atlas of human emotions include? For 50 years, scientists have sought to map emotion-related experience, expression, physiology, and recognition in terms of the ``basic six''\textemdash anger, disgust, fear, happiness, sadness, and surprise. Claims about the relationships between these six emotions and prototypical facial configurations have provided the basis for a long-standing debate over the diagnostic value of expression (for review and latest installment in this debate, see Barrett et al., p. 1). Building on recent empirical findings and methodologies, we offer an alternative conceptual and methodological approach that reveals a richer taxonomy of emotion. Dozens of distinct varieties of emotion are reliably distinguished by language, evoked in distinct circumstances, and perceived in distinct expressions of the face, body, and voice. Traditional models\textemdash both the basic six and affective-circumplex model (valence and arousal)\textemdash capture a fraction of the systematic variability in emotional response. In contrast, emotion-related responses (e.g., the smile of embarrassment, triumphant postures, sympathetic vocalizations, blends of distinct expressions) can be explained by richer models of emotion. Given these developments, we discuss why tests of a basic-six model of emotion are not tests of the diagnostic value of facial expression more generally. Determining the full extent of what facial expressions can tell us, marginally and in conjunction with other behavioral and contextual cues, will require mapping the high-dimensional, continuous space of facial, bodily, and vocal signals onto richly multifaceted experiences using large-scale statistical modeling and machine-learning methods.},
  langid = {english},
  keywords = {affect,emotion,expression,face,semantic space,signal,voice},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\GZE6DN63\\Cowen et al. - 2019 - Mapping the Passions Toward a High-Dimensional Ta.pdf}
}

@article{cowenSixteenFacialExpressions2021,
  title = {Sixteen Facial Expressions Occur in Similar Contexts Worldwide},
  author = {Cowen, Alan S. and Keltner, Dacher and Schroff, Florian and Jou, Brendan and Adam, Hartwig and Prasad, Gautam},
  year = {2021},
  month = jan,
  journal = {Nature},
  volume = {589},
  number = {7841},
  pages = {251--257},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-020-3037-7},
  abstract = {Understanding the degree to which human facial expressions co-vary with specific social contexts across cultures is central to the theory that emotions enable adaptive responses to important challenges and opportunities1\textendash 6. Concrete evidence linking social context to specific facial expressions is sparse and is largely based on survey-based approaches, which are often constrained by language and small sample sizes7\textendash 13. Here, by applying machine-learning methods to real-world, dynamic behaviour, we ascertain whether naturalistic social contexts (for example, weddings or sporting competitions) are associated with specific facial expressions14 across different cultures. In two experiments using deep neural networks, we examined the extent to which 16 types of facial expression occurred systematically in thousands of contexts in 6~million videos from 144 countries. We found that each kind of facial expression had distinct associations with a set of contexts that were 70\% preserved across 12~world regions. Consistent with these associations, regions varied in how frequently different facial expressions were produced as a function of which contexts were most salient. Our results reveal fine-grained patterns in human facial expressions that are preserved across the modern world.},
  copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Emotion,Human behaviour},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\GEZP2IBL\\Cowen et al. - 2021 - Sixteen facial expressions occur in similar contex.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\8BI23NRS\\s41586-020-3037-7.html}
}

@article{daiCoAtNetMarryingConvolution2021,
  title = {{{CoAtNet}}: {{Marrying Convolution}} and {{Attention}} for {{All Data Sizes}}},
  shorttitle = {{{CoAtNet}}},
  author = {Dai, Zihang and Liu, Hanxiao and Le, Quoc V. and Tan, Mingxing},
  year = {2021},
  month = sep,
  journal = {arXiv:2106.04803 [cs]},
  eprint = {2106.04803},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Transformers have attracted increasing interests in computer vision, but they still fall behind state-of-the-art convolutional networks. In this work, we show that while Transformers tend to have larger model capacity, their generalization can be worse than convolutional networks due to the lack of the right inductive bias. To effectively combine the strengths from both architectures, we present CoAtNets(pronounced "coat" nets), a family of hybrid models built from two key insights: (1) depthwise Convolution and self-Attention can be naturally unified via simple relative attention; (2) vertically stacking convolution layers and attention layers in a principled way is surprisingly effective in improving generalization, capacity and efficiency. Experiments show that our CoAtNets achieve state-of-the-art performance under different resource constraints across various datasets: Without extra data, CoAtNet achieves 86.0\% ImageNet top-1 accuracy; When pre-trained with 13M images from ImageNet-21K, our CoAtNet achieves 88.56\% top-1 accuracy, matching ViT-huge pre-trained with 300M images from JFT-300M while using 23x less data; Notably, when we further scale up CoAtNet with JFT-3B, it achieves 90.88\% top-1 accuracy on ImageNet, establishing a new state-of-the-art result.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\RPYIGUFK\\Dai et al. - 2021 - CoAtNet Marrying Convolution and Attention for Al.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\HQKXLDER\\2106.html}
}

@article{daleGPT3WhatIt2021,
  title = {{{GPT-3}}: {{What}}'s It Good for?},
  shorttitle = {{{GPT-3}}},
  author = {Dale, Robert},
  year = {2021},
  month = jan,
  journal = {Natural Language Engineering},
  volume = {27},
  number = {1},
  pages = {113--118},
  publisher = {{Cambridge University Press}},
  issn = {1351-3249, 1469-8110},
  doi = {10.1017/S1351324920000601},
  abstract = {GPT-3 made the mainstream media headlines this year, generating far more interest than we'd normally expect of a technical advance in NLP. People are fascinated by its ability to produce apparently novel text that reads as if it was written by a human. But what kind of practical applications can we expect to see, and can they be trusted?},
  langid = {english},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\ASTVPZEF\\Dale - 2021 - GPT-3 What’s it good for.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\Y92Q2MA5\\0E05CFE68A7AC8BF794C8ECBE28AA990.html}
}

@article{dalviSurveyAIbasedFacial2021,
  title = {A {{Survey}} of {{AI-based Facial Emotion Recognition}}: {{Features}}, {{ML}} Amp; {{DL Techniques}}, {{Age-wise Datasets}} and {{Future Directions}}},
  shorttitle = {A {{Survey}} of {{AI-based Facial Emotion Recognition}}},
  author = {Dalvi, Chirag and Rathod, Manish and Patil, Shruti and Gite, Shilpa and Kotecha, Ketan},
  year = {2021},
  journal = {IEEE Access},
  pages = {1--1},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2021.3131733},
  abstract = {Facial expressions are mirrors of human thoughts and feelings. It provides a wealth of social cues to the viewer, including the focus of attention, intention, motivation, and emotion. It is regarded as a potent tool of silent communication. Analysis of these expressions gives a significantly more profound insight into human behavior. AI-based Facial Expression Recognition (FER) has become one of the crucial research topics in recent years, with applications in dynamic analysis, pattern recognition, interpersonal interaction, mental health monitoring, and many more. However, with the global push towards online platforms due to the Covid-19 pandemic, there has been a pressing need to innovate and offer a new FER analysis framework with the increasing visual data generated by videos and photographs.Furthermore, the emotion-wise facial expressions of kids, adults, and senior citizens vary, which must also be considered in the FER research. Lots of research work has been done in this area. However, it lacks a comprehensive overview of the literature that showcases the past work done and provides the aligned future directions. In this paper, the authors have provided a comprehensive evaluation of AI-based FER methodologies, including datasets, feature extraction techniques, algorithms, and the recent breakthroughs with their applications in facial expression identification. To the best of the author's knowledge, this is the only review paper stating all aspects of FER for various age brackets and would significantly impact the research community in the coming years.},
  keywords = {Emotion recognition,Face recognition,Facial Emotion Recognition (FER),Facial Expressions,Feature Extraction,Licenses,Machine Learning,Market research,Psychology,Symbiosis,Videos},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\XS2YSMXN\\Dalvi et al. - 2021 - A Survey of AI-based Facial Emotion Recognition F.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\8UF97EIG\\9631205.html}
}

@inproceedings{darokarMethodologicalReviewEmotion2021,
  title = {Methodological {{Review}} of {{Emotion Recognition}} for {{Social Media}}: {{A Sentiment Analysis Approach}}},
  shorttitle = {Methodological {{Review}} of {{Emotion Recognition}} for {{Social Media}}},
  booktitle = {2021 {{International Conference}} on {{Computing}}, {{Communication}} and {{Green Engineering}} ({{CCGE}})},
  author = {Darokar, Madhavi S. and Raut, Atul D. and Thakre, Vilas M.},
  year = {2021},
  month = sep,
  pages = {1--5},
  doi = {10.1109/CCGE50943.2021.9776385},
  abstract = {Emotion recognition and their analysis have become a very popular topic nowadays, as most of the world using the social media in the form of various applications such as Twitter, Facebook, Whatsapp, Instagram and many more. Also, there are quite a large number of users, who buy the different daily life products through the online shopping websites like Amazon, Flipkart where the online behaviors and emotions of the consumer buying the product is of great interest to the e-commerce industry. In accordance to, the development in the artificial intelligence field, there exist various algorithms that are programmed to analyze the user behavior and trap their emotions through various tools for analyzing the market trends and to increase the percentage of profit. Furthermore, a prolific rate of development is observed in the AI field. This now can be noticed presently, in the form of `Deep learning' where a very huge amount of data is available and the decision-making process is very crucial. If the tremendous amount of data is accessible, ``Machine Learning'' algorithms are of utmost importance.},
  keywords = {Deep learning,Emotion recognition,Emotion Recognition,Face recognition,Facial Expression,Machine learning algorithms,Market research,Multimedia Web sites,Sentiment analysis,Social Network,Social networking (online)},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\HKD49AL7\\Darokar et al. - 2021 - Methodological Review of Emotion Recognition for S.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\YRMU3B8J\\9776385.html}
}

@misc{DarwinClaimUniversals2014,
  title = {Darwin's {{Claim}} of {{Universals}} in {{Facial Expression Not Challenged}}},
  year = {2014},
  month = mar,
  journal = {Paul Ekman Group},
  abstract = {Paul Ekman and Dacher Keltner respond to Lisa Feldman-Barrett's recent article which seeks to undermine Darwin's claim of universals in facial expression.},
  howpublished = {https://www.paulekman.com/blog/darwins-claim-universals-facial-expression-challenged/},
  langid = {american},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\2J33C83X\\darwins-claim-universals-facial-expression-challenged.html}
}

@book{darwinExpressionEmotionsMan1872,
  title = {The Expression of the Emotions in Man and Animals},
  author = {Darwin, Charles},
  year = {1872},
  publisher = {{John Murray}}
}

@article{daudaFacialExpressionRecognition2014,
  title = {Facial Expression Recognition Using {{PCA}} \& Distance Classifier},
  author = {Dauda, AlpeshKumar and Bhoi, Nilamani},
  year = {2014},
  journal = {Int J Sci Eng Res},
  volume = {5}
}

@article{davenportHowArtificialIntelligence2020,
  title = {How Artificial Intelligence Will Change the Future of Marketing},
  author = {Davenport, Thomas and Guha, Abhijit and Grewal, Dhruv and Bressgott, Timna},
  year = {2020},
  journal = {Journal of the Academy of Marketing Science},
  volume = {48},
  number = {1},
  pages = {24--42},
  issn = {1552-7824},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\Z8V6QC6H\\Davenport2020_Article_HowArtificialIntelligenc.pdf}
}

@article{davoliDriverBehaviorRecognition2020,
  title = {On {{Driver Behavior Recognition}} for {{Increased Safety}}: {{A Roadmap}}},
  author = {Davoli, L. and Martalo, M. and Cilfone, A. and Belli, L. and Ferrari, G. and Presta, R. and Montanari, R. and Mengoni, M. and Giraldi, L. and Amparore, E. G. and Botta, M. and Drago, I. and Carbonara, G. and Castellano, A. and Plomp, J.},
  year = {2020},
  month = dec,
  journal = {Safety},
  volume = {6},
  number = {4},
  doi = {10.3390/safety6040055},
  abstract = {Advanced Driver-Assistance Systems (ADASs) are used for increasing safety in the automotive domain, yet current ADASs notably operate without taking into account drivers' states, e.g., whether she/he is emotionally apt to drive. In this paper, we first review the state-of-the-art of emotional and cognitive analysis for ADAS: we consider psychological models, the sensors needed for capturing physiological signals, and the typical algorithms used for human emotion classification. Our investigation highlights a lack of advanced Driver Monitoring Systems (DMSs) for ADASs, which could increase driving quality and security for both drivers and passengers. We then provide our view on a novel perception architecture for driver monitoring, built around the concept of Driver Complex State (DCS). DCS relies on multiple non-obtrusive sensors and Artificial Intelligence (AI) for uncovering the driver state and uses it to implement innovative Human-Machine Interface (HMI) functionalities. This concept will be implemented and validated in the recently EU-funded NextPerception project, which is briefly introduced.}
}

@article{dawelPerceivedEmotionGenuineness2017,
  title = {Perceived Emotion Genuineness: Normative Ratings for Popular Facial Expression Stimuli and the Development of Perceived-as-Genuine and Perceived-as-Fake Sets},
  shorttitle = {Perceived Emotion Genuineness},
  author = {Dawel, Amy and Wright, Luke and Irons, Jessica and Dumbleton, Rachael and Palermo, Romina and O'Kearney, Richard and McKone, Elinor},
  year = {2017},
  journal = {Behavior research methods},
  volume = {49},
  number = {4},
  pages = {1539--1562},
  publisher = {{Springer}},
  note = {Belongs to KDEF-cropped data set},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\AK7GLEIG\\s13428-016-0813-2.html}
}

@inproceedings{dengPerceivingVisualEmotions2006,
  title = {Perceiving Visual Emotions with Speech},
  booktitle = {International {{Workshop}} on {{Intelligent Virtual Agents}}},
  author = {Deng, Zhigang and Bailenson, Jeremy and Lewis, John P. and Neumann, Ulrich},
  year = {2006},
  pages = {107--120},
  publisher = {{Springer}},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\VK9GXIE4\\Deng et al. - 2006 - Perceiving visual emotions with speech.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\8AAQXBXY\\11821830_9.html}
}

@misc{DEPATISnetDEPATISnetStartseite,
  title = {{{DEPATISnet}} | {{DEPATISnet-Startseite}}},
  howpublished = {https://depatisnet.dpma.de/DepatisNet/depatisnet?window=1\&space=menu\&content=index\&action=index},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\7DJ9FEWG\\depatisnet.html}
}

@article{deraedtStatisticalRelationalNeuroSymbolic2020,
  title = {From {{Statistical Relational}} to {{Neuro-Symbolic Artificial Intelligence}}},
  author = {De Raedt, Luc and Duman{\v c}i{\'c}, Sebastijan and Manhaeve, Robin and Marra, Giuseppe},
  year = {2020},
  month = mar,
  journal = {arXiv:2003.08316 [cs]},
  eprint = {2003.08316},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Neuro-symbolic and statistical relational artificial intelligence both integrate frameworks for learning with logical reasoning. This survey identifies several parallels across seven different dimensions between these two fields. These cannot only be used to characterize and position neuro-symbolic artificial intelligence approaches but also to identify a number of directions for further research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  note = {(Very) Important
\par
See 9. Open Challenges
\par
several parallels across seven different dimensions between these two fields (=Neuro-symbolic and statistical relational artificial intelligence)},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\2HLJSFZI\\De Raedt et al. - 2020 - From Statistical Relational to Neuro-Symbolic Arti.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\ESXKTAQW\\2003.html}
}

@article{deviyaniAssessingDatasetBias2021,
  title = {Assessing {{Dataset Bias}} in {{Computer Vision}}},
  author = {Deviyani, Athiya},
  year = {2021},
  eprint = {2205.01811},
  eprinttype = {arxiv},
  primaryclass = {cs},
  doi = {10.13140/RG.2.2.19950.89924},
  abstract = {A biased dataset is a dataset that generally has attributes with an uneven class distribution. These biases have the tendency to propagate to the models that train on them, often leading to a poor performance in the minority class. In this project, we will explore the extent to which various data augmentation methods alleviate intrinsic biases within the dataset. We will apply several augmentation techniques on a sample of the UTKFace dataset, such as undersampling, geometric transformations, variational autoencoders (VAEs), and generative adversarial networks (GANs). We then trained a classifier for each of the augmented datasets and evaluated their performance on the native test set and on external facial recognition datasets. We have also compared their performance to the state-of-the-art attribute classifier trained on the FairFace dataset. Through experimentation, we were able to find that training the model on StarGAN-generated images led to the best overall performance. We also found that training on geometrically transformed images lead to a similar performance with a much quicker training time. Additionally, the best performing models also exhibit a uniform performance across the classes within each attribute. This signifies that the model was also able to mitigate the biases present in the baseline model that was trained on the original training set. Finally, we were able to show that our model has a better overall performance and consistency on age and ethnicity classification on multiple datasets when compared with the FairFace model. Our final model has an accuracy on the UTKFace test set of 91.75\%, 91.30\%, and 87.20\% for the gender, age, and ethnicity attribute respectively, with a standard deviation of less than 0.1 between the accuracies of the classes of each attribute.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  note = {Comment: 51 pages},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\R7YR5Y3C\\Deviyani - 2021 - Assessing Dataset Bias in Computer Vision.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\MYXC5WFR\\2205.html}
}

@article{dhallCollectingLargeRichly2012,
  title = {Collecting {{Large}}, {{Richly Annotated Facial-Expression Databases}} from {{Movies}}},
  author = {Dhall, Abhinav and Goecke, Roland and Lucey, Simon and Gedeon, Tom},
  year = {2012},
  month = jul,
  journal = {IEEE MultiMedia},
  volume = {19},
  number = {3},
  pages = {34--41},
  issn = {1941-0166},
  doi = {10.1109/MMUL.2012.26},
  abstract = {Two large facial-expression databases depicting challenging real-world conditions were constructed using a semi-automatic approach via a recommender system based on subtitles.},
  keywords = {Databases,emotion database,Face recognition,facial expression recognition,Facial features,Image retrieval,large-scale database,large-scale multimedia data,Large-scale systems,Motion pictures,multimedia,real-world conditions},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\GJYT8YRH\\Dhall et al. - 2012 - Collecting Large, Richly Annotated Facial-Expressi.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\HX8E9HYJ\\6200254.html}
}

@inproceedings{dhallEmotionRecognitionWild2014,
  title = {Emotion Recognition in the Wild Challenge 2014: {{Baseline}}, Data and Protocol},
  shorttitle = {Emotion Recognition in the Wild Challenge 2014},
  booktitle = {Proceedings of the 16th International Conference on Multimodal Interaction},
  author = {Dhall, Abhinav and Goecke, Roland and Joshi, Jyoti and Sikka, Karan and Gedeon, Tom},
  year = {2014},
  pages = {461--466},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\V2ZAPPWD\\Dhall et al. - 2014 - Emotion recognition in the wild challenge 2014 Ba.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\33DUIL5I\\2663204.html}
}

@inproceedings{dhallStaticFacialExpressions2011,
  title = {Static Facial Expressions in Tough Conditions: {{Data}}, Evaluation Protocol and Benchmark},
  shorttitle = {Static Facial Expressions in Tough Conditions},
  booktitle = {1st {{IEEE International Workshop}} on {{Benchmarking Facial Image Analysis Technologies BeFIT}}, {{ICCV2011}}},
  author = {Dhall, Abhinav and Goecke, Roland and Lucey, Simon and Gedeon, Tom},
  year = {2011},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\9UBEBGH6\\Dhall et al. - 2011 - Static facial expressions in tough conditions Dat.pdf}
}

@inproceedings{dhopeRealTimeEmotionRecognition2022,
  title = {Real-{{Time Emotion Recognition}} from {{Facial Expressions}} Using {{Artificial Intelligence}}},
  booktitle = {2022 2nd {{International Conference}} on {{Artificial Intelligence}} and {{Signal Processing}} ({{AISP}})},
  author = {Dhope, Prashant and Neelagar, Mahesh B.},
  year = {2022},
  month = feb,
  pages = {1--6},
  issn = {2640-5768},
  doi = {10.1109/AISP53593.2022.9760654},
  abstract = {Emotion is the most important factor that distinguishes humans from robots. Machines are becoming more aware of human emotions as artificial intelligence advances. The objective of proposed method is to use artificial intelligence to build and construct a real-time facial emotion identification system. The proposed methodology has the capability of recognizing all the seven fundamental human face emotions. Those are angry, disgust, fear, happy, neutral, sad, and surprise. A self-prepared dataset is utilized to train the algorithm. The model is trained and facial expressions are recognized using a convolutional neural network. The real-time testing is accomplished using the Raspberry Pi 3B+ board and Pi-Camera. Using PyQt5, graphical user interface (GUI) is created for the system. The experimental result shows that, the proposed methodology has high recognition accuracy rate up to 99.88\%.},
  keywords = {Artificial intelligence,Artificial Intelligence,Convolutional Neural Network,Deep Learning,Emotion,Emotion recognition,Face recognition,Facial Expressions,Object recognition,Real-time,Real-time systems,Recognition,Signal processing,Signal processing algorithms},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\TWS6AEXY\\Dhope und Neelagar - 2022 - Real-Time Emotion Recognition from Facial Expressi.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\JQBJPGM2\\9760654.html}
}

@article{dosovitskiyImageWorth16x162021,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  year = {2021},
  month = jun,
  journal = {arXiv:2010.11929 [cs]},
  eprint = {2010.11929},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  note = {Comment: Fine-tuning code and pre-trained models are available at https://github.com/google-research/vision\_transformer. ICLR camera-ready version with 2 small modifications: 1) Added a discussion of CLS vs GAP classifier in the appendix, 2) Fixed an error in exaFLOPs computation in Figure 5 and Table 6 (relative performance of models is basically not affected)},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\IRTRRNKH\\Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Im.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\W9ZRZ8FE\\2010.html}
}

@inproceedings{duImpactionArtificialIntelligence2021a,
  title = {Impaction of {{Artificial Intelligence}} on {{Interaction Design}}},
  booktitle = {2021 2nd {{International Conference}} on {{Computer Engineering}} and {{Intelligent Control}} ({{ICCEIC}})},
  author = {Du, Jiamei},
  year = {2021},
  month = nov,
  pages = {187--191},
  doi = {10.1109/ICCEIC54227.2021.00044},
  abstract = {In recent years, artificial intelligence (AI) technology has made rapid development. Speech recognition, face recognition, natural language processing, limb recognition and other technologies are becoming more and more mature. The development of human technology has constantly changed the interaction mode between people and computers. For example, the maturity of touch screen technology has brought the popularity of tablet computers. The theme of this study is to investigate the impact of artificial intelligence (AI) technology on interaction design and how to use the interaction mode of AI technology to improve user experience. The research method of this paper is through the investigation of literature and face recognition experiments. The research results show that AI technology has a great impact on interaction design. Chat robot and speech recognition have been applied in many interaction designs. The recognition of facial expressions and human body movements can recognize more detailed feature information (such as expression, mental state, skin state, etc.), open more interaction space and provide users with better user experience.},
  keywords = {Artificial Intelligence,Emotion AI,Emotion recognition,Face recognition,Interaction Design,Olfactory,Speech recognition,Tablet computers,Touch sensitive screens,User Experience,UX,Visualization},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\876A5PYZ\\Du - 2021 - Impaction of Artificial Intelligence on Interactio.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\6Z9P39UK\\9692070.html}
}

@article{ekmanBasicEmotionsHandbook1999,
  title = {Basic Emotions. {{Handbook}} of Cognition and Emotion},
  author = {Ekman, Paul},
  year = {1999},
  journal = {Wiley, New York},
  pages = {301--320},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\CK7VXL5F\\Ekman - 1999 - Basic emotions.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\GRN25LWB\\books.html}
}

@misc{EkmanConstantsCultures,
  title = {Ekman: {{Constants}} across Cultures in the Face and Emotion. - {{Google Scholar}}},
  howpublished = {https://scholar.google.com/scholar\_lookup?title=\&journal=J.\%20Personal.\%20Soc.\%20Psychol.\&volume=17\&pages=124-129\&publication\_year=1971\&author=Ekman\%2CP.\&author=Friesen\%2CW.\%20V.},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\8ME973IW\\scholar_lookup.html}
}

@article{ekmanConstantsCulturesFace1971,
  title = {Constants across Cultures in the Face and Emotion.},
  author = {Ekman, Paul and Friesen, Wallace V.},
  year = {1971},
  journal = {Journal of personality and social psychology},
  volume = {17},
  number = {2},
  pages = {124},
  publisher = {{American Psychological Association}},
  note = {Emotion Theory Concepts, for instance ekman and pleasure-arousal-dominance framework (PAD) or newer concepts like Plutchnik model},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\BUF8BVFH\\Ekman und Friesen - 1971 - Constants across cultures in the face and emotion..pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\FMSA69KM\\1971-07999-001.html}
}

@misc{ekmanDarwinClaimUniversals2014,
  title = {Darwin's {{Claim}} of {{Universals}} in {{Facial Expression Not Challenged}}},
  author = {Ekman, Paul and Keltner, Dacher},
  year = {2014},
  month = mar,
  journal = {Paul Ekman Group},
  abstract = {Paul Ekman and Dacher Keltner respond to Lisa Feldman-Barrett's recent article which seeks to undermine Darwin's claim of universals in facial expression.},
  howpublished = {https://www.paulekman.com/blog/darwins-claim-universals-facial-expression-challenged/},
  langid = {american},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\ARLEKHSE\\darwins-claim-universals-facial-expression-challenged.html}
}

@article{ekmanExpressionNatureEmotion1984,
  title = {Expression and the Nature of Emotion},
  author = {Ekman, Paul},
  year = {1984},
  journal = {Approaches to emotion},
  volume = {3},
  number = {19},
  pages = {344},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\93UQZDFA\\books.html}
}

@book{ekmanFacialActionCoding2002,
  title = {Facial Action Coding Sytem},
  author = {Ekman, Paul and Friesen, Wallace V and Hager, Joseph C},
  year = {2002},
  publisher = {{A Human Face}},
  address = {{Salt Lake City, Utah}},
  abstract = {Contains the Manaul for the Facial Action Coding System, the Investigator's Guide to FACS, the checker program for practice scoring, and associated multimedia files.},
  isbn = {978-0-931835-01-8},
  langid = {english},
  annotation = {OCLC: 277599050}
}

@article{ekmanFacsManual2002,
  title = {Facs Manual},
  author = {Ekman, Paul and Friesen, W. V. and Hager, J. C.},
  year = {2002},
  journal = {A human face}
}

@misc{EkmanPanculturalElements,
  title = {Ekman: {{Pan-cultural}} Elements in Facial Displays of Emotion - {{Google Scholar}}},
  howpublished = {https://scholar.google.com/scholar\_lookup?title=\&journal=Science\&volume=164\&pages=86-88\&publication\_year=1969\&author=Ekman\%2CP.\&author=Sorenson\%2CE.\%20R.\&author=Friesen\%2CW.\%20V.},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\U8R3XNL7\\Ekman Pan-cultural elements in facial displays of.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\KXU8UBIF\\scholar_lookup.html}
}

@book{ekmanUnmaskingFaceGuide2003,
  title = {Unmasking the Face: {{A}} Guide to Recognizing Emotions from Facial Clues},
  shorttitle = {Unmasking the Face},
  author = {Ekman, Paul and Friesen, Wallace V.},
  year = {2003},
  volume = {10},
  publisher = {{Ishk}},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\ATEXQ2WH\\books.html}
}

@article{ekundayoFacialExpressionRecognition2021,
  title = {Facial {{Expression Recognition}}: {{A Review}} of {{Trends}} and {{Techniques}}},
  shorttitle = {Facial {{Expression Recognition}}},
  author = {Ekundayo, Olufisayo S. and Viriri, Serestina},
  year = {2021},
  journal = {Ieee Access},
  volume = {9},
  pages = {136944--136973},
  publisher = {{Ieee-Inst Electrical Electronics Engineers Inc}},
  address = {{Piscataway}},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2021.3113464},
  abstract = {Facial Expression Recognition (FER) is presently the aspect of cognitive and affective computing with the most attention and popularity, aided by its vast application areas. Several studies have been conducted on FER, and many review works are also available. The existing FER review works only give an account of FER models capable of predicting the basic expressions. None of the works considers intensity estimation of an emotion; neither do they include studies that address data annotation inconsistencies and correlation among labels in their works. This work first introduces some identified FER application areas and provides a discussion on recognised FER challenges. We proceed to provide a comprehensive FER review in three different machine learning problem definitions: Single Label Learning (SLL)- which presents FER as a multiclass problem, Multilabel Learning (MLL)- that resolves the ambiguity nature of FER, and Label Distribution Learning- that recovers the distribution of emotion in FER data annotation. We also include studies on expression intensity estimation from the face. Furthermore, popularly employed FER models are thoroughly and carefully discussed in handcrafted, conventional machine learning and deep learning models. We finally itemise some recognise unresolved issues and also suggest future research areas in the field.},
  langid = {english},
  keywords = {Annotations,Datasets,deep,Deep   learning,Deep learning,Education,emotion,ensemble,face,Face recognition,Facial expression recognition,intensity estimation,label distribution learning,local binary pattern,multiclass,multilabel label learning,network,normalization,read,Security,single label   learning,single label learning,Software,Task analysis},
  annotation = {WOS:000706816700001},
  note = {Overview of FER databases on p. 9
\par
Summary of popular Deep CNN
\par
Summary of experimental results for different models and datasets},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\VRUHBL2M\\Ekundayo und Viriri - 2021 - Facial Expression Recognition A Review of Trends .pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\ZJ32DGKC\\9540650.html}
}

@inproceedings{ekweaririFacialExpressionRecognition2017,
  title = {Facial Expression Recognition Using Enhanced Local Binary Patterns},
  booktitle = {2017 9th International Conference on {{Computational Intelligence}} and {{Communication Networks}} ({{CICN}})},
  author = {Ekweariri, Augustine Nnamdi and Yurtkan, Kamil},
  year = {2017},
  pages = {43--47},
  publisher = {{IEEE}},
  keywords = {read},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\DQKJ6BBY\\Ekweariri und Yurtkan - 2017 - Facial expression recognition using enhanced local.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\WYR6FQ7C\\8319353.html}
}

@misc{ElfenbeinUniversalityCultural,
  title = {Elfenbein: {{On}} the Universality and Cultural Specificity... - {{Google Scholar}}},
  howpublished = {https://scholar.google.com/scholar\_lookup?title=\&journal=Psychol.\%20Bull.\&volume=128\&pages=203-235\&publication\_year=2002\&author=Elfenbein\%2CH.\%20A.\&author=Ambady\%2CN.},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\HUCQAF6B\\scholar_lookup.html}
}

@article{elfenbeinUniversalityCulturalSpecificity2002,
  title = {On the Universality and Cultural Specificity of Emotion Recognition: {{A}} Meta-Analysis},
  shorttitle = {On the Universality and Cultural Specificity of Emotion Recognition},
  author = {Elfenbein, Hillary Anger and Ambady, Nalini},
  year = {2002},
  journal = {Psychological Bulletin},
  volume = {128},
  number = {2},
  pages = {203--235},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1455},
  doi = {10.1037/0033-2909.128.2.203},
  abstract = {A meta-analysis examined emotion recognition within and across cultures. Emotions were universally recognized at better-than-chance levels. Accuracy was higher when emotions were both expressed and recognized by members of the same national, ethnic, or regional group, suggesting an in-group advantage. This advantage was smaller for cultural groups with greater exposure to one another, measured in terms of living in the same nation, physical proximity, and telephone communication. Majority group members were poorer at judging minority group members than the reverse. Cross-cultural accuracy was lower in studies that used a balanced research design, and higher in studies that used imitation rather than posed or spontaneous emotional expressions. Attributes of study design appeared not to moderate the size of the in-group advantage. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Cross Cultural Differences,Emotion Recognition,Emotional States,Nonverbal Communication},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\PE7WE62D\\Elfenbein On the universality and cultural specif.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\2C3IMFJA\\2002-00947-001.html}
}

@article{eliotEmotionTheoryEducation2019,
  title = {Emotion Theory in Education Research Practice: {{An}} Interdisciplinary Critical Literature Review},
  shorttitle = {Emotion Theory in Education Research Practice},
  author = {Eliot, Joy AR and Hirumi, Atsusi},
  year = {2019},
  journal = {Educational technology research and development},
  volume = {67},
  number = {5},
  pages = {1065--1084},
  publisher = {{Springer}},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\R4TNFKUM\\Eliot und Hirumi - 2019 - Emotion theory in education research practice An .pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\RQY2IAS4\\s11423-018-09642-3.html}
}

@article{ericssonSelfSupervisedRepresentationLearning2021,
  title = {Self-{{Supervised Representation Learning}}: {{Introduction}}, {{Advances}} and {{Challenges}}},
  shorttitle = {Self-{{Supervised Representation Learning}}},
  author = {Ericsson, Linus and Gouk, Henry and Loy, Chen Change and Hospedales, Timothy M.},
  year = {2021},
  month = oct,
  journal = {arXiv:2110.09327 [cs, stat]},
  eprint = {2110.09327},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Self-supervised representation learning methods aim to provide powerful deep feature learning without the requirement of large annotated datasets, thus alleviating the annotation bottleneck that is one of the main barriers to practical deployment of deep learning today. These methods have advanced rapidly in recent years, with their efficacy approaching and sometimes surpassing fully supervised pre-training alternatives across a variety of data modalities including image, video, sound, text and graphs. This article introduces this vibrant area including key concepts, the four main families of approach and associated state of the art, and how self-supervised methods are applied to diverse modalities of data. We further discuss practical considerations including workflows, representation transferability, and compute cost. Finally, we survey the major open challenges in the field that provide fertile ground for future work.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\4GZMT8Y7\\Ericsson et al. - 2021 - Self-Supervised Representation Learning Introduct.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\D2B7SWMH\\2110.html}
}

@misc{europeancomissionProposalRegulationLaying2021,
  title = {Proposal for a {{Regulation}} Laying down Harmonised Rules on Artificial Intelligence ({{Artificial Intelligence Act}})},
  author = {European Comission},
  year = {2021},
  note = {The following values have no corresponding Zotero field:\\
number: 19.05.2021},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\XTWV28RP\\European_Comission.pdf}
}

@inproceedings{fabianbenitez-quirozEmotionetAccurateRealtime2016,
  title = {Emotionet: {{An}} Accurate, Real-Time Algorithm for the Automatic Annotation of a Million Facial Expressions in the Wild},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {{Fabian Benitez-Quiroz}, C. and Srinivasan, Ramprakash and Martinez, Aleix M.},
  year = {2016},
  pages = {5562--5570},
  keywords = {Datasets},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\FSFQGJC4\\EmotioNet_An_Accurate_CVPR_2016_paper.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\NG7CJEB6\\Fabian Benitez-Quiroz et al. - 2016 - Emotionet An accurate, real-time algorithm for th.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\QS5Y727H\\Benitez-Quiroz_EmotioNet_An_Accurate_CVPR_2016_paper.html}
}

@inproceedings{fanLearningLongtermRepresentations2020,
  title = {Learning Longterm Representations for Person Re-Identification Using Radio Signals},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Fan, Lijie and Li, Tianhong and Fang, Rongyao and Hristov, Rumen and Yuan, Yuan and Katabi, Dina},
  year = {2020},
  pages = {10699--10709},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\FT8MG8LD\\Learning Longterm Representations for Person R.pdf}
}

@inproceedings{faragherAnalysisAccuracyBluetooth2014,
  title = {An Analysis of the Accuracy of Bluetooth Low Energy for Indoor Positioning Applications},
  booktitle = {Proceedings of the 27th {{International Technical Meeting}} of {{The Satellite Division}} of the {{Institute}} of {{Navigation}} ({{ION GNSS}}+ 2014)},
  author = {Faragher, Ramsey and Harle, Robert},
  year = {2014},
  pages = {201--210},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\4D4DBKGW\\abstract.html}
}

@inproceedings{faragherSmartSLAManEfficientSmartphone2013,
  title = {{{SmartSLAM-an}} Efficient Smartphone Indoor Positioning System Exploiting Machine Learning and Opportunistic Sensing},
  booktitle = {Proceedings of the 26th {{International Technical Meeting}} of {{The Satellite Division}} of the {{Institute}} of {{Navigation}} ({{ION GNSS}}+ 2013)},
  author = {Faragher, R. M. and Harle, R. K.},
  year = {2013},
  pages = {1006--1019},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\2IC5Q9DC\\Faragher und Harle - 2013 - SmartSLAM-an efficient smartphone indoor positioni.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\FCJHTZGK\\abstract.html}
}

@article{faridRecentAdvancesWireless2013,
  title = {Recent {{Advances}} in {{Wireless Indoor Localization Techniques}} and {{System}}},
  author = {Farid, Zahid and Nordin, Rosdiadee and Ismail, Mahamod},
  year = {2013},
  month = sep,
  journal = {Journal of Computer Networks and Communications},
  volume = {2013},
  pages = {e185138},
  publisher = {{Hindawi}},
  issn = {2090-7141},
  doi = {10.1155/2013/185138},
  abstract = {The advances in localization based technologies and the increasing importance of ubiquitous computing and context-dependent information have led to a growing business interest in location-based applications and services. Today, most application requirements are locating or real-time tracking of physical belongings inside buildings accurately; thus, the demand for indoor localization services has become a key prerequisite in some markets. Moreover, indoor localization technologies address the inadequacy of global positioning system inside a closed environment, like buildings. Based on this, though, this paper aims to provide the reader with a review of the recent advances in wireless indoor localization techniques and system to deliver a better understanding of state-of-the-art technologies and motivate new research efforts in this promising field. For this purpose, existing wireless localization position system and location estimation schemes are reviewed, as we also compare the related techniques and systems along with a conclusion and future trends.},
  langid = {english},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\A8GZGK9H\\Farid et al. - 2013 - Recent Advances in Wireless Indoor Localization Te.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\JMK9C27I\\185138.html}
}

@misc{FeinaugleDistanceLearning,
  title = {Fein\"augle: {{Distance}} Learning from the European Patent... - {{Google Scholar}}},
  howpublished = {https://scholar.google.com/scholar\_lookup?title=Distance\%20learning\%20from\%20the\%20european\%20patent\%20office\&author=R.\%20Feinaeugle\&publication\_year=2006\&pages=63-74},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\PQU7RXNH\\scholar_lookup.html}
}

@article{feinaugleDistanceLearningEuropean2006,
  title = {Distance Learning from the {{European Patent Office}}},
  author = {Fein{\"a}ugle, Roland},
  year = {2006},
  month = mar,
  journal = {World Patent Information},
  volume = {28},
  number = {1},
  pages = {63--74},
  issn = {0172-2190},
  doi = {10.1016/j.wpi.2005.07.012},
  abstract = {In the light of the changes in the way that patent information is delivered, the way that learning about patent information and related topics occurs is also changing. Accordingly, the EPO has recently developed electronic and distance learning materials on patent information products and related topics to complement the existing classroom training programme. A user survey on their training needs confirmed that these developments are in-line with the users' interests. The first initiative is a series of online self-study modules to help users to make best use of esp@cenet\textregistered. This series of short, interactive modules is targeted at new and occasional users and introduces the basic functionalities of esp@cenet. Secondly, the EPO has initiated a series of virtual classroom seminars with a pilot event in March 2004. In these live, interactive and moderator-guided training sessions participants can ask questions in oral and written form while following the explanations and live searches on their screens. By the end of 2004 more than 270 users had already participated in virtual seminars on various subjects related to patent information.},
  langid = {english},
  keywords = {assistant,Asynchronous learning,Blended learning,Computer-based training,Distance learning,Education,EPO,European Patent Office,Interactive,Learning,Moderator,Online,Patent information,Self-paced learning,Self-study,Synchronous learning,Trainer,Training,Tutor-lead training,Virtual classroom,Web-based training},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\C7X6HSX2\\S0172219005001213.html}
}

@article{feurerEfficientRobustAutomated2015,
  title = {Efficient and Robust Automated Machine Learning},
  author = {Feurer, Matthias and Klein, Aaron and Eggensperger, Katharina and Springenberg, Jost and Blum, Manuel and Hutter, Frank},
  year = {2015},
  journal = {Advances in neural information processing systems},
  volume = {28},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\WGVTTT9F\\Feurer et al. - 2015 - Efficient and robust automated machine learning.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\FJEA7H4A\\11d0e6287202fced83f79975ec59a3a6-Abstract.html}
}

@article{floridiGPT3ItsNature2020,
  title = {{{GPT-3}}: {{Its Nature}}, {{Scope}}, {{Limits}}, and {{Consequences}}},
  author = {Floridi, Luciano and Chiriatti, Massimo},
  year = {2020},
  month = dec,
  journal = {Minds and Machines},
  volume = {30},
  number = {4},
  pages = {681--694},
  issn = {1572-8641},
  doi = {10.1007/s11023-020-09548-1},
  abstract = {In this commentary, we discuss the nature of reversible and irreversible questions, that is, questions that may enable one to identify the nature of the source of their answers. We then introduce GPT-3, a third-generation, autoregressive language model that uses deep learning to produce human-like texts, and use the previous distinction to analyse it. We expand the analysis to present three tests based on mathematical, semantic (that is, the Turing Test), and ethical questions and show that GPT-3 is not designed to pass any of them. This is a reminder that GPT-3 does not do what it is not supposed to do, and that any interpretation of GPT-3 as the beginning of the emergence of a general form of artificial intelligence is merely uninformed science fiction. We conclude by outlining some of the significant consequences of the industrialisation of automatic and cheap production of good, semantic artefacts.},
  langid = {english},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\JMYTPU9Z\\Floridi und Chiriatti - 2020 - GPT-3 Its Nature, Scope, Limits, and Consequences.pdf}
}

@article{frasconiGeneralFrameworkAdaptive1998,
  title = {A General Framework for Adaptive Processing of Data Structures},
  author = {Frasconi, P. and Gori, M. and Sperduti, A.},
  year = {1998},
  month = sep,
  journal = {IEEE Transactions on Neural Networks},
  volume = {9},
  number = {5},
  pages = {768--786},
  issn = {1941-0093},
  doi = {10.1109/72.712151},
  abstract = {A structured organization of information is typically required by symbolic processing. On the other hand, most connectionist models assume that data are organized according to relatively poor structures, like arrays or sequences. The framework described in this paper is an attempt to unify adaptive models like artificial neural nets and belief nets for the problem of processing structured information. In particular, relations between data variables are expressed by directed acyclic graphs, where both numerical and categorical values coexist. The general framework proposed in this paper can be regarded as an extension of both recurrent neural networks and hidden Markov models to the case of acyclic graphs. In particular we study the supervised learning problem as the problem of learning transductions from an input structured space to an output structured space, where transductions are assumed to admit a recursive hidden state-space representation. We introduce a graphical formalism for representing this class of adaptive transductions by means of recursive networks, i.e., cyclic graphs where nodes are labeled by variables and edges are labeled by generalized delay elements. This representation makes it possible to incorporate the symbolic and subsymbolic nature of data. Structures are processed by unfolding the recursive network into an acyclic graph called encoding network. In so doing, inference and learning algorithms can be easily inherited from the corresponding algorithms for artificial neural networks or probabilistic graphical model.},
  keywords = {Artificial neural networks,Data structures,Encoding,Graphical models,Hidden Markov models,Inference algorithms,Neural networks,Problem-solving,Recurrent neural networks,Supervised learning},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\PR5XNZ4N\\Frasconi et al. - 1998 - A general framework for adaptive processing of dat.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\D2Z39IBG\\712151.html}
}

@article{friesenFacialActionCoding1978,
  title = {Facial Action Coding System: A Technique for the Measurement of Facial Movement},
  shorttitle = {Facial Action Coding System},
  author = {Friesen, E. and Ekman, Paul},
  year = {1978},
  journal = {Palo Alto},
  volume = {3},
  number = {2},
  pages = {5},
  publisher = {{Consulting Psychologists Press}}
}

@inproceedings{fuCMNASCrossModalityNeural2021,
  title = {{{CM-NAS}}: {{Cross-Modality Neural Architecture Search}} for {{Visible-Infrared Person Re-Identification}}},
  shorttitle = {{{CM-NAS}}},
  booktitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Fu, Chaoyou and Hu, Yibo and Wu, Xiang and Shi, Hailin and Mei, Tao and He, Ran},
  year = {2021},
  month = oct,
  pages = {11803--11812},
  issn = {2380-7504},
  doi = {10.1109/ICCV48922.2021.01161},
  abstract = {Visible-Infrared person re-identification (VI-ReID) aims to match cross-modality pedestrian images, breaking through the limitation of single-modality person ReID in dark environment. In order to mitigate the impact of large modality discrepancy, existing works manually design various two-stream architectures to separately learn modality-specific and modality-sharable representations. Such a manual design routine, however, highly depends on massive experiments and empirical practice, which is time consuming and labor intensive. In this paper, we systematically study the manually designed architectures, and identify that appropriately separating Batch Normalization (BN) layers is the key to bring a great boost towards cross-modality matching. Based on this observation, the essential objective is to find the optimal separation scheme for each BN layer. To this end, we propose a novel method, named Cross-Modality Neural Architecture Search (CM-NAS). It consists of a BN-oriented search space in which the standard optimization can be fulfilled subject to the cross-modality task. Equipped with the searched architecture, our method outperforms state-of-the-art counter-parts in both two benchmarks, improving the Rank-1/mAP by 6.70\%/6.13\% on SYSU-MM01 and by 12.17\%/11.23\% on RegDB. Code is released at https://github.com/JDAI-CV/CM-NAS.},
  keywords = {Benchmark testing,Codes,Computer architecture,Computer vision,Image and video retrieval,Manuals,Solids,Task analysis},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\95VGWZA7\\Fu et al. - 2021 - CM-NAS Cross-Modality Neural Architecture Search .pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\G86E3VNQ\\9710774.html}
}

@article{fuentes-pachecoVisualSimultaneousLocalization2015,
  title = {Visual Simultaneous Localization and Mapping: A Survey},
  shorttitle = {Visual Simultaneous Localization and Mapping},
  author = {{Fuentes-Pacheco}, Jorge and {Ruiz-Ascencio}, Jos{\'e} and {Rend{\'o}n-Mancha}, Juan Manuel},
  year = {2015},
  month = jan,
  journal = {Artificial Intelligence Review},
  volume = {43},
  number = {1},
  pages = {55--81},
  issn = {1573-7462},
  doi = {10.1007/s10462-012-9365-8},
  abstract = {Visual SLAM (simultaneous localization and mapping) refers to the problem of using images, as the only source of external information, in order to establish the position of a robot, a vehicle, or a moving camera in an environment, and at the same time, construct a representation of the explored zone. SLAM is an essential task for the autonomy of a robot. Nowadays, the problem of SLAM is considered solved when range sensors such as lasers or sonar are used to built 2D maps of small static environments. However SLAM for dynamic, complex and large scale environments, using vision as the sole external sensor, is an active area of research. The computer vision techniques employed in visual SLAM, such as detection, description and matching of salient features, image recognition and retrieval, among others, are still susceptible of improvement. The objective of this article is to provide new researchers in the field of visual SLAM a brief and comprehensible review of the state-of-the-art.},
  langid = {english},
  keywords = {Data association,Image matching,Salient feature selection,Topological and metric maps,Visual SLAM},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\YK39W5SL\\Fuentes-Pacheco et al. - 2015 - Visual simultaneous localization and mapping a su.pdf}
}

@misc{gajaneSurveyFairReinforcement2022,
  title = {Survey on {{Fair Reinforcement Learning}}: {{Theory}} and {{Practice}}},
  shorttitle = {Survey on {{Fair Reinforcement Learning}}},
  author = {Gajane, Pratik and Saxena, Akrati and Tavakol, Maryam and Fletcher, George and Pechenizkiy, Mykola},
  year = {2022},
  month = may,
  number = {arXiv:2205.10032},
  eprint = {2205.10032},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  abstract = {Fairness-aware learning aims at satisfying various fairness constraints in addition to the usual performance criteria via data-driven machine learning techniques. Most of the research in fairness-aware learning employs the setting of fair-supervised learning. However, many dynamic real-world applications can be better modeled using sequential decision-making problems and fair reinforcement learning provides a more suitable alternative for addressing these problems. In this article, we provide an extensive overview of fairness approaches that have been implemented via a reinforcement learning (RL) framework. We discuss various practical applications in which RL methods have been applied to achieve a fair solution with high accuracy. We further include various facets of the theory of fair reinforcement learning, organizing them into single-agent RL, multi-agent RL, long-term fairness via RL, and offline learning. Moreover, we highlight a few major issues to explore in order to advance the field of fair-RL, namely - i) correcting societal biases, ii) feasibility of group fairness or individual fairness, and iii) explainability in RL. Our work is beneficial for both researchers and practitioners as we discuss articles providing mathematical guarantees as well as articles with empirical studies on real-world problems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\SVKM44KW\\Gajane et al. - 2022 - Survey on Fair Reinforcement Learning Theory and .pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\88D33URJ\\2205.html}
}

@article{garay-vitoriaApplicationOntologyBasedPlatform2019,
  title = {Application of an {{Ontology-Based Platform}} for {{Developing Affective Interaction Systems}}},
  author = {{Garay-Vitoria}, Nestor and Cearreta, Idoia and {Larraza-Mendiluze}, Edurne},
  year = {2019},
  journal = {IEEE Access},
  volume = {7},
  pages = {40503--40515},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2903436},
  abstract = {Computer systems need to have sufficient ability and intelligence to communicate with people. To this end, they have to be able to interpret or to manage certain types of information that people are used to perceiving in human communications, such as speech modulation, facial expression, and so on taking human emotions into account. The ontology-based platform proposed in this paper attempts to support the development of resources that need to take emotion transmission into account, especially in communication between users and interactive systems. To this end, the factors relevant to the transmission of affective states have been studied and included in an ontology. Based on this ontology, a platform was created to guide the development of emotional resources that provide users with more natural interfaces. Finally, an interactive multimodal system was created to validate the proposed ontology-based platform and to apply the study to real-life cases.},
  keywords = {Affective computing,affective recognition and synthesis,Brain modeling,Computational modeling,Context modeling,Emotion recognition,interaction context modeling,Ontologies,ontology knowledge representation},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\QA2R2DCS\\Garay-Vitoria et al. - 2019 - Application of an Ontology-Based Platform for Deve.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\LT6IS86G\\8662572.html}
}

@article{gebeleFaceValueImpact2022,
  title = {Face {{Value}}: {{On}} the {{Impact}} of {{Annotation}} ({{In-}}){{Consistencies}} and {{Label Ambiguity}} in {{Facial Data}} on {{Emotion Recognition}}},
  author = {Gebele, Jens and Brune, Philipp and Fau{\ss}er, Stefan},
  year = {2022},
  journal = {IEEE 26th International Conference on Pattern Recognition}
}

@inproceedings{generosiDeepLearningbasedSystem2018,
  title = {A Deep Learning-Based System to Track and Analyze Customer Behavior in Retail Store},
  booktitle = {2018 {{IEEE}} 8th {{International Conference}} on {{Consumer Electronics-Berlin}} ({{ICCE-Berlin}})},
  author = {Generosi, Andrea and Ceccacci, Silvia and Mengoni, Maura},
  year = {2018},
  pages = {1--6},
  publisher = {{IEEE}},
  isbn = {1-5386-6095-4},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\T8CMITZG\\ADeepLearning-basedSystemtoTrackandAnalyzeCust.pdf}
}

@inproceedings{geraUnifiedHybridModel1991,
  title = {Towards a Unified Hybrid Model of Category Representation},
  booktitle = {[{{Proceedings}}] 1991 {{IEEE International Joint Conference}} on {{Neural Networks}}},
  author = {Gera, M.H.},
  year = {1991},
  month = nov,
  pages = {2484-2489 vol.3},
  doi = {10.1109/IJCNN.1991.170762},
  abstract = {The author (1991) has argued that existing symbolic and subsymbolic models of category representation fail to capture critical features of hierarchy and partonomy. In this paper a hybrid connectionist model that addresses these shortfalls is described. By way of demonstration, it is shown how it can deliver faster basic level categorization times in categorizing isolated objects, and equality of basic level and superordinate level categorization times of objects in scenes. The learning algorithm is described. This algorithm is unique in that it allows relearning of already-learned definitions under the guidance of a high-level symbol system.{$<>$}},
  keywords = {Artificial intelligence,Displays,Educational institutions,Haptic interfaces,Humans,Layout,Machine learning,Power system modeling,Testing},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\EDBF4XUE\\Gera - 1991 - Towards a unified hybrid model of category represe.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\IJGHQSY8\\170762.html}
}

@article{ghahramaniProbabilisticMachineLearning2015,
  title = {Probabilistic Machine Learning and Artificial Intelligence},
  author = {Ghahramani, Zoubin},
  year = {2015},
  month = may,
  journal = {Nature},
  volume = {521},
  number = {7553},
  pages = {452--459},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature14541},
  abstract = {How can a machine learn from experience? Probabilistic modelling provides a framework for understanding what learning is, and has therefore emerged as one of the principal theoretical and practical approaches for designing machines that learn from data acquired through experience. The probabilistic framework, which describes how to represent and manipulate uncertainty about models and predictions, has a central role in scientific data analysis, machine learning, robotics, cognitive science and artificial intelligence. This Review provides an introduction to this framework, and discusses some of the state-of-the-art advances in the field, namely, probabilistic programming, Bayesian optimization, data compression and automatic model discovery.},
  copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  keywords = {Computer science,Mathematics and computing,Neuroscience},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\G9GK7CFU\\Ghahramani - 2015 - Probabilistic machine learning and artificial inte.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\282L56EN\\nature14541.html}
}

@article{gilFutureComputingBits2019,
  title = {The {{Future}} of {{Computing}}: {{Bits}} + {{Neurons}} + {{Qubits}}},
  shorttitle = {The {{Future}} of {{Computing}}},
  author = {Gil, Dario and Green, William M. J.},
  year = {2019},
  month = nov,
  journal = {arXiv:1911.08446 [physics]},
  eprint = {1911.08446},
  eprinttype = {arxiv},
  primaryclass = {physics},
  abstract = {The laptops, cell phones, and internet applications commonplace in our daily lives are all rooted in the idea of zeros and ones - in bits. This foundational element originated from the combination of mathematics and Claude Shannon's Theory of Information. Coupled with the 50-year legacy of Moore's Law, the bit has propelled the digitization of our world. In recent years, artificial intelligence systems, merging neuron-inspired biology with information, have achieved superhuman accuracy in a range of narrow classification tasks by learning from labelled data. Advancing from Narrow AI to Broad AI will encompass the unification of learning and reasoning through neuro-symbolic systems, resulting in a form of AI which will perform multiple tasks, operate across multiple domains, and learn from small quantities of multi-modal input data. Finally, the union of physics and information led to the emergence of Quantum Information Theory and the development of the quantum bit - the qubit - forming the basis of quantum computers. We have built the first programmable quantum computers, and although the technology is still in its early days, these systems offer the potential to solve problems which even the most powerful classical computers cannot. The future of computing will look fundamentally different than it has in the past. It will not be based on more and cheaper bits alone, but rather, it will be built upon bits + neurons + qubits. This future will enable the next generation of intelligent mission-critical systems and accelerate the rate of science-driven discovery.},
  archiveprefix = {arXiv},
  keywords = {Physics - Applied Physics,Physics - Popular Physics},
  note = {Less Important
\par
Comment: 30 pages, 20 figures. To be presented by Dario Gil at the Plenary Session of the 2020 International Solid-State Circuits Conference (ISSCC 2020, February 16-20, San Francisco, California)},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\62RYT4VZ\\Gil und Green - 2019 - The Future of Computing Bits + Neurons + Qubits.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\S96WER8H\\1911.html}
}

@article{goelLowPowerMultiCameraObject2021,
  title = {Low-{{Power Multi-Camera Object Re-Identification}} Using {{Hierarchical Neural Networks}}},
  author = {Goel, Abhinav and Tung, Caleb and Hu, Xiao and Wang, Haobo and Davis, James C. and Thiruvathukal, George K. and Lu, Yung-Hsiang},
  year = {2021},
  month = jun,
  journal = {arXiv:2106.10588 [cs, eess]},
  eprint = {2106.10588},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {Low-power computer vision on embedded devices has many applications. This paper describes a low-power technique for the object re-identification (reID) problem: matching a query image against a gallery of previously seen images. State-of-the-art techniques rely on large, computationally-intensive Deep Neural Networks (DNNs). We propose a novel hierarchical DNN architecture that uses attribute labels in the training dataset to perform efficient object reID. At each node in the hierarchy, a small DNN identifies a different attribute of the query image. The small DNN at each leaf node is specialized to re-identify a subset of the gallery: only the images with the attributes identified along the path from the root to a leaf. Thus, a query image is re-identified accurately after processing with a few small DNNs. We compare our method with state-of-the-art object reID techniques. With a 4\% loss in accuracy, our approach realizes significant resource savings: 74\% less memory, 72\% fewer operations, and 67\% lower query latency, yielding 65\% less energy consumption.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  note = {Comment: Accepted to ISLPED 2021},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\SVKA2HCC\\Goel et al. - 2021 - Low-Power Multi-Camera Object Re-Identification us.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\2LYXBCZZ\\2106.html}
}

@article{goldbergAlternativeDescriptionPersonality1990,
  title = {An Alternative" Description of Personality": The Big-Five Factor Structure.},
  shorttitle = {An Alternative" Description of Personality"},
  author = {Goldberg, Lewis R.},
  year = {1990},
  journal = {Journal of personality and social psychology},
  volume = {59},
  number = {6},
  pages = {1216},
  publisher = {{American Psychological Association}},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\YY7FA487\\doiLanding.html}
}

@incollection{gongReidentificationChallenge2014,
  title = {The Re-Identification Challenge},
  booktitle = {Person Re-Identification},
  author = {Gong, Shaogang and Cristani, Marco and Loy, Chen Change and Hospedales, Timothy M.},
  year = {2014},
  pages = {1--20},
  publisher = {{Springer}},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\ATX7AVNG\\Gong et al. - 2014 - The re-identification challenge.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\HACVBSII\\978-1-4471-6296-4_1.html}
}

@article{goodfellowChallengesRepresentationLearning2013,
  title = {Challenges in {{Representation Learning}}: {{A}} Report on Three Machine Learning Contests},
  shorttitle = {Challenges in {{Representation Learning}}},
  author = {Goodfellow, Ian J. and Erhan, Dumitru and Carrier, Pierre Luc and Courville, Aaron and Mirza, Mehdi and Hamner, Ben and Cukierski, Will and Tang, Yichuan and Thaler, David and Lee, Dong-Hyun and Zhou, Yingbo and Ramaiah, Chetan and Feng, Fangxiang and Li, Ruifan and Wang, Xiaojie and Athanasakis, Dimitris and {Shawe-Taylor}, John and Milakov, Maxim and Park, John and Ionescu, Radu and Popescu, Marius and Grozea, Cristian and Bergstra, James and Xie, Jingjing and Romaszko, Lukasz and Xu, Bing and Chuang, Zhang and Bengio, Yoshua},
  year = {2013},
  month = jul,
  journal = {arXiv:1307.0414 [cs, stat]},
  eprint = {1307.0414},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The ICML 2013 Workshop on Challenges in Representation Learning focused on three challenges: the black box learning challenge, the facial expression recognition challenge, and the multimodal learning challenge. We describe the datasets created for these challenges and summarize the results of the competitions. We provide suggestions for organizers of future challenges and some comments on what kind of knowledge can be gained from machine learning competitions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: 8 pages, 2 figures},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\ENQAMBZY\\Goodfellow et al. - 2013 - Challenges in Representation Learning A report on.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\I8EBNMZV\\1307.html}
}

@article{goriEncodingNondeterministicFuzzy2004,
  title = {Encoding Nondeterministic Fuzzy Tree Automata into Recursive Neural Networks},
  author = {Gori, M. and Petrosino, A.},
  year = {2004},
  month = nov,
  journal = {IEEE Transactions on Neural Networks},
  volume = {15},
  number = {6},
  pages = {1435--1449},
  issn = {1941-0093},
  doi = {10.1109/TNN.2004.837585},
  abstract = {Fuzzy neural systems have been a subject of great interest in the last few years, due to their abilities to facilitate the exchange of information between symbolic and subsymbolic domains. However, the models in the literature are not able to deal with structured organization of information, that is typically required by symbolic processing. In many application domains, the patterns are not only structured, but a fuzziness degree is attached to each subsymbolic pattern primitive. The purpose of this paper is to show how recursive neural networks, properly conceived for dealing with structured information, can represent nondeterministic fuzzy frontier-to-root tree automata. Whereas available prior knowledge expressed in terms of fuzzy state transition rules are injected into a recursive network, unknown rules are supposed to be filled in by data-driven learning. We also prove the stability of the encoding algorithm, extending previous results on the injection of fuzzy finite-state dynamics in high-order recurrent networks.},
  keywords = {Application software,Councils,Encoding,Fuzzy neural networks,Fuzzy sets,fuzzy systems,Fuzzy systems,fuzzy tree automata,knowledge representation,Learning automata,Neural networks,Pattern recognition,Production,recursive neural networks},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\KY4Y66UH\\Gori und Petrosino - 2004 - Encoding nondeterministic fuzzy tree automata into.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\9X9J7M4F\\1353280.html}
}

@inproceedings{grothEnhancingFeatureSelection2021,
  title = {Enhancing {{Feature Selection}} in {{Single Shot Robot Learning}} by {{Using Multi-Modal Inputs}}},
  booktitle = {2021 4th {{International Conference}} on {{Artificial Intelligence}} for {{Industries}} ({{AI4I}})},
  author = {Groth, Christian},
  year = {2021},
  month = sep,
  pages = {5--9},
  doi = {10.1109/AI4I51902.2021.00010},
  abstract = {To provide robots for a wide range of users, there needs to be an easy and intuitive way to program them. This issue is addressed by the robot programming by demonstration or imitation learning paradigm, where the user demonstrates the task to the robot by teleoperation. Although single-shot approaches could save a lot of time and effort, they are still a niche due to some drawbacks, like ambiguities in selecting the relevant features.In this work we try to enhance a single shot programming by demonstration approach on sub-symbolic level by extending it to a multi modal input. While most approaches mainly focus on the trajectories and visual detection of objects, we combine speech and kinestethic teaching in order to resolve ambiguities and to rise the level of transferred information.},
  keywords = {Education,Feature extraction,Industries,Robot learning,Task analysis,Trajectory,Visualization},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\FDRF5XUP\\Groth - 2021 - Enhancing Feature Selection in Single Shot Robot L.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\L3E34PR6\\9565486.html}
}

@inproceedings{gStudyVariousAlgorithms2021,
  title = {A {{Study}} of {{Various Algorithms}} for {{Facial Expression Recognition}}: {{A Review}}},
  shorttitle = {A {{Study}} of {{Various Algorithms}} for {{Facial Expression Recognition}}},
  booktitle = {2021 {{International Conference}} on {{Computational Intelligence}} and {{Computing Applications}} ({{ICCICA}})},
  author = {G, Devasena and V, Vidhya},
  year = {2021},
  month = nov,
  pages = {1--8},
  doi = {10.1109/ICCICA52458.2021.9697318},
  abstract = {Facial Expression Recognition (FER) is an important thrust area in the field of artificial intelligence and computer vision. The features of various faces and their characteristics are analyzed to achieve the concept of FER. The facial characteristics are retrieved using an automated face detection method which helps to identify the emotions of a person. This study examines in-depth FER investigations using several techniques, such as template, appearance, knowledge-based and feature-based approaches, coupled with a variety of algorithms such as viola jones, Faster RCNN, SSD, MTCNN and Face landmark Detection. These techniques are used to classify the different emotions of the human face such as happiness, wrath, sorrow, disgust, fear, neutrality, surprise and disdain. Moreover, research works based on deep learning based FER models are also examined.},
  keywords = {Classification algorithms,Emotion Classification,Face detection,Face Detection,Face Expression Recognition,Face recognition,Face Recognition,Feature extraction,Knowledge based systems,Lighting,Pain},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\TWD6DVZT\\G und V - 2021 - A Study of Various Algorithms for Facial Expressio.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\NXUC3H4X\\9697318.html}
}

@inproceedings{gunesAffectRecognitionFace2005,
  title = {Affect Recognition from Face and Body: Early Fusion vs. Late Fusion},
  shorttitle = {Affect Recognition from Face and Body},
  booktitle = {2005 {{IEEE International Conference}} on {{Systems}}, {{Man}} and {{Cybernetics}}},
  author = {Gunes, H. and Piccardi, M.},
  year = {2005},
  month = oct,
  volume = {4},
  pages = {3437-3443 Vol. 4},
  issn = {1062-922X},
  doi = {10.1109/ICSMC.2005.1571679},
  abstract = {This paper presents an approach to automatic visual emotion recognition from two modalities: face and body. Firstly, individual classifiers are trained from individual modalities. Secondly, we fuse facial expression and affective body gesture information first at a feature-level, in which the data from both modalities are combined before classification, and later at a decision-level, in which we integrate the outputs of the monomodal systems by the use of suitable criteria. We then evaluate these two fusion approaches, in terms of performance over monomodal emotion recognition based on facial expression modality only. In the experiments performed the emotion classification using the two modalities achieved a better recognition accuracy outperforming the classification using the individual facial modality. Moreover, fusion at the feature-level proved better recognition than fusion at the decision-level.},
  keywords = {bimodal affect recognition,body gesture,Computer displays,Computer vision,Data mining,early fusion,Emotion recognition,Face detection,Face recognition,Facial animation,Facial expression,Fuses,Human computer interaction,Information technology,late fusion,monomodal affect recognition},
  note = {Early vs. Late Fusion
\par
Channel = Type of signal recorded, for instance, video
\par
Modality = Type of information processed, for instance, facial expressions},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\FI4D8YLQ\\Gunes und Piccardi - 2005 - Affect recognition from face and body early fusio.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\8H954TRQ\\1571679.html}
}

@article{guoSemanticHistogramBased2021,
  title = {Semantic {{Histogram Based Graph Matching}} for {{Real-Time Multi-Robot Global Localization}} in {{Large Scale Environment}}},
  author = {Guo, Xiyue and Hu, Junjie and Chen, Junfeng and Deng, Fuqin and Lam, Tin Lun},
  year = {2021},
  month = oct,
  journal = {IEEE Robotics and Automation Letters},
  volume = {6},
  number = {4},
  pages = {8349--8356},
  issn = {2377-3766},
  doi = {10.1109/LRA.2021.3058935},
  abstract = {The core problem of visual multi-robot simultaneous localization and mapping (MR-SLAM) is how to efficiently and accurately perform multi-robot global localization (MR-GL). The difficulties are two-fold. The first is the difficulty of global localization for significant viewpoint difference. Appearance-based localization methods tend to fail under large viewpoint changes. Recently, semantic graphs have been utilized to overcome the viewpoint variation problem. However, the methods are highly time-consuming, especially in large-scale environments. This leads to the second difficulty, which is how to perform real-time global localization. In this letter, we propose a semantic histogram based graph matching method that is robust to viewpoint variation and can achieve real-time global localization. Based on that, we develop a system that can accurately and efficiently perform MR-GL for both homogeneous and heterogeneous robots. The experimental results show that our approach is about 30 times faster than Random Walk based semantic descriptors. Moreover, it achieves an accuracy of 95\% for global localization, while the accuracy of the state-of-the-art method is 85\%.},
  keywords = {Global localization,Histograms,large viewpoint difference,Location awareness,multi-robot map merging,Real-time systems,semantic SLAM,Semantics,Simultaneous localization and mapping,Three-dimensional displays,Visualization},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\DP44XKSS\\Guo et al. - 2021 - Semantic Histogram Based Graph Matching for Real-T.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\DC2VGZTT\\9353207.html}
}

@inproceedings{guptaDeepTransferLearning2015,
  title = {Deep Transfer Learning with Ontology for Image Classification},
  booktitle = {2015 {{Fifth National Conference}} on {{Computer Vision}}, {{Pattern Recognition}}, {{Image Processing}} and {{Graphics}} ({{NCVPRIPG}})},
  author = {Gupta, Umang and Chaudhury, Santanu},
  year = {2015},
  month = dec,
  pages = {1--4},
  doi = {10.1109/NCVPRIPG.2015.7490037},
  abstract = {Purely, data-driven large scale image classification has been achieved using various feature descriptors like SIFT, HOG etc. Major milestone in this regards is Convolutional Neural Networks (CNN) based methods which learn optimal feature descriptors as filters. Little attention has been given to the use of domain knowledge. Ontology plays an important role in learning to categorize images into abstract classes where there may not be a clear visual connect between category and image, for example identifying image mood - happy, sad and neutral. Our algorithm combines CNN and ontology priors to infer abstract patterns in Indian Monument Images. We use a transfer learning based approach in which, knowledge of domain is transferred to CNN while training (top down transfer) and inference is made using CNN prediction and ontology tree/priors (bottom up transfer ). We classify images to categories like Tomb, Fort and Mosque. We demonstrate that our method improves remarkably over logistic classifier and other transfer learning approach. We conclude with a remark on possible applications of the model and note about scaling this to bigger ontology.},
  keywords = {Feature extraction,Linear programming,Logistics,Neural networks,Ontologies,Shape,Training},
  note = {Check: but we hypothesize as categories become abstract it may not perform well.},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\Z5ZGARWM\\Gupta und Chaudhury - 2015 - Deep transfer learning with ontology for image cla.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\W8JJGMRC\\7490037.html}
}

@inproceedings{guptaREDEDetectingHuman2022,
  title = {{{REDE}} - {{Detecting}} Human Emotions Using {{CNN}} and {{RASA}}},
  booktitle = {2022 {{International Conference}} for {{Advancement}} in {{Technology}} ({{ICONAT}})},
  author = {Gupta, Anya and Raj, Monica Arul and Singh, Khushi and Deshmukh, Rupali},
  year = {2022},
  month = jan,
  pages = {1--6},
  doi = {10.1109/ICONAT53423.2022.9726090},
  abstract = {The involvement of technology in medical health has already been a great success to a large extent; it is used to measure depression and initiate the advancement into the field of mental health toward therapy and counselling. According to the WHO, good health is not only about zero sicknesses or disability but is also about physical well-being, sound mental state and social and spiritual welfare. The technological implementation of artificial intelligence (AI) in mental health has vast potential for personalizing treatment selection, prognostication, and relapse monitoring. Moreover, it provides remedies to reduce stress and anxiety for situations that do not require immediate and necessary medical intrusion and emergency contacts and services in case of a severe condition. Particularly, to discern depressive behaviours, multi-modal data is used to examine and exploit a large variety of parameters. Unlike the usual method of having an observational study that is done by taking surveys or questionnaires, the AI model helps us to understand and explore the inconspicuous and reliable detection of depressive symptoms obtained from visual and vocal features of the user. In today's time, vocalizing one's concerns regarding their mental health must be normalized. As humans, it is normal to feel different emotions at once. The application is free and anonymous to make the users feel empowered and safe in seeking treatment. Mental health is all about how an individual thinks, feels and copes up with events in their life.},
  keywords = {ChatBot,Convolutional Neural Network,Convolutional neural networks,Depression,Mental health,Mental Health,Neural networks,RASA,Sociology,Speech recognition,Visualization},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\265JZMNT\\Gupta et al. - 2022 - REDE - Detecting human emotions using CNN and RASA.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\9AIKGR4K\\9726090.html}
}

@inproceedings{haiduKnowRobSIMGameEngineEnabled2018,
  title = {{{KnowRobSIM}} \textemdash{} {{Game Engine-Enabled Knowledge Processing Towards Cognition-Enabled Robot Control}}},
  booktitle = {2018 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Haidu, Andrei and Be{\ss}ler, Daniel and Bozcuo{\u g}lu, Asil Kaan and Beetz, Michael},
  year = {2018},
  month = oct,
  pages = {4491--4498},
  issn = {2153-0866},
  doi = {10.1109/IROS.2018.8593935},
  abstract = {AI knowledge representation and reasoning methods consider actions to be blackboxes that abstract away from how they are executed. This abstract view does not suffice for the decision making capabilities required by robotic agents that are to accomplish manipulation tasks. Such robots have to reason about how to pour without spilling, where to grasp a pot, how to open different containers, and so on. To enable such reasoning it is necessary to consider how objects are perceived, how motions can be executed and parameterized, and how motion parameterization affects the physical effects of actions. To this end, we propose to complement and extend symbolic reasoning methods with KnowRobSIM, an additional reasoning infrastructure based on modern game engine technology, including the subsymbolic world modeling through data structures, action simulation based on physics engine, and world scene rendering. We demonstrate how KnowRobSIM can perform powerful reasoning, prediction, and learning tasks that are required for informed decision making in object manipulation.},
  keywords = {Cognition,Data structures,Engines,Force,Games,Robots},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\MN5RF8XA\\Haidu et al. - 2018 - KnowRobSIM — Game Engine-Enabled Knowledge Process.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\3G8BGSWE\\8593935.html}
}

@article{hamiltonNeuroSymbolicAIMeeting2022,
  title = {Is {{Neuro-Symbolic AI Meeting}} Its {{Promise}} in {{Natural Language Processing}}? {{A Structured Review}}},
  shorttitle = {Is {{Neuro-Symbolic AI Meeting}} Its {{Promise}} in {{Natural Language Processing}}?},
  author = {Hamilton, Kyle and Nayak, Aparna and Bo{\v z}i{\'c}, Bojan and Longo, Luca},
  year = {2022},
  month = feb,
  journal = {arXiv:2202.12205 [cs]},
  eprint = {2202.12205},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Advocates for Neuro-Symbolic AI (NeSy) assert that combining deep learning with symbolic reasoning will lead to stronger AI than either paradigm on its own. As successful as deep learning has been, it is generally accepted that even our best deep learning systems are not very good at abstract reasoning. And since reasoning is inextricably linked to language, it makes intuitive sense that Natural Language Processing (NLP), would be a particularly well-suited candidate for NeSy. We conduct a structured review of studies implementing NeSy for NLP, challenges and future directions, and aim to answer the question of whether NeSy is indeed meeting its promises: reasoning, out-of-distribution generalization, interpretability, learning and reasoning from small data, and transferability to new domains. We examine the impact of knowledge representation, such as rules and semantic networks, language structure and relational structure, and whether implicit or explicit reasoning contributes to higher promise scores. We find that knowledge encoded in relational structures and explicit reasoning tend to lead to more NeSy goals being satisfied. We also advocate for a more methodical approach to the application of theories of reasoning, which we hope can reduce some of the friction between the symbolic and sub-symbolic schools of AI.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  note = {Comment: Survey},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\YC6EUDMY\\Hamilton et al. - 2022 - Is Neuro-Symbolic AI Meeting its Promise in Natura.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\95YMYYJ5\\2202.html}
}

@article{hammAutomatedFacialAction2011,
  title = {Automated {{Facial Action Coding System}} for Dynamic Analysis of Facial Expressions in Neuropsychiatric Disorders},
  author = {Hamm, Jihun and Kohler, Christian G. and Gur, Ruben C. and Verma, Ragini},
  year = {2011},
  month = sep,
  journal = {Journal of Neuroscience Methods},
  volume = {200},
  number = {2},
  pages = {237--256},
  issn = {0165-0270},
  doi = {10.1016/j.jneumeth.2011.06.023},
  abstract = {Facial expression is widely used to evaluate emotional impairment in neuropsychiatric disorders. Ekman and Friesen's Facial Action Coding System (FACS) encodes movements of individual facial muscles from distinct momentary changes in facial appearance. Unlike facial expression ratings based on categorization of expressions into prototypical emotions (happiness, sadness, anger, fear, disgust, etc.), FACS can encode ambiguous and subtle expressions, and therefore is potentially more suitable for analyzing the small differences in facial affect. However, FACS rating requires extensive training, and is time consuming and subjective thus prone to bias. To overcome these limitations, we developed an automated FACS based on advanced computer science technology. The system automatically tracks faces in a video, extracts geometric and texture features, and produces temporal profiles of each facial muscle movement. These profiles are quantified to compute frequencies of single and combined Action Units (AUs) in videos, and they can facilitate a statistical study of large populations in disorders known to impact facial expression. We derived quantitative measures of flat and inappropriate facial affect automatically from temporal AU profiles. Applicability of the automated FACS was illustrated in a pilot study, by applying it to data of videos from eight schizophrenia patients and controls. We created temporal AU profiles that provided rich information on the dynamics of facial muscle movements for each subject. The quantitative measures of flatness and inappropriateness showed clear differences between patients and the controls, highlighting their potential in automatic and objective quantification of symptom severity.},
  langid = {english},
  keywords = {Action Units,Computerized method,Facial Action Coding System,Facial expressions},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\92AIR59H\\Hamm et al. - 2011 - Automated Facial Action Coding System for dynamic .pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\ULXGC5PI\\S016502701100358X.html}
}

@inproceedings{hanNeuralCollapseMSE2021,
  title = {Neural {{Collapse Under MSE Loss}}: {{Proximity}} to and {{Dynamics}} on the {{Central Path}}},
  shorttitle = {Neural {{Collapse Under MSE Loss}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Han, X. Y. and Papyan, Vardan and Donoho, David L.},
  year = {2021},
  month = sep,
  abstract = {The recently discovered Neural Collapse (NC) phenomenon occurs pervasively in today's deep net training paradigm of driving cross-entropy (CE) loss towards zero. During NC, last-layer features...},
  langid = {english},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\7A8KJLUE\\Han et al. - 2021 - Neural Collapse Under MSE Loss Proximity to and D.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\B3AHYD52\\forum.html}
}

@article{happyAutomaticFacialExpression2015,
  title = {Automatic Facial Expression Recognition Using Features of Salient Facial Patches},
  author = {Happy, S L and Routray, Aurobinda},
  year = {2015},
  month = jan,
  journal = {IEEE Transactions on Affective Computing},
  volume = {6},
  number = {1},
  pages = {1--12},
  issn = {1949-3045},
  doi = {10.1109/TAFFC.2014.2386334},
  abstract = {Extraction of discriminative features from salient facial patches plays a vital role in effective facial expression recognition. The accurate detection of facial landmarks improves the localization of the salient patches on face images. This paper proposes a novel framework for expression recognition by using appearance features of selected facial patches. A few prominent facial patches, depending on the position of facial landmarks, are extracted which are active during emotion elicitation. These active patches are further processed to obtain the salient patches which contain discriminative features for classification of each pair of expressions, thereby selecting different facial patches as salient for different pair of expression classes. One-against-one classification method is adopted using these features. In addition, an automated learning-free facial landmark detection technique has been proposed, which achieves similar performances as that of other state-of-art landmark detection methods, yet requires significantly less execution time. The proposed method is found to perform well consistently in different resolutions, hence, providing a solution for expression recognition in low resolution images. Experiments on CK+ and JAFFE facial expression databases show the effectiveness of the proposed system.},
  keywords = {Action Units,Eyebrows,Face,Face recognition,Facial expression analysis,facial landmark detection,Feature extraction,feature selection,Image edge detection,Image resolution,low resolution image,Nose,salient facial patches},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\CPQSXHYU\\Happy und Routray - 2015 - Automatic facial expression recognition using feat.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\5SDCRW8D\\6998925.html}
}

@inproceedings{hardjadinataFacialExpressionRecognition2021,
  title = {Facial {{Expression Recognition Using Xception And DenseNet Architecture}}},
  booktitle = {2021 6th {{International Conference}} on {{New Media Studies}} ({{CONMEDIA}})},
  author = {Hardjadinata, Hannatassja and Oetama, Raymond Sunardi and Prasetiawan, Iwan},
  year = {2021},
  month = oct,
  pages = {60--65},
  doi = {10.1109/CONMEDIA53104.2021.9617173},
  abstract = {Researchers pay much attention to facial expressions recognition due to the rapid development of Artificial Intelligence. Facial expression recognition is used to help human-computer interaction. In addition, facial expression recognition is also used in psychological recognition, Human-computer interaction, assisted driving, and security station in everyday life. But most of the research focused on the machine learning approach rather than deep learning and the emotion classifications are also smaller. This facial expression recognition can be implemented using a deep learning approach. The architecture that is often used and considered to be the best in image classification is Convolutional Neural Network. Therefore, this study builds a Convolutional Neural Network Model with Xception and DenseNet architecture. The accuracy of the two models is compared, with Xception received an accuracy of 70\% and DenseNet got 79\%.},
  keywords = {Basic Emotion,Convolutional Neural Network,Convolutional neural networks,Deep learning,Deep Learning,Face recognition,Facial Expression,Human computer interaction,Media,Psychology,Security},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\IC7CVQ5K\\Hardjadinata et al. - 2021 - Facial Expression Recognition Using Xception And D.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\8P33F4AB\\9617173.html}
}

@inproceedings{hauserPrinciplesRiemannianGeometry2017,
  title = {Principles of {{Riemannian Geometry}} in {{Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Hauser, Michael and Ray, Asok},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\RZDWSDM5\\Hauser und Ray - 2017 - Principles of Riemannian Geometry in Neural Networ.pdf}
}

@article{heavenWhyFacesDon2020,
  title = {Why Faces Don't Always Tell the Truth about Feelings},
  author = {Heaven, Douglas},
  year = {2020},
  month = feb,
  journal = {Nature},
  volume = {578},
  number = {7796},
  pages = {502--504},
  publisher = {{Nature Publishing Group}},
  doi = {10.1038/d41586-020-00507-5},
  abstract = {Although AI companies market software for recognizing emotions in faces, psychologists debate whether expressions can be read so easily.},
  copyright = {2021 Nature},
  langid = {english},
  keywords = {Computer science,Psychology,Society},
  annotation = {Bandiera\_abtest: a Cg\_type: News Feature Subject\_term: Psychology, Society, Computer science},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\CC9JS8NP\\Heaven - 2020 - Why faces don’t always tell the truth about feelin.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\9F42UMEM\\d41586-020-00507-5.html}
}

@article{heBERTMapBERTbasedOntology2022,
  title = {{{BERTMap}}: {{A BERT-based Ontology Alignment System}}},
  shorttitle = {{{BERTMap}}},
  author = {He, Yuan and Chen, Jiaoyan and Antonyrajah, Denvar and Horrocks, Ian},
  year = {2022},
  month = jan,
  journal = {arXiv:2112.02682 [cs]},
  eprint = {2112.02682},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Ontology alignment (a.k.a ontology matching (OM)) plays a critical role in knowledge integration. Owing to the success of machine learning in many domains, it has been applied in OM. However, the existing methods, which often adopt ad-hoc feature engineering or non-contextual word embeddings, have not yet outperformed rule-based systems especially in an unsupervised setting. In this paper, we propose a novel OM system named BERTMap which can support both unsupervised and semi-supervised settings. It first predicts mappings using a classifier based on fine-tuning the contextual embedding model BERT on text semantics corpora extracted from ontologies, and then refines the mappings through extension and repair by utilizing the ontology structure and logic. Our evaluation with three alignment tasks on biomedical ontologies demonstrates that BERTMap can often perform better than the leading OM systems LogMap and AML.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  note = {Comment: Full version (with appendix) of the accepted paper in 36th AAAI Conference on Artificial Intelligence 2022
\par
Comment: Full version (with appendix) of the accepted paper in 36th AAAI Conference on Artificial Intelligence 2022},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\AH9TBV4V\\He et al. - 2022 - BERTMap A BERT-based Ontology Alignment System.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\6ULQ94RC\\2112.html}
}

@inproceedings{heDeepResidualLearning2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2016},
  month = jun,
  pages = {770--778},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2016.90},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\texttimes{} deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  keywords = {Complexity theory,Degradation,Image recognition,Image segmentation,Neural networks,Training,Visualization},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\KBLMZ4PN\\He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\YMN6SQK4\\7780459.html}
}

@article{heDenseInteractionLearning2021,
  title = {Dense {{Interaction Learning}} for {{Video-based Person Re-identification}}},
  author = {He, Tianyu and Jin, Xin and Shen, Xu and Huang, Jianqiang and Chen, Zhibo and Hua, Xian-Sheng},
  year = {2021},
  month = aug,
  journal = {arXiv:2103.09013 [cs]},
  eprint = {2103.09013},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Video-based person re-identification (re-ID) aims at matching the same person across video clips. Efficiently exploiting multi-scale fine-grained features while building the structural interaction among them is pivotal for its success. In this paper, we propose a hybrid framework, Dense Interaction Learning (DenseIL), that takes the principal advantages of both CNN-based and Attention-based architectures to tackle video-based person re-ID difficulties. DenseIL contains a CNN encoder and a Dense Interaction (DI) decoder. The CNN encoder is responsible for efficiently extracting discriminative spatial features while the DI decoder is designed to densely model spatial-temporal inherent interaction across frames. Different from previous works, we additionally let the DI decoder densely attends to intermediate fine-grained CNN features and that naturally yields multi-grained spatial-temporal representation for each video clip. Moreover, we introduce Spatio-TEmporal Positional Embedding (STEP-Emb) into the DI decoder to investigate the positional relation among the spatial-temporal inputs. Our experiments consistently and significantly outperform all the state-of-the-art methods on multiple standard video-based person re-ID datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: ICCV 2021, Oral},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\ZBT9UJGU\\He et al. - 2021 - Dense Interaction Learning for Video-based Person .pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\JXRQ864S\\2103.html}
}

@article{hintonHowRepresentPartwhole2021,
  title = {How to Represent Part-Whole Hierarchies in a Neural Network},
  author = {Hinton, Geoffrey},
  year = {2021},
  month = feb,
  journal = {arXiv:2102.12627 [cs]},
  eprint = {2102.12627},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {This paper does not describe a working system. Instead, it presents a single idea about representation which allows advances made by several different groups to be combined into an imaginary system called GLOM. The advances include transformers, neural fields, contrastive representation learning, distillation and capsules. GLOM answers the question: How can a neural network with a fixed architecture parse an image into a part-whole hierarchy which has a different structure for each image? The idea is simply to use islands of identical vectors to represent the nodes in the parse tree. If GLOM can be made to work, it should significantly improve the interpretability of the representations produced by transformer-like systems when applied to vision or language},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,I.2.6,I.4.8},
  note = {Comment: 43 pages, 5 figures},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\3IJMKJ9J\\Hinton - 2021 - How to represent part-whole hierarchies in a neura.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\UHW89MU3\\2102.html}
}

@book{hjortsjoManFaceMimic1969,
  title = {Man's {{Face}} and {{Mimic Language}}},
  author = {Hjortsj{\"o}, Carl-Herman},
  year = {1969},
  publisher = {{Studentlitteratur}},
  googlebooks = {BakQAQAAIAAJ},
  langid = {english}
}

@inproceedings{holzingerMachineLearningExplainable2018,
  title = {From {{Machine Learning}} to {{Explainable AI}}},
  booktitle = {2018 {{World Symposium}} on {{Digital Intelligence}} for {{Systems}} and {{Machines}} ({{DISA}})},
  author = {Holzinger, Andreas},
  year = {2018},
  month = aug,
  pages = {55--66},
  doi = {10.1109/DISA.2018.8490530},
  abstract = {The success of statistical machine learning (ML) methods made the field of Artificial Intelligence (AI) so popular again, after the last AI winter. Meanwhile deep learning approaches even exceed human performance in particular tasks. However, such approaches have some disadvantages besides of needing big quality data, much computational power and engineering effort; those approaches are becoming increasingly opaque, and even if we understand the underlying mathematical principles of such models they still lack explicit declarative knowledge. For example, words are mapped to high-dimensional vectors, making them unintelligible to humans. What we need in the future are context-adaptive procedures, i.e. systems that construct contextual explanatory models for classes of real-world phenomena. This is the goal of explainable AI, which is not a new field; rather, the problem of explainability is as old as AI itself. While rule-based approaches of early AI were comprehensible ``glass-box'' approaches at least in narrow domains, their weakness was in dealing with uncertainties of the real world. Maybe one step further is in linking probabilistic learning methods with large knowledge representations (ontologies) and logical approaches, thus making results re-traceable, explainable and comprehensible on demand.},
  keywords = {Cognitive science,Data mining,Data visualization,Games,Machine learning,Uncertainty},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\QN9GWGYF\\Holzinger - 2018 - From Machine Learning to Explainable AI.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\LE79UM3J\\8490530.html}
}

@inproceedings{houBiCnetTKSLearningEfficient2021,
  title = {{{BiCnet-TKS}}: {{Learning Efficient Spatial-Temporal Representation}} for {{Video Person Re-Identification}}},
  shorttitle = {{{BiCnet-TKS}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Hou, Ruibing and Chang, Hong and Ma, Bingpeng and Huang, Rui and Shan, Shiguang},
  year = {2021},
  month = jun,
  pages = {2014--2023},
  publisher = {{IEEE}},
  address = {{Nashville, TN, USA}},
  doi = {10.1109/CVPR46437.2021.00205},
  abstract = {In this paper, we present an efficient spatial-temporal representation for video person re-identification (reID). Firstly, we propose a Bilateral Complementary Network (BiCnet) for spatial complementarity modeling. Specifically, BiCnet contains two branches. Detail Branch processes frames at original resolution to preserve the detailed visual clues, and Context Branch with a down-sampling strategy is employed to capture long-range contexts. On each branch, BiCnet appends multiple parallel and diverse attention modules to discover divergent body parts for consecutive frames, so as to obtain an integral characteristic of target identity. Furthermore, a Temporal Kernel Selection (TKS) block is designed to capture short-term as well as long-term temporal relations by an adaptive mode. TKS can be inserted into BiCnet at any depth to construct BiCnetTKS for spatial-temporal modeling. Experimental results on multiple benchmarks show that BiCnet-TKS outperforms state-of-the-arts with about 50\% less computations. The source code is available at https://github.com/ blue-blue272/BiCnet-TKS.},
  isbn = {978-1-66544-509-2},
  langid = {english},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\EPFCHVML\\Hou et al. - 2021 - BiCnet-TKS Learning Efficient Spatial-Temporal Re.pdf}
}

@article{huangArtificialIntelligenceService2018,
  title = {Artificial Intelligence in Service},
  author = {Huang, Ming-Hui and Rust, Roland T.},
  year = {2018},
  journal = {Journal of Service Research},
  volume = {21},
  number = {2},
  pages = {155--172},
  issn = {1094-6705},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\WVM5G6LT\\AI_in_Service.pdf}
}

@article{huangStrategicFrameworkArtificial2021,
  title = {A Strategic Framework for Artificial Intelligence in Marketing},
  author = {Huang, Ming-Hui and Rust, Roland T.},
  year = {2021},
  month = jan,
  journal = {Journal of the Academy of Marketing Science},
  volume = {49},
  number = {1},
  pages = {30--50},
  issn = {0092-0703},
  doi = {10.1007/s11747-020-00749-9},
  abstract = {The authors develop a three-stage framework for strategic marketing planning, incorporating multiple artificial intelligence (AI) benefits: mechanical AI for automating repetitive marketing functions and activities, thinking AI for processing data to arrive at decisions, and feeling AI for analyzing interactions and human emotions. This framework lays out the ways that AI can be used for marketing research, strategy (segmentation, targeting, and positioning, STP), and actions. At the marketing research stage, mechanical AI can be used for data collection, thinking AI for market analysis, and feeling AI for customer understanding. At the marketing strategy (STP) stage, mechanical AI can be used for segmentation (segment recognition), thinking AI for targeting (segment recommendation), and feeling AI for positioning (segment resonance). At the marketing action stage, mechanical AI can be used for standardization, thinking AI for personalization, and feeling AI for relationalization. We apply this framework to various areas of marketing, organized by marketing 4Ps/4Cs, to illustrate the strategic use of AI.},
  note = {The following values have no corresponding Zotero field:\\
accession-num: WOS:000585729700001},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\QBGN88B2\\AStrategicFrameworkForArtifici.pdf}
}

@misc{huDeepDepthCompletion2022,
  title = {Deep {{Depth Completion}}: {{A Survey}}},
  shorttitle = {Deep {{Depth Completion}}},
  author = {Hu, Junjie and Bao, Chenyu and Ozay, Mete and Fan, Chenyou and Gao, Qing and Liu, Honghai and Lam, Tin Lun},
  year = {2022},
  month = may,
  number = {arXiv:2205.05335},
  eprint = {2205.05335},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  abstract = {Depth completion aims at predicting dense pixel-wise depth from a sparse map captured from a depth sensor. It plays an essential role in various applications such as autonomous driving, 3D reconstruction, augmented reality, and robot navigation. Recent successes on the task have been demonstrated and dominated by deep learning based solutions. In this article, for the first time, we provide a comprehensive literature review that helps readers better grasp the research trends and clearly understand the current advances. We investigate the related studies from the design aspects of network architectures, loss functions, benchmark datasets, and learning strategies with a proposal of a novel taxonomy that categorizes existing methods. Besides, we present a quantitative comparison of model performance on two widely used benchmark datasets, including an indoor and an outdoor dataset. Finally, we discuss the challenges of prior works and provide readers with some insights for future research directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\D7X5HACI\\Hu et al. - 2022 - Deep Depth Completion A Survey.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\TQRIMUK7\\2205.html}
}

@inproceedings{hupontScalableMultimodalFusion2011,
  title = {Scalable Multimodal Fusion for Continuous Affect Sensing},
  booktitle = {2011 {{IEEE Workshop}} on {{Affective Computational Intelligence}} ({{WACI}})},
  author = {Hupont, Isabelle and Ballano, Sergio and Baldassarri, Sandra and Cerezo, Eva},
  year = {2011},
  month = apr,
  pages = {1--8},
  doi = {10.1109/WACI.2011.5953150},
  abstract = {The success of affective interfaces lies in the fusion of emotional information coming from different modalities. This paper proposes a scalable methodology for fusing multiple affect sensing modules, allowing the subsequent addition of new modules without having to retrain the existing ones. It relies on a 2-dimensional affective model and is able to output a continuous emotional path characterizing the user's affective progress over time.},
  keywords = {Affective Computing,Covariance matrix,Databases,Emotion recognition,facial expression analysis,Humans,Instant messaging,Kalman filters,Mathematical model,multimodal recognition,sentiment analysis},
  note = {Multimodal fusion},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\3LM95Y77\\Hupont et al. - 2011 - Scalable multimodal fusion for continuous affect s.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\8J6DILR2\\5953150.html}
}

@inproceedings{huSqueezeandExcitationNetworks2018,
  title = {Squeeze-and-{{Excitation Networks}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Hu, Jie and Shen, Li and Sun, Gang},
  year = {2018},
  month = jun,
  pages = {7132--7141},
  issn = {2575-7075},
  doi = {10.1109/CVPR.2018.00745},
  abstract = {Convolutional neural networks are built upon the convolution operation, which extracts informative features by fusing spatial and channel-wise information together within local receptive fields. In order to boost the representational power of a network, several recent approaches have shown the benefit of enhancing spatial encoding. In this work, we focus on the channel relationship and propose a novel architectural unit, which we term the "Squeeze-and-Excitation" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We demonstrate that by stacking these blocks together, we can construct SENet architectures that generalise extremely well across challenging datasets. Crucially, we find that SE blocks produce significant performance improvements for existing state-of-the-art deep architectures at minimal additional computational cost. SENets formed the foundation of our ILSVRC 2017 classification submission which won first place and significantly reduced the top-5 error to 2.251\%, achieving a 25\% relative improvement over the winning entry of 2016. Code and models are available at https://github.com/hujie-frank/SENet.},
  keywords = {Adaptation models,Computational modeling,Computer architecture,Convolution,Convolutional codes,Stacking,Task analysis},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\MBGI8W8R\\Hu et al. - 2018 - Squeeze-and-Excitation Networks.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\VCYUVGNS\\8578843.html}
}

@book{hutterAutomatedMachineLearning2019,
  title = {Automated Machine Learning: Methods, Systems, Challenges},
  shorttitle = {Automated Machine Learning},
  author = {Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
  year = {2019},
  publisher = {{Springer Nature}},
  file = {C\:\\Users\\gebele\\Desktop\\IT-to-read\\Machine_Learning\\AutoML.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\TU7WBJKB\\23012.html}
}

@inproceedings{jacinthaReviewFacialEmotion2019,
  title = {A {{Review}} on {{Facial Emotion Recognition Techniques}}},
  booktitle = {2019 {{International Conference}} on {{Communication}} and {{Signal Processing}} ({{ICCSP}})},
  author = {Jacintha, V and Simon, Judy and Tamilarasu, S and Thamizhmani, R and {Thanga yogesh}, K and Nagarajan, J.},
  year = {2019},
  month = apr,
  pages = {0517--0521},
  doi = {10.1109/ICCSP.2019.8698067},
  abstract = {As a major phase skin Recognition, together with elliptical boundary model, is accomplished. Further, facial Feature Identification process is carried out. The next step is to initiate a technique for extorting geometric and anthropometric facial characteristics. At last we train as well as test the classifiers. We accomplished a categorization precision of 58.6\% for six types of emotions (bliss, anguish, curiosity, despair, fury, hatred) and mean efficiency of 96.8\% for two emotions (bliss and curiosity).The current study utilizes interest points as markers in face images that are damaged by few emotions as well as correlates its location to that of a normal expression. The outputs are viewed in contrast with Paul Ekman's FACS (Facial Action Coding System) tool to check on the efficacy of the algorithm .The automated identification of face expressions utilizing image template matching method faces various issues pertaining to facial features and recording circumstances. Although this field has reached great heights , choosing of features as well as categorization method for emotion identification , till today remains an unsolved mystery. To suppress feature outliers, the proposed technique comprises of pixel normalization which is used to eliminate intensity offsets backed up using a Min-Max metric in a nearest neighbor classifier. The proposed Min-Max classification technique has an efficiency of 92.85\% to 98.57\% when checked on JAFFE database. Classification task also done using KNN, SVM and Bagged Tree Classifier.},
  keywords = {Emotion recognition,Eyebrows,Face,FACS,Feature extraction,JAFFE,KNN,Min-Max classifier,Skin,Support vector machines,SVM,Task analysis},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\6JULRNG8\\Jacintha et al. - 2019 - A Review on Facial Emotion Recognition Techniques.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\78KFU5V3\\8698067.html}
}

@article{jacotNeuralTangentKernel2020,
  title = {Neural {{Tangent Kernel}}: {{Convergence}} and {{Generalization}} in {{Neural Networks}}},
  shorttitle = {Neural {{Tangent Kernel}}},
  author = {Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  year = {2020},
  month = feb,
  journal = {arXiv:1806.07572 [cs, math, stat]},
  eprint = {1806.07572},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {At initialization, artificial neural networks (ANNs) are equivalent to Gaussian processes in the infinite-width limit, thus connecting them to kernel methods. We prove that the evolution of an ANN during training can also be described by a kernel: during gradient descent on the parameters of an ANN, the network function \$f\_\textbackslash theta\$ (which maps input vectors to output vectors) follows the kernel gradient of the functional cost (which is convex, in contrast to the parameter cost) w.r.t. a new kernel: the Neural Tangent Kernel (NTK). This kernel is central to describe the generalization features of ANNs. While the NTK is random at initialization and varies during training, in the infinite-width limit it converges to an explicit limiting kernel and it stays constant during training. This makes it possible to study the training of ANNs in function space instead of parameter space. Convergence of the training can then be related to the positive-definiteness of the limiting NTK. We prove the positive-definiteness of the limiting NTK when the data is supported on the sphere and the non-linearity is non-polynomial. We then focus on the setting of least-squares regression and show that in the infinite-width limit, the network function \$f\_\textbackslash theta\$ follows a linear differential equation during training. The convergence is fastest along the largest kernel principal components of the input data with respect to the NTK, hence suggesting a theoretical motivation for early stopping. Finally we study the NTK numerically, observe its behavior for wide networks, and compare it to the infinite-width limit.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Mathematics - Probability,Statistics - Machine Learning},
  note = {Code and Data Associated with this Article: \href{https://github.com/thegregyang/GP4A}{https://github.com/thegregyang/GP4A}},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\63GQZGQX\\Jacot et al. - 2020 - Neural Tangent Kernel Convergence and Generalizat.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\MSL2L3EQ\\1806.html}
}

@article{jaisonReviewFacialEmotion2021,
  title = {A {{Review}} on {{Facial Emotion Recognition}} and {{Classification Analysis}} with {{Deep Learning}}},
  author = {Jaison, Asha and Deepa, C.},
  year = {2021},
  journal = {Bioscience Biotechnology Research Communications},
  volume = {14},
  number = {5},
  pages = {154--161},
  publisher = {{Soc Science \& Nature}},
  address = {{Bhopal}},
  issn = {0974-6455},
  doi = {10.21786/bbrc/14.5/29},
  abstract = {Automatic face expression recognition is an exigent research subject and a challenge in computer vision. It is an interdisciplinary domain standing at the crossing of behavioural science, psychology, neurology, and artificial intelligence. Human-robot interaction is getting more significant with the automation of every field, like treating autistic patients, child therapy, babysitting, etc. In all the cases robots need to understand the present state of mind for better decision making. It is difficult for machine learning techniques to recognize the expressions of people since there will be significant changes in the way of their expressions. The emotions expressed through the human face have its importance in making arguments and decisions on different subjects. Machine Learning with Computer Vision and Deep Learning can be used to recognize facial expressions from the preloaded or real time images with human faces. DNN (Deep Neural Networking) is one among the hottest areas of research and is found to be very effective in classification of images with a high degree of accuracy. In the proposed work, the popular dataset CK+ is analysed for comparison. The dataset FER 2013 and home-brewed data sets are used in the work for calculating the accuracy of the model created. The results are obtained in such a way that DCNN approach is very efficient in facial emotion recognition. Experiments and study show that the dataset, FER 2013 is a high-quality dataset with equal efficiency as the other two popular datasets. This paper aims to ameliorate the accuracy of classification of facial emotion.},
  langid = {english},
  keywords = {deep convolutional neural net work   (dcnn),expression recognition,face,facial emotion recognition (fer),FER,network,open cv (open-source computer vision library),read,vgg 16},
  annotation = {WOS:000697827200029},
  note = {Methodological Approach NOT to complex},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\IU5MHXAN\\Jaison und Deepa - 2021 - A Review on Facial Emotion Recognition and Classif.pdf}
}

@inproceedings{jaiswalRealtimeMonitoringSystem2022a,
  title = {Real-Time Monitoring System for Attendance and Attentiveness in Virtual Classroom Environments},
  booktitle = {2022 2nd {{International Conference}} on {{Artificial Intelligence}} and {{Signal Processing}} ({{AISP}})},
  author = {Jaiswal, Rishav and Nair, Akarsh K and Sahoo, Jayakrushna},
  year = {2022},
  month = feb,
  pages = {1--6},
  issn = {2640-5768},
  doi = {10.1109/AISP53593.2022.9760690},
  abstract = {With the outbreak of the COVID-19 pandemic, classroom environments have been subjected to revolutionary changes via the employment of virtual classrooms and allied technological advancements. The traditional methodologies are proving to be inefficient in such an environment for teaching as well as managerial tasks. Also considering their cumbersome nature, the need for a newer, stronger, and better model is evident. As of now, many Deep Learning techniques have been employed for the purpose, ranging from the usage of standard object detection APIs or even CNNs and their variants. Our study proposes a model based on SVM embedded on top of embedding vectors combined with a Single-shot detector for real-time monitoring of attendance and attentiveness of students in a virtual classroom set up making use of video feed. A small comparative study between the proposed model and dlib, a standard library for the purpose as well is performed. The results show that our model outperforms dlib methodology significantly with high accuracy and performance efficiency. We had done experimentations on the fer2013 dataset particularly for emotion detection and custom datasets in general. Even though the model performs well in our experimentations, the need for a stronger and better dataset is high for evaluating the model and implementing it in real-life scenarios.},
  keywords = {Emotion recognition,face recognition,Libraries,Neural network,Quality assurance,Real-time systems,Real-time video monitoring,Signal processing,Streaming media,Support vector machines,virtual class},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\3MWTRW2I\\Jaiswal et al. - 2022 - Real-time monitoring system for attendance and att.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\6MHRSZN8\\9760690.html}
}

@incollection{jamesWhatEmotion18841948,
  title = {What Is Emotion? 1884},
  shorttitle = {What Is Emotion?},
  booktitle = {Readings in the History of Psychology},
  author = {James, William},
  year = {1948},
  series = {Century Psychology Series},
  pages = {290--303},
  publisher = {{Appleton-Century-Crofts}},
  address = {{East Norwalk, CT, US}},
  doi = {10.1037/11304-033},
  abstract = {This essay, published in Mind, IX (1884), 188-204, constitutes James' first publication on the James-Lange theory of the emotions. The physiologists who, during the past few years, have been so industriously exploring the functions of the brain, have limited their attempts at explanation to its cognitive and volitional performances. Dividing the brain into sensorial and motor centres, they have found their division to be exactly paralleled by the analysis made by empirical psychology, of the perceptive and volitional parts of the mind into their simplest elements. But the aesthetic sphere of the mind, its longings, its pleasures and pains, and its emotions, have been ignored in all these researches. And yet it is even now certain that of two things concerning the emotions, one must be true. Either separate and special centres, affected to them alone, are their brain-seat, or else they correspond to processes occurring in the motor and sensory centres, already assigned, or in others like them, not yet mapped out. If the former be the case we must deny the current view, and hold the cortex to be something more than the surface of projection for every sensitive spot and every muscle in the body. If the latter be the case, we must ask whether the emotional process in the sensory or motor centre be an altogether peculiar one, or whether it resembles the ordinary perceptive processes of which those centres are already recognised to be the seat. The purpose of the following pages is to show that the last alternative comes nearest to the truth, and that the emotional brain-processes not only resemble the ordinary sensorial brain-processes, but in very truth, are nothing but such processes variously combined. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Brain,Emotions,History of Psychology,James (William),Neurophysiology,Theories},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\KF5D3AS2\\2006-10213-033.html}
}

@inproceedings{jarwarExploitingIoTServices2017,
  title = {Exploiting {{IoT}} Services by Integrating Emotion Recognition in {{Web}} of {{Objects}}},
  booktitle = {2017 {{International Conference}} on {{Information Networking}} ({{ICOIN}})},
  author = {Jarwar, Muhammad Aslam and Chong, Ilyoung},
  year = {2017},
  month = jan,
  pages = {54--56},
  doi = {10.1109/ICOIN.2017.7899474},
  abstract = {Web of Objects (WoO) is an IoT platform, which is an avenue for better support to connect heterogeneous data sources with the web, because of its modular and plug \& play nature. This modular and plug \& play approach of WoO, is an amazing support towards the adaptions of other concepts and technologies. The better support based on ontology and semantic based knowledge creations helps in context aware IoT services. The one aspect of context aware IoT services, is emotion based IoT services. Because emotion is an important aspect of human life, he takes his actions based on his emotions, so the emotions aware IoT services, assist the human in achieving his daily task and makes his life easier. Our approach of integration of emotion recognition and creation of emotion aware IoT services based on WoO is a novel approach towards affective IoT services in WoO.},
  keywords = {Emotion aware IoT Services,Emotion Recognition,Internet of Things,Web of Objects},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\5M6FZ4AZ\\Jarwar und Chong - 2017 - Exploiting IoT services by integrating emotion rec.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\IBRIDTF9\\7899474.html}
}

@article{jiDifferentialPrivacyMachine2014,
  title = {Differential Privacy and Machine Learning: A Survey and Review},
  shorttitle = {Differential Privacy and Machine Learning},
  author = {Ji, Zhanglong and Lipton, Zachary C. and Elkan, Charles},
  year = {2014},
  journal = {arXiv preprint arXiv:1412.7584},
  eprint = {1412.7584},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\GYQPKCLQ\\Ji et al. - 2014 - Differential privacy and machine learning a surve.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\HWZ6H64B\\1412.html}
}

@inproceedings{jiMetaPairwiseRelationship2021,
  title = {Meta {{Pairwise Relationship Distillation}} for {{Unsupervised Person Re-identification}}},
  booktitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Ji, Haoxuanye and Wang, Le and Zhou, Sanping and Tang, Wei and Zheng, Nanning and Hua, Gang},
  year = {2021},
  month = oct,
  pages = {3641--3650},
  issn = {2380-7504},
  doi = {10.1109/ICCV48922.2021.00364},
  abstract = {Unsupervised person re-identification (Re-ID) remains challenging due to the lack of ground-truth labels. Existing methods often rely on estimated pseudo labels via iterative clustering and classification, and they are unfortunately highly susceptible to performance penalties incurred by the inaccurate estimated number of clusters. Alternatively, we propose the Meta Pairwise Relationship Distillation (MPRD) method to estimate the pseudo labels of sample pairs for unsupervised person Re-ID. Specifically, it consists of a Convolutional Neural Network (CNN) and Graph Convolutional Network (GCN), in which the GCN estimates the pseudo labels of sample pairs based on the current features extracted by CNN, and the CNN learns better features by involving high-fidelity positive and negative sample pairs imposed by GCN. To achieve this goal, a small amount of labeled samples are used to guide GCN training, which can distill meta knowledge to judge the difference in the neighborhood structure between positive and negative sample pairs. Extensive experiments on Market-1501, DukeMTMC-reID and MSMT17 datasets show that our method outperforms the state-of-the-art approaches.},
  keywords = {Computer vision,Detection and localization in 2D and 3D,Estimation,Feature extraction,Image color analysis,Representation learning,Training,Transfer/Low-shot/Semi/Unsupervised Learning,Visualization},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\QH4SUXL6\\Ji et al. - 2021 - Meta Pairwise Relationship Distillation for Unsupe.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\BRTY98SG\\9711503.html}
}

@article{johnsonAdvancingNeuroscienceWearable2020,
  title = {Advancing {{Neuroscience}} through {{Wearable Devices}}},
  author = {Johnson, Kristina T. and Picard, Rosalind W.},
  year = {2020},
  journal = {Neuron},
  volume = {108},
  number = {1},
  pages = {8--12},
  issn = {0896-6273},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\9XZN2HBT\\Advancing_NeuroscienceThroughWearableDevices.pdf}
}

@article{kambhampatiSymbolsLinguaFranca2021,
  title = {Symbols as a {{Lingua Franca}} for {{Bridging Human-AI Chasm}} for {{Explainable}} and {{Advisable AI Systems}}},
  author = {Kambhampati, Subbarao and Sreedharan, Sarath and Verma, Mudit and Zha, Yantian and Guan, Lin},
  year = {2021},
  month = dec,
  journal = {arXiv:2109.09904 [cs]},
  eprint = {2109.09904},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Despite the surprising power of many modern AI systems that often learn their own representations, there is significant discontent about their inscrutability and the attendant problems in their ability to interact with humans. While alternatives such as neuro-symbolic approaches have been proposed, there is a lack of consensus on what they are about. There are often two independent motivations (i) symbols as a lingua franca for human-AI interaction and (ii) symbols as system-produced abstractions used by the AI system in its internal reasoning. The jury is still out on whether AI systems will need to use symbols in their internal reasoning to achieve general intelligence capabilities. Whatever the answer there is, the need for (human-understandable) symbols in human-AI interaction seems quite compelling. Symbols, like emotions, may well not be sine qua non for intelligence per se, but they will be crucial for AI systems to interact with us humans -- as we can neither turn off our emotions nor get by without our symbols. In particular, in many human-designed domains, humans would be interested in providing explicit (symbolic) knowledge and advice -- and expect machine explanations in kind. This alone requires AI systems to to maintain a symbolic interface for interaction with humans. In this blue sky paper, we argue this point of view, and discuss research directions that need to be pursued to allow for this type of human-AI interaction.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\E8WZN6RD\\Kambhampati et al. - 2021 - Symbols as a Lingua Franca for Bridging Human-AI C.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\YQL8VYI4\\Kambhampati et al. - 2021 - Symbols as a Lingua Franca for Bridging Human-AI C.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\QLKYCY2R\\2109.html}
}

@article{kamisakaDesignImplementationPedestrian2011,
  title = {Design and Implementation of Pedestrian Dead Reckoning System on a Mobile Phone},
  author = {Kamisaka, Daisuke and Muramatsu, Shigeki and Iwamoto, Takeshi and Yokoyama, Hiroyuki},
  year = {2011},
  journal = {IEICE transactions on information and systems},
  volume = {94},
  number = {6},
  pages = {1137--1146},
  publisher = {{The Institute of Electronics, Information and Communication Engineers}},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\Z7NW6EUZ\\Kamisaka et al. - 2011 - Design and implementation of pedestrian dead recko.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\NNBW3VEF\\summary.html}
}

@inproceedings{kanadeComprehensiveDatabaseFacial2000,
  title = {Comprehensive Database for Facial Expression Analysis},
  booktitle = {Proceedings {{Fourth IEEE International Conference}} on {{Automatic Face}} and {{Gesture Recognition}} ({{Cat}}. {{No}}. {{PR00580}})},
  author = {Kanade, Takeo and Cohn, Jeffrey F. and Tian, Yingli},
  year = {2000},
  pages = {46--53},
  publisher = {{IEEE}},
  keywords = {Datasets},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\DFPQE3V9\\840611.html}
}

@article{kandelEffectBatchSize2020,
  title = {The Effect of Batch Size on the Generalizability of the Convolutional Neural Networks on a Histopathology Dataset},
  author = {Kandel, Ibrahem and Castelli, Mauro},
  year = {2020},
  month = dec,
  journal = {ICT Express},
  volume = {6},
  number = {4},
  pages = {312--315},
  issn = {2405-9595},
  doi = {10.1016/j.icte.2020.04.010},
  abstract = {Many hyperparameters have to be tuned to have a robust convolutional neural network that will be able to accurately classify images. One of the most important hyperparameters is the batch size, which is the number of images used to train a single forward and backward pass. In this study, the effect of batch size on the performance of convolutional neural networks and the impact of learning rates will be studied for image classification, specifically for medical images. To train the network faster, a VGG16 network with ImageNet weights was used in this experiment. Our results concluded that a higher batch size does not usually achieve high accuracy, and the learning rate and the optimizer used will have a significant impact as well. Lowering the learning rate and decreasing the batch size will allow the network to train better, especially in the case of fine-tuning.},
  langid = {english},
  keywords = {Batch size,Convolutional neural networks,Deep learning,Image classification,Medical images},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\IVL4HRUW\\Kandel und Castelli - 2020 - The effect of batch size on the generalizability o.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\QUPYCNB7\\S2405959519303455.html}
}

@article{karimUNICONCombatingLabel2022,
  title = {{{UNICON}}: {{Combating Label Noise Through Uniform Selection}} and {{Contrastive Learning}}},
  shorttitle = {{{UNICON}}},
  author = {Karim, Nazmul and Rizve, Mamshad Nayeem and Rahnavard, Nazanin and Mian, Ajmal and Shah, Mubarak},
  year = {2022},
  month = mar,
  journal = {arXiv:2203.14542 [cs]},
  eprint = {2203.14542},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Supervised deep learning methods require a large repository of annotated data; hence, label noise is inevitable. Training with such noisy data negatively impacts the generalization performance of deep neural networks. To combat label noise, recent state-of-the-art methods employ some sort of sample selection mechanism to select a possibly clean subset of data. Next, an off-the-shelf semi-supervised learning method is used for training where rejected samples are treated as unlabeled data. Our comprehensive analysis shows that current selection methods disproportionately select samples from easy (fast learnable) classes while rejecting those from relatively harder ones. This creates class imbalance in the selected clean set and in turn, deteriorates performance under high label noise. In this work, we propose UNICON, a simple yet effective sample selection method which is robust to high label noise. To address the disproportionate selection of easy and hard samples, we introduce a Jensen-Shannon divergence based uniform selection mechanism which does not require any probabilistic modeling and hyperparameter tuning. We complement our selection method with contrastive learning to further combat the memorization of noisy labels. Extensive experimentation on multiple benchmark datasets demonstrates the effectiveness of UNICON; we obtain an 11.4\% improvement over the current state-of-the-art on CIFAR100 dataset with a 90\% noise rate. Our code is publicly available},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  note = {Comment: Accepted at IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2022},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\D5VIHYP2\\Karim et al. - 2022 - UNICON Combating Label Noise Through Uniform Sele.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\6BWETDIG\\2203.html}
}

@article{karpasMRKLSystemsModular2022,
  title = {{{MRKL Systems}}: {{A}} Modular, Neuro-Symbolic Architecture That Combines Large Language Models, External Knowledge Sources and Discrete Reasoning},
  shorttitle = {{{MRKL Systems}}},
  author = {Karpas, Ehud and Abend, Omri and Belinkov, Yonatan and Lenz, Barak and Lieber, Opher and Ratner, Nir and Shoham, Yoav and Bata, Hofit and Levine, Yoav and {Leyton-Brown}, Kevin and Muhlgay, Dor and Rozen, Noam and Schwartz, Erez and Shachaf, Gal and {Shalev-Shwartz}, Shai and Shashua, Amnon and Tenenholtz, Moshe},
  year = {2022},
  month = may,
  journal = {arXiv:2205.00445 [cs]},
  eprint = {2205.00445},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Huge language models (LMs) have ushered in a new era for AI, serving as a gateway to natural-language-based knowledge tasks. Although an essential element of modern AI, LMs are also inherently limited in a number of ways. We discuss these limitations and how they can be avoided by adopting a systems approach. Conceptualizing the challenge as one that involves knowledge and reasoning in addition to linguistic processing, we define a flexible architecture with multiple neural models, complemented by discrete knowledge and reasoning modules. We describe this neuro-symbolic architecture, dubbed the Modular Reasoning, Knowledge and Language (MRKL, pronounced "miracle") system, some of the technical challenges in implementing it, and Jurassic-X, AI21 Labs' MRKL system implementation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\MFX73ZJD\\Karpas et al. - 2022 - MRKL Systems A modular, neuro-symbolic architectu.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\6UE7564E\\2205.html}
}

@inproceedings{kaurDeepLearningTechnique2021,
  title = {A {{Deep Learning Technique}} for {{Emotion Recognition Using Face}} and {{Voice Features}}},
  booktitle = {2021 {{IEEE Pune Section International Conference}} ({{PuneCon}})},
  author = {Kaur, Sukhpreet and Kulkarni, Nilima},
  year = {2021},
  month = dec,
  pages = {1--6},
  doi = {10.1109/PuneCon52575.2021.9686510},
  abstract = {Automatic emotion recognition is an emerging application in the field of artificial intelligence. Where the use of technology helps to curb various mental illnesses like anxiety, depression, schizophrenia etc. So, there is a need to create efficient artificial intelligent devices. In this experimental project, we developed a multimodal system using neural networks (CNN) for recognizing emotions from facial expressions and voice. We used an RGB images dataset for facial expression recognition. For voice emotion recognition, an audio recorded files dataset was used. We presented phases of automatic emotion recognition in the form of data acquisition, data analysis and data visualization. Comparative analysis among recent papers, features affecting the performance and proposed model in terms of accuracy have also been given in tabular form. The proposed model shows accuracy of 73\% and 90.91\% for face and voice features respectively. It's observed that variations in the factors of the feature-training phase of neural networks like epochs, batch sizes, training datasets, also affect the results.},
  keywords = {Automated emotion recognition,Convolutional neural networks,Data augmentation,Data overfitting,Deep learning,Emotion recognition,Face recognition,Human computer interaction,IEEE Sections,Image recognition,Neural networks,Training},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\ARF4K3TP\\Kaur und Kulkarni - 2021 - A Deep Learning Technique for Emotion Recognition .pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\DVA9669M\\9686510.html}
}

@inproceedings{kennedyConceptualFoundationAutonomous1998,
  title = {A Conceptual Foundation for Autonomous Learning in Unforeseen Situations},
  booktitle = {Proceedings of the 1998 {{IEEE International Symposium}} on {{Intelligent Control}} ({{ISIC}}) Held Jointly with {{IEEE International Symposium}} on {{Computational Intelligence}} in {{Robotics}} and {{Automation}} ({{CIRA}}) {{Intell}}},
  author = {Kennedy, C.M.},
  year = {1998},
  month = sep,
  pages = {483--488},
  issn = {2158-9860},
  doi = {10.1109/ISIC.1998.713709},
  abstract = {A cognitive agent should have the capability to learn autonomously in completely unforeseen situations. "Unforeseen" means something that was not taken into account in an internal representation of the world. However, it is detectable in the form of anomalous sensor measurements. Two problems must be solved: 1) the "newness" of the situation must be detected, i.e. it should not be allocated (wrongly) to an existing category; and 2) new concepts must be learned so that when a similar situation occurs again it is no longer anomalous. A conceptual framework is presented here based on a form of symbol grounding which emphasises a continual distinction between model-driven expectancy and actual reality. Large differences between expectation and reality indicate that a new concept is required which corresponds more accurately to the sensor data. This results in the autonomous growth and change of symbol groundings. Genetic programming is considered as a tool (both on the symbolic and the subsymbolic levels).},
  keywords = {Artificial intelligence,Cognition,Fuels,Fuzzy reasoning,Genetic programming,Grounding,Learning,Stacking,Technological innovation},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\GCD4B5G6\\Kennedy - 1998 - A conceptual foundation for autonomous learning in.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\DUFKARUK\\713709.html}
}

@article{khaireddinFacialEmotionRecognition2021,
  title = {Facial {{Emotion Recognition}}: {{State}} of the {{Art Performance}} on {{FER2013}}},
  shorttitle = {Facial {{Emotion Recognition}}},
  author = {Khaireddin, Yousif and Chen, Zhuofa},
  year = {2021},
  month = may,
  journal = {arXiv:2105.03588 [cs]},
  eprint = {2105.03588},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Facial emotion recognition (FER) is significant for human-computer interaction such as clinical practice and behavioral description. Accurate and robust FER by computer models remains challenging due to the heterogeneity of human faces and variations in images such as different facial pose and lighting. Among all techniques for FER, deep learning models, especially Convolutional Neural Networks (CNNs) have shown great potential due to their powerful automatic feature extraction and computational efficiency. In this work, we achieve the highest single-network classification accuracy on the FER2013 dataset. We adopt the VGGNet architecture, rigorously fine-tune its hyperparameters, and experiment with various optimization methods. To our best knowledge, our model achieves state-of-the-art single-network accuracy of 73.28 \% on FER2013 without using extra training data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  note = {Comment: 9 pages, 5 figures, 2 tables},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\LPGVDQNJ\\Khaireddin und Chen - 2021 - Facial Emotion Recognition State of the Art Perfo.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\K6BMNI4G\\2105.html}
}

@inproceedings{khanDeepLearningBased2021,
  title = {Deep Learning Based {{Intelligent Emotion Recognition}} and {{Classification System}}},
  booktitle = {2021 {{International Conference}} on {{Frontiers}} of {{Information Technology}} ({{FIT}})},
  author = {Khan, Misha Urooj and Abbasi, Muhammad Abdullah and Saeed, Zubair and Asif, Muneeb and Raza, Ali and Urooj, Urooj},
  year = {2021},
  month = dec,
  pages = {25--30},
  doi = {10.1109/FIT53504.2021.00015},
  abstract = {Deep learning techniques for automatic facial emotion recognition (ER) have recently received a lot of attention, however, the models that have been built are still unable to generalize properly due to a lack of large emotion datasets for deep learning. To solve this issue, in this paper we propose to use a MobileNetV2-based transfer learning approach to investigate how knowledge may be gathered inside a specific dataset and how information from ImageNet dataset (14,197,122 images) can be transferred into Kaggle emotion dataset (36082 images) to improve overall performance. To test the reliability of our system, we ran a series of tests using Kaggle emotion datasets containing seven different emotions. The experimental findings show that our emotion recognition system (ERS) system, which obtained 98.70\% accuracy at a learning rate of 0.0001, leads to enhanced emotion detection performance and outperforms previous state-of-the-art methods that use fine-tuning and pre-trained techniques.},
  keywords = {artificial intelligence,Cost function,Data models,deep learning,Deep learning,Emotion recognition,Information technology,MobileNetV2,Reliability,transfer learning,Transfer learning},
  note = {MobileNetV2 Transfer Learning Approach with ImageNet data set, transferred to Kaggle emotion data set},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\NFMPB9CD\\Khan et al. - 2021 - Deep learning based Intelligent Emotion Recognitio.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\4IZE2VBM\\9701424.html}
}

@article{khoaCollaborativeLearningCyberattack2022,
  title = {Collaborative {{Learning}} for {{Cyberattack Detection}} in {{Blockchain Networks}}},
  author = {Khoa, Tran Viet and Son, Do Hai and Hoang, Dinh Thai and Trung, Nguyen Linh and Quynh, Tran Thi Thuy and Nguyen, Diep N. and Ha, Nguyen Viet and Dutkiewicz, Eryk},
  year = {2022},
  month = mar,
  journal = {arXiv:2203.11076 [cs]},
  eprint = {2203.11076},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {This article aims to study intrusion attacks and then develop a novel cyberattack detection framework for blockchain networks. Specifically, we first design and implement a blockchain network in our laboratory. This blockchain network will serve two purposes, i.e., generate the real traffic data (including both normal data and attack data) for our learning models and implement real-time experiments to evaluate the performance of our proposed intrusion detection framework. To the best of our knowledge, this is the first dataset that is synthesized in a laboratory for cyberattacks in a blockchain network. We then propose a novel collaborative learning model that allows efficient deployment in the blockchain network to detect attacks. The main idea of the proposed learning model is to enable blockchain nodes to actively collect data, share the knowledge learned from its data, and then exchange the knowledge with other blockchain nodes in the network. In this way, we can not only leverage the knowledge from all the nodes in the network but also do not need to gather all raw data for training at a centralized node like conventional centralized learning solutions. Such a framework can also avoid the risk of exposing local data's privacy as well as the excessive network overhead/congestion. Both intensive simulations and real-time experiments clearly show that our proposed collaborative learning-based intrusion detection framework can achieve an accuracy of up to 97.7\% in detecting attacks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\QC2JHF4G\\Khoa et al. - 2022 - Collaborative Learning for Cyberattack Detection i.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\ZCRWMXV8\\2203.html}
}

@article{kimHierarchicalCommitteeDeep2016,
  title = {Hierarchical Committee of Deep Convolutional Neural Networks for Robust Facial Expression Recognition},
  author = {Kim, Bo-Kyeong and Roh, Jihyeon and Dong, Suh-Yeon and Lee, Soo-Young},
  year = {2016},
  journal = {Journal on Multimodal User Interfaces},
  volume = {10},
  number = {2},
  pages = {173--189},
  publisher = {{Springer}},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\GNRCC53A\\s12193-015-0209-0.html}
}

@inproceedings{kingmaGlowGenerativeFlow2018,
  title = {Glow: {{Generative Flow}} with {{Invertible}} 1x1 {{Convolutions}}},
  shorttitle = {Glow},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Kingma, Durk P and Dhariwal, Prafulla},
  year = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\A2VJ94EE\\Kingma und Dhariwal - 2018 - Glow Generative Flow with Invertible 1x1 Convolut.pdf}
}

@article{knyazevConvolutionalNeuralNetworks2017,
  title = {Convolutional Neural Networks Pretrained on Large Face Recognition Datasets for Emotion Classification from Video},
  author = {Knyazev, Boris and Shvetsov, Roman and Efremova, Natalia and Kuharenko, Artem},
  year = {2017},
  month = nov,
  journal = {arXiv:1711.04598 [cs]},
  eprint = {1711.04598},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In this paper we describe a solution to our entry for the emotion recognition challenge EmotiW 2017. We propose an ensemble of several models, which capture spatial and audio features from videos. Spatial features are captured by convolutional neural networks, pretrained on large face recognition datasets. We show that usage of strong industry-level face recognition networks increases the accuracy of emotion recognition. Using our ensemble we improve on the previous best result on the test set by about 1 \%, achieving a 60.03 \% classification accuracy without any use of visual temporal information.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: 4 pages},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\5JWP74VU\\Knyazev et al. - 2017 - Convolutional neural networks pretrained on large .pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\8IVUZ8S4\\1711.html}
}

@article{koelstraDeapDatabaseEmotion2011,
  title = {Deap: {{A}} Database for Emotion Analysis; Using Physiological Signals},
  author = {Koelstra, Sander and Muhl, Christian and Soleymani, Mohammad and Lee, Jong-Seok and Yazdani, Ashkan and Ebrahimi, Touradj and Pun, Thierry and Nijholt, Anton and Patras, Ioannis},
  year = {2011},
  journal = {IEEE Transactions on Affective Computing},
  volume = {3},
  number = {1},
  pages = {18--31},
  issn = {1949-3045},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\Y4M6DXLA\\DEAP_Database for Emotion Analysis Using Physi.pdf}
}

@inproceedings{kolodziejAnalysisFacialFeatures2018,
  title = {Analysis of {{Facial Features}} for the {{Use}} of {{Emotion Recognition}}},
  booktitle = {19th {{International Conference Computational Problems}} of {{Electrical Engineering}}},
  author = {Kolodziej, Marcin and Majkowski, Andrzej and Rak, Remigiusz J. and Tarnowski, Pawel and Pielaszkiewicz, Tomasz},
  year = {2018},
  month = sep,
  pages = {1--4},
  doi = {10.1109/CPEE.2018.8507137},
  abstract = {The article presents a face image classification system for emotion recognition. In the first step skin recognition, using the elliptical boundary model, is performed. Then, the detection of facial features takes place. Next, an algorithm for extracting geometric and anthropometric features, from the face image is activated. Finally, training and testing classifiers are performed. We achieved averaged classification accuracy 57.7\% for 6 different emotions (joy, surprise, sadness, anger, fear and disgust) and average accuracy 95.9\% for 2 emotions (joy and surprise).},
  keywords = {Bagged Trees,Classification algorithms,elliptical boundary model,Emotion recognition,Emotion Recognition,Eyebrows,Face,Face Parts Detection,Feature extraction,kNN,Skin,Skin Detection,Support vector machines,SVM},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\3V4WT7LH\\Kolodziej et al. - 2018 - Analysis of Facial Features for the Use of Emotion.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\TFVE32F3\\8507137.html}
}

@inproceedings{kongResolvingTrainingBiases2021,
  title = {Resolving {{Training Biases}} via {{Influence-based Data Relabeling}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Kong, Shuming and Shen, Yanyan and Huang, Linpeng},
  year = {2021},
  month = sep,
  abstract = {The performance of supervised learning methods easily suffers from the training bias issue caused by train-test distribution mismatch or label noise. Influence function is a  technique that...},
  langid = {english},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\E6GDZ54T\\Kong et al. - 2021 - Resolving Training Biases via Influence-based Data.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\IK4QTLFL\\forum.html}
}

@inproceedings{kousalyaPredictionBestOptimizer2022,
  title = {Prediction of {{Best Optimizer}} for {{Facial Expression Detection}} Using {{Convolutional Neural Network}}},
  booktitle = {2022 {{International Conference}} on {{Computer Communication}} and {{Informatics}} ({{ICCCI}})},
  author = {Kousalya, K. and Mohana, R.S. and Jithendiran, E.K. and Kanishk, R.C. and Logesh, T.},
  year = {2022},
  month = jan,
  pages = {1--7},
  issn = {2329-7190},
  doi = {10.1109/ICCCI54379.2022.9740832},
  abstract = {Deep learning is some sort of machine learning which resembles the features of the human's brain while analyzing data and generating patterns in order to draw conclusions. It is a part of machine learning, which in turns a part of artificial intelligence, and it consists of networks that are capable of learning from unsupervised, unstructured, or unlabeled data. When the size of the input data is vast, deep learning can outperform machine learning. Despite traditional programs, which develop insights with data in a linear fashion, deep learning systems include a hierarchical characteristic which permits machines to analyze the data in a non-linear fashion. Deep learning is advised for complicated tasks involving massive quantities of data, such as picture classification, natural language processing, and speech recognition In the scientific world, facial expression recognition is a passionately disputed issue. Human express their feelings using facial expressions. Humans can easily distinguish facial expressions, but computers find it challenging to do so because human facial expressions might fluctuate in photos due to elements such as brightness, contrast, and so on. This research uses seven main human emotions to train computers. Sadness, contempt, fear, happiness, anger, surprise, and neutral are among the emotions. Convolutional Neural Networks have the potential to solve among the most popular complex challenges in the fields such as deep learning and computer vision: facial emotion recognition. The system is being developed in three stages: image pre-processing, detection, and classification. Every iteration, the accuracy will be enhanced by fine-tuning key parameters for example, the initial thick layer's unit count, the rate of dropout in the dropout layer, as well as optimization strategies. Various optimizers, including as Adam, Adagrad, Stochastic Gradient Descent, and Nadam, were utilized to achieve the best accuracy.},
  keywords = {Computers,Convolutional Neural Network (CNN),Convolutional neural networks,Deep learning,Deep Learning,Different Optimizers,Facial Expression Detection,Learning (artificial intelligence),Natural language processing,Neural networks,Speech recognition},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\6SFQDZLI\\Kousalya et al. - 2022 - Prediction of Best Optimizer for Facial Expression.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\RK4J7EQF\\9740832.html}
}

@article{krestelSurveyDeepLearning2021,
  title = {A Survey on Deep Learning for Patent Analysis},
  author = {Krestel, Ralf and Chikkamath, Renukswamy and Hewel, Christoph and Risch, Julian},
  year = {2021},
  month = jun,
  journal = {World Patent Information},
  volume = {65},
  pages = {102035},
  issn = {0172-2190},
  doi = {10.1016/j.wpi.2021.102035},
  abstract = {Patent document collections are an immense source of knowledge for research and innovation communities worldwide. The rapid growth of the number of patent documents poses an enormous challenge for retrieving and analyzing information from this source in an effective manner. Based on deep learning methods for natural language processing, novel approaches have been developed in the field of patent analysis. The goal of these approaches is to reduce costs by automating tasks that previously only domain experts could solve. In this article, we provide a comprehensive survey of the application of deep learning for patent analysis. We summarize the state-of-the-art techniques and describe how they are applied to various tasks in the patent domain. In a detailed discussion, we categorize 40 papers based on the dataset, the representation, and the deep learning architecture that were used, as well as the patent analysis task that was targeted. With our survey, we aim to foster future research at the intersection of patent analysis and deep learning and we conclude by listing promising paths for future work.},
  langid = {english},
  keywords = {Deep learning,Natural language processing,Patent analysis,Text mining},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\ZGY3I2V5\\Krestel et al. - 2021 - A survey on deep learning for patent analysis.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\M69SA773\\S017221902100017X.html}
}

@misc{kristenfrenchYourNewBest2018,
  title = {Your New Best Friend: {{AI}} Chatbot},
  author = {Kristen French},
  year = {2018},
  note = {The following values have no corresponding Zotero field:\\
number: 31.05.2021}
}

@article{krizhevskyImagenetClassificationDeep2012,
  title = {Imagenet Classification with Deep Convolutional Neural Networks},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  year = {2012},
  journal = {Advances in neural information processing systems},
  volume = {25},
  pages = {1097--1105},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\MQRY6S4G\\Krizhevsky et al. - 2012 - Imagenet classification with deep convolutional ne.pdf}
}

@inproceedings{kuangOntologyDrivenHierarchicalDeep2018,
  title = {Ontology-{{Driven Hierarchical Deep Learning}} for {{Fashion Recognition}}},
  booktitle = {2018 {{IEEE Conference}} on {{Multimedia Information Processing}} and {{Retrieval}} ({{MIPR}})},
  author = {Kuang, Zhenzhong and Yu, Jun and Yu, Zhou and Fan, Jianping},
  year = {2018},
  month = apr,
  pages = {19--24},
  doi = {10.1109/MIPR.2018.00012},
  abstract = {We present an automatic approach for large-scale fashion recognition, given an image without any kind of annotation. We formulate the problem as a hierarchical deep learning (HDL) algorithm which can: (i) integrate the deep CNNs to learn more discriminative high-level features for fashion image representations of both coarse-grained and fine-grained classes at different levels of the fashion ontology tree; (ii) leverage multi-task learning and inter-task relationship constraint to train more discriminative classifiers for the nodes on the fashion ontology; (iii) use back propagation to simultaneously refine both the relevant node classifiers and the deep CNNs according to a joint objective function; and (iv) accelerate the fashion retrieval process via path-based classification. The experimental results have verified the effectiveness and efficiency of our proposed algorithm on both classification and retrieval performance.},
  keywords = {Fashion Recognition,Feature extraction,Hardware design languages,Hierarchical Deep Learning,Machine learning,Ontologies,Path based Classification,Task analysis,Training,Visualization},
  note = {Check: Fashion},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\SJHC2XWL\\Kuang et al. - 2018 - Ontology-Driven Hierarchical Deep Learning for Fas.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\PPZ3YCTN\\8396968.html}
}

@article{kulkarniAutomaticRecognitionFacial2018,
  title = {Automatic Recognition of Facial Displays of Unfelt Emotions},
  author = {Kulkarni, Kaustubh and Corneanu, Ciprian and Ofodile, Ikechukwu and Escalera, Sergio and Baro, Xavier and Hyniewska, Sylwia and Allik, Juri and Anbarjafari, Gholamreza},
  year = {2018},
  journal = {IEEE transactions on affective computing},
  publisher = {{IEEE}},
  keywords = {Datasets},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\ZI6FED78\\8493613.html}
}

@inproceedings{kumarComprehensiveStudyMachine2022,
  title = {A {{Comprehensive Study}} on {{Machine Learning Approaches}} for {{Emotion Recognition}}},
  booktitle = {2022 2nd {{International Conference}} on {{Artificial Intelligence}} and {{Signal Processing}} ({{AISP}})},
  author = {Kumar, Nitin and Gupta, Nidhi},
  year = {2022},
  month = feb,
  pages = {1--5},
  issn = {2640-5768},
  doi = {10.1109/AISP53593.2022.9760652},
  abstract = {Emotion recognition is the process to study about the emotions in a human being. This is a research field where one can understand and recognize the feelings of human and ability of expression which varies from each other at great extent. Several methods have been developed to study emotions such as facial expression, speech method, textual method and EEG signal. In this study work, we have reviewed several methods to find an efficiency of emotions up to accurate observations. Several papers on emotion recognition from the year 2007 to 2021 are been explored in this paper to observe the accuracy 95.20\% using electroencephalogram (EEG) signal and 95\% using EEG signals with statistical features and neural network. The average accuracy ranges in between 63\% to 73\% using EEG signal and facial expressions, both.},
  keywords = {Artificial Intelligence,Classification,EEG,Electroencephalography,Emotion recognition,Emotion Recognition,Facial Expression,Machine learning,Machine Learning,Neural networks,Pain,Signal processing,Support vector machines},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\ZMCAENKA\\Kumar und Gupta - 2022 - A Comprehensive Study on Machine Learning Approach.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\XUG6A3XF\\9760652.html}
}

@misc{kumarEmpiricalStudySelfsupervised2022,
  title = {An {{Empirical Study Of Self-supervised Learning Approaches For Object Detection With Transformers}}},
  author = {Kumar, Gokul Karthik and Mullappilly, Sahal Shaji and Gehlot, Abhishek Singh},
  year = {2022},
  month = may,
  number = {arXiv:2205.05543},
  eprint = {2205.05543},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  abstract = {Self-supervised learning (SSL) methods such as masked language modeling have shown massive performance gains by pretraining transformer models for a variety of natural language processing tasks. The follow-up research adapted similar methods like masked image modeling in vision transformer and demonstrated improvements in the image classification task. Such simple self-supervised methods are not exhaustively studied for object detection transformers (DETR, Deformable DETR) as their transformer encoder modules take input in the convolutional neural network (CNN) extracted feature space rather than the image space as in general vision transformers. However, the CNN feature maps still maintain the spatial relationship and we utilize this property to design self-supervised learning approaches to train the encoder of object detection transformers in pretraining and multi-task learning settings. We explore common self-supervised methods based on image reconstruction, masked image modeling and jigsaw. Preliminary experiments in the iSAID dataset demonstrate faster convergence of DETR in the initial epochs in both pretraining and multi-task learning settings; nonetheless, similar improvement is not observed in the case of multi-task learning with Deformable DETR. The code for our experiments with DETR and Deformable DETR are available at https://github.com/gokulkarthik/detr and https://github.com/gokulkarthik/Deformable-DETR respectively.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  note = {Comment: Final Project for the course "Visual Object Detection And Recognition" (CV703) at MBZUAI},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\KMMS6EWR\\Kumar et al. - 2022 - An Empirical Study Of Self-supervised Learning App.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\WW3AJINR\\2205.html}
}

@article{kunhothIndoorPositioningWayfinding2020,
  title = {Indoor Positioning and Wayfinding Systems: A Survey},
  shorttitle = {Indoor Positioning and Wayfinding Systems},
  author = {Kunhoth, Jayakanth and Karkar, AbdelGhani and {Al-Maadeed}, Somaya and {Al-Ali}, Abdulla},
  year = {2020},
  month = may,
  journal = {Human-centric Computing and Information Sciences},
  volume = {10},
  number = {1},
  pages = {18},
  issn = {2192-1962},
  doi = {10.1186/s13673-020-00222-0},
  abstract = {Navigation systems help users access unfamiliar environments. Current technological advancements enable users to encapsulate these systems in handheld devices, which effectively increases the popularity of navigation systems and the number of users. In indoor environments, lack of Global Positioning System (GPS) signals and line of sight with orbiting satellites makes navigation more challenging compared to outdoor environments. Radio frequency (RF) signals, computer vision, and sensor-based solutions are more suitable for tracking the users in indoor environments. This article provides a comprehensive summary of evolution in indoor navigation and indoor positioning technologies. In particular, the paper reviews different computer vision-based indoor navigation and positioning systems along with indoor scene recognition methods that can aid the indoor navigation. Navigation and positioning systems that utilize pedestrian dead reckoning (PDR) methods and various communication technologies, such as Wi-Fi, Radio Frequency Identification (RFID) visible light, Bluetooth and ultra-wide band (UWB), are detailed as well. Moreover, this article investigates and contrasts the different navigation systems in each category. Various evaluation criteria for indoor navigation systems are proposed in this work. The article concludes with a brief insight into future directions in indoor positioning and navigation systems.},
  keywords = {Computer vision,Indoor navigation,Indoor positioning,PDR,RF signals,Visible lights},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\YIKJM9TA\\Kunhoth et al. - 2020 - Indoor positioning and wayfinding systems a surve.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\XGLVFP4U\\s13673-020-00222-0.html}
}

@inproceedings{laiOntologybasedInterpretableMachine2020,
  title = {Ontology-Based {{Interpretable Machine Learning}} for {{Textual Data}}},
  booktitle = {2020 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Lai, Phung and Phan, NhatHai and Hu, Han and Badeti, Anuja and Newman, David and Dou, Dejing},
  year = {2020},
  month = jul,
  pages = {1--10},
  issn = {2161-4407},
  doi = {10.1109/IJCNN48605.2020.9206753},
  abstract = {In this paper, we introduce a novel interpreting framework that learns an interpretable model based on an ontology-based sampling technique to explain agnostic prediction models. Different from existing approaches, our algorithm considers contextual correlation among words, described in domain knowledge ontologies, to generate semantic explanations. To narrow down the search space for explanations, which is a major problem of long and complicated text data, we design a learnable anchor algorithm, to better extract explanations locally. A set of regulations is further introduced, regarding combining learned interpretable representations with anchors to generate comprehensible semantic explanations. An extensive experiment conducted on two real-world datasets shows that our approach generates more precise and insightful explanations compared with baseline approaches.},
  keywords = {anchor,Correlation,Data mining,information extraction,interpretable machine learning,Machine learning,natural language processing,Ontologies,ontology,Prediction algorithms,Predictive models,Semantics},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\MUSHEBC8\\Lai et al. - 2020 - Ontology-based Interpretable Machine Learning for .pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\HTPEFPTI\\9206753.html}
}

@article{lecunGradientbasedLearningApplied1998,
  title = {Gradient-Based Learning Applied to Document Recognition},
  author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  year = {1998},
  month = nov,
  journal = {Proceedings of the IEEE},
  volume = {86},
  number = {11},
  pages = {2278--2324},
  issn = {1558-2256},
  doi = {10.1109/5.726791},
  abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
  keywords = {Character recognition,Feature extraction,Hidden Markov models,Machine learning,Multi-layer neural network,Neural networks,Optical character recognition software,Optical computing,Pattern recognition,Principal component analysis},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\ZBI9R6Q8\\Lecun et al. - 1998 - Gradient-based learning applied to document recogn.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\L6PEPXHP\\726791.html}
}

@inproceedings{lengResearchOptimizingFacial2021,
  title = {Research on {{Optimizing Facial Expression Recognition Based}} on {{Convolutional Neural Network}}},
  booktitle = {2021 {{International Conference}} on {{Signal Processing}} and {{Machine Learning}} ({{CONF-SPML}})},
  author = {Leng, Zirui},
  year = {2021},
  month = nov,
  pages = {306--309},
  doi = {10.1109/CONF-SPML54095.2021.00066},
  abstract = {With the development of deep learning in recent years, artificial intelligence has been widely applied in daily lives, industries, and services, which has attracted widespread attention. Based on the above application, this paper studies the typical application technology of artificial intelligence, and builds an ``emotional intelligence'' model using traditional facial emotion recognition as an example, accelerating the response of the model as much as possible while ensuring correct recognition.},
  keywords = {Computers,convolutional neural network,Deep learning,Emotion recognition,Face recognition,facial expression recognition,Human computer interaction,Industries,pruning,Signal processing},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\NG6EGW2E\\Leng - 2021 - Research on Optimizing Facial Expression Recogniti.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\59BM3TF2\\9706797.html}
}

@inproceedings{liAutomaticDepressionLevel2022,
  title = {Automatic {{Depression Level Assessment}} from {{Speech By Long-Term Global Information Embedding}}},
  booktitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Li, Ya and Niu, Mingyue and Zhao, Ziping and Tao, Jianhua},
  year = {2022},
  month = may,
  pages = {8507--8511},
  issn = {2379-190X},
  doi = {10.1109/ICASSP43922.2022.9747292},
  abstract = {Depression is a serious mood disorder which brings negative effects on people's social activities. Therefore, growing attention has been paid to automatic depression assessment, especially from speech. However, most of the previous work uses hand-crafted features or deep neural network-based feature extractors to obtain deep features and then feed them into a classifier or a regression, which ignores the temporal relation of these features. To address this issue, this paper proposes a global information embedding (GIE) to make use of the long-term global information of depression and re-weight the LSTM output sequence. The short-term features are then pooled into long-term features by LASSO optimization to further improve the accuracy of depression recognition. Experiments on AVEC 2013 and AVEC 2014 verified the proposed method, and the RMSEs are 9.63 and 9.40, respectively.},
  keywords = {attention,Correlation,depression,Depression,Emotion recognition,Feature extraction,global information embedding,LSTM,Mood,Signal processing,speech processing,Speech recognition},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\YN2PJY93\\Li et al. - 2022 - Automatic Depression Level Assessment from Speech .pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\Z3IAMY8C\\9747292.html}
}

@article{liDeepFacialExpression2020,
  title = {Deep Facial Expression Recognition: {{A}} Survey},
  shorttitle = {Deep Facial Expression Recognition},
  author = {Li, Shan and Deng, Weihong},
  year = {2020},
  journal = {IEEE transactions on affective computing},
  publisher = {{IEEE}},
  keywords = {Affect,Databases,Deep learning,Deep Learning,Face recognition,Facial Expression Datasets,Facial Expression Recognition,Lighting,Neural networks,read,Survey,Three-dimensional displays,Training data},
  note = {Review on Machine Learning Models},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\VM4ENYJT\\Li und Deng - 2020 - Deep facial expression recognition A survey.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\HHCY2EZ4\\9039580.html;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\HQCL27KZ\\9039580.html}
}

@article{liFacialExpressionRecognition2021,
  title = {Facial {{Expression Recognition}} with {{Identity}} and {{Emotion Joint Learning}}},
  author = {Li, Ming and Xu, Hao and Huang, Xingchang and Song, Zhanmei and Liu, Xiaolin and Li, Xin},
  year = {2021},
  month = apr,
  journal = {IEEE Transactions on Affective Computing},
  volume = {12},
  number = {2},
  pages = {544--550},
  issn = {1949-3045},
  doi = {10.1109/TAFFC.2018.2880201},
  abstract = {Different subjects may express a specific expression in different ways due to inter-subject variabilities. In this work, besides training deep-learned facial expression feature (emotional feature), we also consider the influence of latent face identity feature such as the shape or appearance of face. We propose an identity and emotion joint learning approach with deep convolutional neural networks (CNNs) to enhance the performance of facial expression recognition (FER) tasks. First, we learn the emotion and identity features separately using two different CNNs with their corresponding training data. Second, we concatenate these two features together as a deep-learned Tandem Facial Expression (TFE) Feature and feed it to the subsequent fully connected layers to form a new model. Finally, we perform joint learning on the newly merged network using only the facial expression training data. Experimental results show that our proposed approach achieves 99.31 and 84.29 percent accuracy on the CK+ and the FER+ database, respectively, which outperforms the residual network baseline as well as many other state-of-the-art methods.},
  keywords = {Convolution,emotion recognition,Emotion recognition,face recognition,Face recognition,Facial expression recognition,Feature extraction,joint learning,Learning data,Training data,transfer learning,Transfer learning},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\RBXZXAZT\\Li et al. - 2021 - Facial Expression Recognition with Identity and Em.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\IY6ZPTUQ\\8528894.html}
}

@article{liGMSSGraphBasedMultiTask2022,
  title = {{{GMSS}}: {{Graph-Based Multi-Task Self-Supervised Learning}} for {{EEG Emotion Recognition}}},
  shorttitle = {{{GMSS}}},
  author = {Li, Yang and Chen, Ji and Li, Fu and Fu, Boxun and Wu, Hao and Ji, Youshuo and Zhou, Yijin and Niu, Yi and Shi, Guangming and Zheng, Wenming},
  year = {2022},
  journal = {IEEE Transactions on Affective Computing},
  pages = {1--1},
  issn = {1949-3045},
  doi = {10.1109/TAFFC.2022.3170428},
  abstract = {Previous electroencephalogram (EEG) emotion recognition relies on single-task learning, which may lead to overfitting and learned emotion features lacking generalization. In this paper, a graph-based multi-task self-supervised learning model (GMSS) for EEG emotion recognition is proposed. GMSS has the ability to learn more general representations by integrating multiple self-supervised tasks, including spatial and frequency jigsaw puzzle tasks, and contrastive learning tasks. By learning from multiple tasks simultaneously, GMSS can find a representation that captures all of the tasks thereby decreasing the chance of overfitting on the original task, i.e., emotion recognition task. In particular, the spatial jigsaw puzzle task aims to capture the intrinsic spatial relationships of different brain regions. Considering the importance of frequency information in EEG emotional signals, the goal of the frequency jigsaw puzzle task is to explore the crucial frequency bands for EEG emotion recognition. To further regularize the learned features and encourage the network to learn inherent representations, contrastive learning task is adopted in this work by mapping the transformed data into a common feature space. Experiments on SEED, SEED-IV, and MPED datasets show that the proposed model has remarkable advantages in learning more discriminative and general features for EEG emotional signals.},
  keywords = {Brain modeling,Convolution,EEG emotion recognition,Electroencephalography,Emotion recognition,Feature extraction,graph neural network,multi-task learning,Multitasking,self-supervised learning,Task analysis},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\KUDFF7X7\\Li et al. - 2022 - GMSS Graph-Based Multi-Task Self-Supervised Learn.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\XRRMUVEI\\9765326.html}
}

@article{liImageBasedIndoorLocalization2021,
  title = {Image-{{Based Indoor Localization Using Smartphone Camera}}},
  author = {Li, Shuang and Yu, Baoguo and Jin, Yi and Huang, Lu and Zhang, Heng and Liang, Xiaohu},
  year = {2021},
  month = jul,
  journal = {Wireless Communications and Mobile Computing},
  volume = {2021},
  pages = {e3279059},
  publisher = {{Hindawi}},
  issn = {1530-8669},
  doi = {10.1155/2021/3279059},
  abstract = {With the increasing demand for location-based services such as railway stations, airports, and shopping malls, indoor positioning technology has become one of the most attractive research areas. Due to the effects of multipath propagation, wireless-based indoor localization methods such as WiFi, bluetooth, and pseudolite have difficulty achieving high precision position. In this work, we present an image-based localization approach which can get the position just by taking a picture of the surrounding environment. This paper proposes a novel approach which classifies different scenes based on deep belief networks and solves the camera position with several spatial reference points extracted from depth images by the perspective--point algorithm. To evaluate the performance, experiments are conducted on public data and real scenes; the result demonstrates that our approach can achieve submeter positioning accuracy. Compared with other methods, image-based indoor localization methods do not require infrastructure and have a wide range of applications that include self-driving, robot navigation, and augmented reality.},
  langid = {english},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\KC2S62YB\\Li et al. - 2021 - Image-Based Indoor Localization Using Smartphone C.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\FZ6W6JWS\\3279059.html}
}

@inproceedings{liLabelSimilarityAttention2022,
  title = {A {{Label Similarity Attention Mechanism}} for {{Multi-label Emotion Recognition}}},
  booktitle = {2022 3rd {{International Conference}} on {{Electronic Communication}} and {{Artificial Intelligence}} ({{IWECAI}})},
  author = {Li, Lin and Liu, Fan and Huang, JiangPing},
  year = {2022},
  month = jan,
  pages = {392--396},
  doi = {10.1109/IWECAI55315.2022.00083},
  abstract = {In recent years, multi-label emotion recognition has attracted more and more attention, and its purpose is to identify the emotional state of a text, such as happiness and like. At present, most of the methods to solve the problem of multi-label emotion recognition focus on analyzing the semantics of the text, ignoring the semantic information brought by emotion tags, and not modeling the semantic relationship between emotion labels and text. In this paper, a method of multi-label emotion recognition based on label similarity attention mechanism is proposed. In this method, bidirectional LSTM (Long Short-Term Memory) is used to obtain the context knowledge of the text, so as to achieve the purpose of learning the global information of the text. The label features are learned by the label encoder. The label similarity attention mechanism is used to obtain the correlation matrix between the label and the word representation, and further strengthen the semantic information. At the same time, CNN (Convolutional Neural Networks) is used to obtain local semantic information. The semantic representation of sentences is enriched. Finally, emotion recognition is carried out through the classifier. In the classifier, this paper proposes a joint loss function, which enables the model to learn the emotional characteristics of texts and labels at the same time. The model combines the global and local information of the text to show the semantic relationship between the text and the emotional label. The experimental results on the NLPCC2014Task1 dataset show that the proposed model has a certain performance improvement compared with the benchmark method.},
  keywords = {Analytical models,Attention mechanism,Benchmark testing,Blogs,Codes,Correlation,Emotion recognition,Multi-label classification,Semantics,Sentiment classification},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\MA798IAN\\Li et al. - 2022 - A Label Similarity Attention Mechanism for Multi-l.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\UHIPX3XS\\9750847.html}
}

@inproceedings{limkarIntelligentEmotionRecognition2021,
  title = {Intelligent Emotion Recognition System Using Natural Language Processing and {{BERT}}},
  booktitle = {The 2nd {{International Conference}} on {{Distributed Sensing}} and {{Intelligent Systems}} ({{ICDSIS}} 2021)},
  author = {Limkar, S. and Itkarkar, A. and Kasat, Y. and Mandale, N.},
  year = {2021},
  month = jul,
  volume = {2021},
  pages = {259--270},
  doi = {10.1049/icp.2021.2681},
  abstract = {Emotions have always been one of the important aspects of our lives that influences everything. Being able to recognize emotions can help us in various domains such as psychology, human-computer interaction, e-learning, marketing and many more. A comparative study has been done on different approaches i.e., Traditional Machine Learning approach, Neural Network approach and Bidirectional Encoder Representations from Transformers (BERT) to evaluate which approach gives us the maximum accuracy, it has been evaluated on a dataset that was combined from dailydialog, isear, and emotion-stimulus datasets to create a balanced dataset with five labels: joy, sad, anger, fear, and neutral. We'll be concentrating on implicit emotion recognition, which is a challenging problem to tackle because emotion is hidden inside the text and solving it necessitates identifying the text's context. The resulting intelligent system will display emotions extracted from the text, which will be divided into five categories: joy, sadness, fear, rage, and neutral. The accuracy of the model was 91.72 percent, which is much higher than that of typical machine learning and neural network approaches.},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\NZUB7SNG\\Limkar et al. - 2021 - Intelligent emotion recognition system using natur.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\C853KR5Q\\9740095.html}
}

@inproceedings{linDesigningOntologyEmotiondriven2017,
  title = {Designing an Ontology for Emotion-Driven Visual Representations},
  booktitle = {2017 {{IEEE International Conference}} on {{Bioinformatics}} and {{Biomedicine}} ({{BIBM}})},
  author = {Lin, Rebecca and Amith, Muhammad Tuan and Liang, Chen and Tao, Cui},
  year = {2017},
  month = nov,
  pages = {1280--1283},
  doi = {10.1109/BIBM.2017.8217844},
  abstract = {Emotions influence our perceptions and decisions and are often felt more strongly in situations related to healthcare. Therefore, it is important to understand how both providers and patients express their emotions in face-to-face scenarios. An ontology is a way to represent domain concepts and the relationships between them in a polyarchical manner. We have created an ontological model called the Visualized Emotion Ontology (VEO) that expresses the semantic definitions and visualizations of 25 emotions based on published research. With VEO, we can augment patient-facing software tools, like embodied conversational agents, to improve patient-provider interaction in clinical environments.},
  keywords = {Color,crowdsourcing,Data visualization,graphical user interfaces,Image color analysis,knowledge engineering,knowledge representation,Medical services,Ontologies,public healthcare,semantic web,Shape,software agents,Visualization},
  note = {Seems to be less relevant},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\23PLUHYX\\Lin et al. - 2017 - Designing an ontology for emotion-driven visual re.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\3IYH6FRW\\8217844.html}
}

@inproceedings{liReliableCrowdsourcingDeep2017,
  title = {Reliable {{Crowdsourcing}} and {{Deep Locality-Preserving Learning}} for {{Expression Recognition}} in the {{Wild}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Li, Shan and Deng, Weihong and Du, JunPing},
  year = {2017},
  month = jul,
  pages = {2584--2593},
  publisher = {{IEEE}},
  address = {{Honolulu, HI}},
  doi = {10.1109/CVPR.2017.277},
  abstract = {Past research on facial expressions have used relatively limited datasets, which makes it unclear whether current methods can be employed in real world. In this paper, we present a novel database, RAF-DB, which contains about 30000 facial images from thousands of individuals. Each image has been individually labeled about 40 times, then EM algorithm was used to filter out unreliable labels. Crowdsourcing reveals that real-world faces often express compound emotions, or even mixture ones. For all we know, RAF-DB is the first database that contains compound expressions in the wild. Our cross-database study shows that the action units of basic emotions in RAF-DB are much more diverse than, or even deviate from, those of labcontrolled ones. To address this problem, we propose a new DLP-CNN (Deep Locality-Preserving CNN) method, which aims to enhance the discriminative power of deep features by preserving the locality closeness while maximizing the inter-class scatters. The benchmark experiments on the 7class basic expressions and 11-class compound expressions, as well as the additional experiments on SFEW and CK+ databases, show that the proposed DLP-CNN outperforms the state-of-the-art handcrafted features and deep learning based methods for the expression recognition in the wild.},
  isbn = {978-1-5386-0457-1},
  langid = {english},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\62T8PZQG\\Li et al. - 2017 - Reliable Crowdsourcing and Deep Locality-Preservin.pdf}
}

@article{liReliableCrowdsourcingDeep2018,
  title = {Reliable Crowdsourcing and Deep Locality-Preserving Learning for Unconstrained Facial Expression Recognition},
  author = {Li, Shan and Deng, Weihong},
  year = {2018},
  journal = {IEEE Transactions on Image Processing},
  volume = {28},
  number = {1},
  pages = {356--370},
  publisher = {{IEEE}},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\56L48HPL\\Li und Deng - 2018 - Reliable crowdsourcing and deep locality-preservin.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\Q3S4UL8S\\8453893.html}
}

@inproceedings{liuAdaptiveDeepMetric2017,
  title = {Adaptive {{Deep Metric Learning}} for {{Identity-Aware Facial Expression Recognition}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Liu, Xiaofeng and Kumar, B. V. K. Vijaya and You, Jane and Jia, Ping},
  year = {2017},
  month = jul,
  pages = {522--531},
  issn = {2160-7516},
  doi = {10.1109/CVPRW.2017.79},
  abstract = {A key challenge of facial expression recognition (FER) is to develop effective representations to balance the complex distribution of intra- and inter- class variations. The latest deep convolutional networks proposed for FER are trained by penalizing the misclassification of images via the softmax loss. In this paper, we show that better FER performance can be achieved by combining the deep metric loss and softmax loss in a unified two fully connected layer branches framework via joint optimization. A generalized adaptive (N+M)-tuplet clusters loss function together with the identity-aware hard-negative mining and online positive mining scheme are proposed for identity-invariant FER. It reduces the computational burden of deep metric learning, and alleviates the difficulty of threshold validation and anchor selection. Extensive evaluations demonstrate that our method outperforms many state-of-art approaches on the posed as well as spontaneous facial expression databases.},
  keywords = {Conferences,Face recognition,Feature extraction,Measurement,Optimization,Training},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\YMYRZI9B\\Liu et al. - 2017 - Adaptive Deep Metric Learning for Identity-Aware F.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\YSL2XW4D\\8014813.html}
}

@article{liuDeepLearningBasedReasoning2019,
  title = {Deep {{Learning-Based Reasoning With Multi-Ontology}} for {{IoT Applications}}},
  author = {Liu, Jin and Zhang, Xin and Li, Yunhui and Wang, Jin and Kim, Hye-Jin},
  year = {2019},
  journal = {IEEE Access},
  volume = {7},
  pages = {124688--124701},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2937353},
  abstract = {In the era of mobile big data, data driven intelligent Internet of Things (IoT) applications are becoming widespread, and knowledge-based reasoning is one of the essential tasks of these applications. While most knowledge-based reasoning work is conducted with knowledge graph, ontology-based reasoning method can inherently achieve higher level intelligence by leveraging both explicit and tacit knowledge in specific domains, and its performance is determined by precise refinement of the inference rules. However, most ontology-based reasoning work concentrates on semantic reasoning in a single ontology, and fail to utilize association of multiple ontologies in various domains to extend reasoning capacity. This is even the case for the IoT applications where knowledge from multiple domains needs to be utilized. To overcome this issue, we propose a deep learning-based method to associate multiple ontology rule bases, thereby discover new inference rules. In our method, we first use a regression tree model to determine the threshold value for parameters in inference rules that constitute the ontology rule base, avoiding the influence of uncertainty factors on knowledge reasoning results. Then, a two-way GRU (Gated Recurrent Unit) neural network with attention mechanism is used to discover semantic relations among the rule bases of ontologies. Therefore, the association of multiple ontology rule bases is realized, and the rule base of knowledge reasoning is expanded by acquiring some unspecified rules. To the best our knowledge, this work is the first one to leverage deep learning in reasoning with multiple ontologies. In order to verify the effectiveness of our method, we apply it in a real traffic safety monitoring application by relating rule bases of a vehicle ontology and a traffic management ontology, and achieve effective knowledge reasoning.},
  keywords = {Big Data,Cognition,data mining,Data mining,Deep learning,Internet of Things,IoT,Knowledge based systems,Ontologies,ontology based reasoning,Semantics,sensor ontology},
  note = {Check: ~To the best our knowledge, this work is the first one to leverage deep learning in reasoning with multiple ontologies.},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\5JIEH4XN\\Liu et al. - 2019 - Deep Learning-Based Reasoning With Multi-Ontology .pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\XEHY9JPK\\8812717.html}
}

@article{liuReviewEmotionRecognition2021,
  title = {Review on {{Emotion Recognition Based}} on {{Electroencephalography}}},
  author = {Liu, Haoran and Zhang, Ying and Li, Yujun and Kong, Xiangyi},
  year = {2021},
  month = oct,
  journal = {Frontiers in Computational Neuroscience},
  volume = {15},
  pages = {758212},
  publisher = {{Frontiers Media Sa}},
  address = {{Lausanne}},
  doi = {10.3389/fncom.2021.758212},
  abstract = {Emotions are closely related to human behavior, family, and society. Changes in emotions can cause differences in electroencephalography (EEG) signals, which show different emotional states and are not easy to disguise. EEG-based emotion recognition has been widely used in human-computer interaction, medical diagnosis, military, and other fields. In this paper, we describe the common steps of an emotion recognition algorithm based on EEG from data acquisition, preprocessing, feature extraction, feature selection to classifier. Then, we review the existing EEG-based emotional recognition methods, as well as assess their classification effect. This paper will help researchers quickly understand the basic theory of emotion recognition and provide references for the future development of EEG. Moreover, emotion is an important representation of safety psychology.{$<$}/p{$>$}},
  langid = {english},
  keywords = {classification,convolution neural network,deap,dreamer,eeg,emotion recognition,feature-extraction,ocular artifacts,seed},
  annotation = {WOS:000709054400001},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\4TLDMZVP\\Liu et al. - 2021 - Review on Emotion Recognition Based on Electroence.pdf}
}

@inproceedings{liuSpatialTemporalCorrelationTopology2021,
  title = {Spatial-{{Temporal Correlation}} and {{Topology Learning}} for {{Person Re-Identification}} in {{Videos}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Liu, Jiawei and Zha, Zheng-Jun and Wu, Wei and Zheng, Kecheng and Sun, Qibin},
  year = {2021},
  month = jun,
  pages = {4368--4377},
  publisher = {{IEEE}},
  address = {{Nashville, TN, USA}},
  doi = {10.1109/CVPR46437.2021.00435},
  abstract = {Video-based person re-identification aims to match pedestrians from video sequences across non-overlapping camera views. The key factor for video person reidentification is to effectively exploit both spatial and temporal clues from video sequences. In this work, we propose a novel Spatial-Temporal Correlation and Topology Learning framework (CTL) to pursue discriminative and robust representation by modeling cross-scale spatial-temporal correlation. Specifically, CTL utilizes a CNN backbone and a key-points estimator to extract semantic local features from human body at multiple granularities as graph nodes. It explores a context-reinforced topology to construct multiscale graphs by considering both global contextual information and physical connections of human body. Moreover, a 3D graph convolution and a cross-scale graph convolution are designed, which facilitate direct cross-spacetime and cross-scale information propagation for capturing hierarchical spatial-temporal dependencies and structural information. By jointly performing the two convolutions, CTL effectively mines comprehensive clues that are complementary with appearance information to enhance representational capacity. Extensive experiments on two video benchmarks have demonstrated the effectiveness of the proposed method and the state-of-the-art performance.},
  isbn = {978-1-66544-509-2},
  langid = {english},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\3XY3NEIZ\\Liu et al. - 2021 - Spatial-Temporal Correlation and Topology Learning.pdf}
}

@article{liuSwinTransformerHierarchical2021,
  title = {Swin {{Transformer}}: {{Hierarchical Vision Transformer}} Using {{Shifted Windows}}},
  shorttitle = {Swin {{Transformer}}},
  author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  year = {2021},
  month = aug,
  journal = {arXiv:2103.14030 [cs]},
  eprint = {2103.14030},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with \textbackslash textbf\{S\}hifted \textbackslash textbf\{win\}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at\textasciitilde\textbackslash url\{https://github.com/microsoft/Swin-Transformer\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\CA65ERAQ\\Liu et al. - 2021 - Swin Transformer Hierarchical Vision Transformer .pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\HB3M7R5S\\2103.html}
}

@inproceedings{loobDominantComplementaryMultiemotional2017,
  title = {Dominant and Complementary Multi-Emotional Facial Expression Recognition Using c-Support Vector Classification},
  booktitle = {2017 12th {{IEEE International Conference}} on {{Automatic Face}} \& {{Gesture Recognition}} ({{FG}} 2017)},
  author = {Loob, Christer and Rasti, Pejman and L{\"u}si, Iiris and Jacques, Julio CS and Bar{\'o}, Xavier and Escalera, Sergio and Sapinski, Tomasz and Kaminska, Dorota and Anbarjafari, Gholamreza},
  year = {2017},
  pages = {833--838},
  publisher = {{IEEE}},
  keywords = {Datasets},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\Y53PBFAB\\7961829.html}
}

@article{lundqvistAveragedKarolinskaDirected1998,
  title = {The Averaged {{Karolinska}} Directed Emotional Faces-{{AKDEF}}},
  author = {Lundqvist, Daniel and Litton, J. E.},
  year = {1998},
  journal = {AKDEF CD ROM. Psychology section, Karolinska Institutet, Stockholm},
  note = {Belongs to AKDEF-original data set}
}

@article{lundqvistKarolinskaDirectedEmotional1998,
  title = {The {{Karolinska Directed Emotional Faces}} ({{KDEF}}), 1998},
  author = {Lundqvist, D. and Flykt, A. and {\"O}hman, A.},
  year = {1998},
  journal = {Department of Neurosciences Karolinska Hospital: Stockholm, Sweden},
  note = {Belongs to KDEF-original data set}
}

@article{lundqvistKarolinskaDirectedEmotional1998a,
  title = {Karolinska Directed Emotional Faces},
  author = {Lundqvist, Daniel and Flykt, Anders and {\"O}hman, Arne},
  year = {1998},
  journal = {Cognition and Emotion},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\G5V3PBIC\\doiLanding.html}
}

@inproceedings{lusiJointChallengeDominant2017,
  title = {Joint Challenge on Dominant and Complementary Emotion Recognition Using Micro Emotion Features and Head-Pose Estimation: {{Databases}}},
  shorttitle = {Joint Challenge on Dominant and Complementary Emotion Recognition Using Micro Emotion Features and Head-Pose Estimation},
  booktitle = {2017 12th {{IEEE International Conference}} on {{Automatic Face}} \& {{Gesture Recognition}} ({{FG}} 2017)},
  author = {L{\"u}si, Iiris and Junior, Julio CS Jacques and Gorbova, Jelena and Bar{\'o}, Xavier and Escalera, Sergio and Demirel, Hasan and Allik, Juri and Ozcinar, Cagri and Anbarjafari, Gholamreza},
  year = {2017},
  pages = {809--813},
  publisher = {{IEEE}},
  keywords = {Datasets},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\FE7WY8NA\\7961825.html}
}

@inproceedings{lyonsCodingFacialExpressions1998,
  title = {Coding Facial Expressions with {{Gabor}} Wavelets},
  booktitle = {Proceedings {{Third IEEE International Conference}} on {{Automatic Face}} and {{Gesture Recognition}}},
  author = {Lyons, M. and Akamatsu, S. and Kamachi, M. and Gyoba, J.},
  year = {1998},
  month = apr,
  pages = {200--205},
  doi = {10.1109/AFGR.1998.670949},
  abstract = {A method for extracting information about facial expressions from images is presented. Facial expression images are coded using a multi-orientation multi-resolution set of Gabor filters which are topographically ordered and aligned approximately with the face. The similarity space derived from this representation is compared with one derived from semantic ratings of the images by human observers. The results show that it is possible to construct a facial expression classifier with Gabor coding of the facial images as the input stage. The Gabor representation shows a significant degree of psychological plausibility, a design feature which may be important for human-computer interfaces.},
  keywords = {Data mining,Face recognition,Gabor filters,Humans,Image coding,Information processing,Optical filters,Psychology,Read only memory,Skin},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\QXQMPQ3M\\Lyons et al. - 1998 - Coding facial expressions with Gabor wavelets.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\SRKSNQBN\\670949.html}
}

@article{lyonsCodingFacialExpressions2020,
  title = {Coding {{Facial Expressions}} with {{Gabor Wavelets}} ({{IVC Special Issue}})},
  author = {Lyons, Michael J. and Kamachi, Miyuki and Gyoba, Jiro},
  year = {2020},
  month = sep,
  journal = {arXiv:2009.05938 [cs]},
  eprint = {2009.05938},
  eprinttype = {arxiv},
  primaryclass = {cs},
  doi = {10.5281/zenodo.4029679},
  abstract = {We present a method for extracting information about facial expressions from digital images. The method codes facial expression images using a multi-orientation, multi-resolution set of Gabor filters that are topographically ordered and approximately aligned with the face. A similarity space derived from this code is compared with one derived from semantic ratings of the images by human observers. Interestingly the low-dimensional structure of the image-derived similarity space shares organizational features with the circumplex model of affect, suggesting a bridge between categorical and dimensional representations of facial expression. Our results also indicate that it would be possible to construct a facial expression classifier based on a topographically-linked multi-orientation, multi-resolution Gabor coding of the facial images at the input stage. The significant degree of psychological plausibility exhibited by the proposed code may also be useful in the design of human-computer interfaces.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Human-Computer Interaction,Datasets,I.0},
  note = {Comment: 13 pages, 7 figures, 23 references},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\D58A6SPD\\Lyons et al. - 2020 - Coding Facial Expressions with Gabor Wavelets (IVC.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\GX9D7MAR\\2009.html}
}

@inproceedings{malhotraSmartArtificialIntelligence2022,
  title = {Smart {{Artificial Intelligence Based Online Proctoring System}}},
  booktitle = {2022 {{IEEE Delhi Section Conference}} ({{DELCON}})},
  author = {Malhotra, Neil and Suri, Ram and Verma, Puru and Kumar, Rajesh},
  year = {2022},
  month = feb,
  pages = {1--5},
  doi = {10.1109/DELCON54057.2022.9753313},
  abstract = {Since COVID 19, there have been significant advancements in the field of teaching and learning. Academic institutions are going digital to provide their students more resources. Due to technology, students now have more alternatives to study and improve skills at their own pace. In terms of assessments, there has been a shift toward online tests. The absence of a physical invigilator is perhaps the most significant impediment in online mode. Henceforth, online proctoring services are becoming more popular, and AI-powered proctoring solutions are becoming demanding. In this project, we describe a strategy for avoiding the physical presence of a proctor during the test by developing a multi-modal system. We captured video using a webcam along active window capture. The face of the test taker is identified and analyzed to forecast his emotions. To identify his head pose, his feature points are identified. Furthermore, aspects including a phone, a book, or the presence of another person are detected. This combination of models creates an intelligent rule-based inference system which is capable of determining if any malpractice took place during the examination.},
  keywords = {Conferences,COVID-19,Emotion recognition,Emotion Recognition,Face Detection,Face recognition,Head Pose Estimation,IEEE Sections,Online Exam Proctoring,Phone and Book Detection,Visualization,Webcams},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\3WXT5EHY\\Malhotra et al. - 2022 - Smart Artificial Intelligence Based Online Proctor.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\XLULS8IF\\9753313.html}
}

@article{mardagaPersonalityTraitsModulate2006,
  title = {Personality Traits Modulate Skin Conductance Response to Emotional Pictures: {{An}} Investigation with {{Cloninger}}'s Model of Personality},
  shorttitle = {Personality Traits Modulate Skin Conductance Response to Emotional Pictures},
  author = {Mardaga, Solange and Laloyaux, Olivier and Hansenne, Michel},
  year = {2006},
  journal = {Personality and individual differences},
  volume = {40},
  number = {8},
  pages = {1603--1614},
  publisher = {{Elsevier}},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\JDIBBMM6\\Mardaga et al. - 2006 - Personality traits modulate skin conductance respo.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\ZY4LCH2N\\S0191886906000316.html}
}

@article{marin-moralesAffectiveComputingVirtual2018,
  title = {Affective Computing in Virtual Reality: Emotion Recognition from Brain and Heartbeat Dynamics Using Wearable Sensors},
  author = {{Mar{\'i}n-Morales}, Javier and {Higuera-Trujillo}, Juan Luis and Greco, Alberto and Guixeres, Jaime and Llinares, Carmen and Scilingo, Enzo Pasquale and Alca{\~n}iz, Mariano and Valenza, Gaetano},
  year = {2018},
  journal = {Scientific reports},
  volume = {8},
  number = {1},
  pages = {1--15},
  issn = {2045-2322},
  keywords = {VR and AI},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\ULZ5VNY4\\AffectiveComputinginReality.pdf}
}

@article{maussMeasuresEmotionReview2009a,
  title = {Measures of Emotion: {{A}} Review},
  shorttitle = {Measures of Emotion},
  author = {Mauss, Iris B. and Robinson, Michael D.},
  year = {2009},
  month = feb,
  journal = {Cognition \& emotion},
  volume = {23},
  number = {2},
  pages = {209--237},
  issn = {0269-9931},
  doi = {10.1080/02699930802204677},
  abstract = {A consensual, componential model of emotions conceptualises them as experiential, physiological, and behavioural responses to personally meaningful stimuli. The present review examines this model in terms of whether different types of emotion-evocative stimuli are associated with discrete and invariant patterns of responding in each response system, how such responses are structured, and if such responses converge across different response systems. Across response systems, the bulk of the available evidence favours the idea that measures of emotional responding reflect dimensions rather than discrete states. In addition, experiential, physiological, and behavioural response systems are associated with unique sources of variance, which in turn limits the magnitude of convergence across measures. Accordingly, the authors suggest that there is no ``gold standard'' measure of emotional responding. Rather, experiential, physiological, and behavioural measures are all relevant to understanding emotion and cannot be assumed to be interchangeable.},
  pmcid = {PMC2756702},
  pmid = {19809584},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\QJX2VUZA\\Mauss und Robinson - 2009 - Measures of emotion A review.pdf}
}

@article{mavadatiDISFASpontaneousFacial2013,
  title = {{{DISFA}}: {{A Spontaneous Facial Action Intensity Database}}},
  shorttitle = {{{DISFA}}},
  author = {Mavadati, S. Mohammad and Mahoor, Mohammad H. and Bartlett, Kevin and Trinh, Philip and Cohn, Jeffrey F.},
  year = {2013},
  month = apr,
  journal = {IEEE Transactions on Affective Computing},
  volume = {4},
  number = {2},
  pages = {151--160},
  issn = {1949-3045},
  doi = {10.1109/T-AFFC.2013.4},
  abstract = {Access to well-labeled recordings of facial expression is critical to progress in automated facial expression recognition. With few exceptions, publicly available databases are limited to posed facial behavior that can differ markedly in conformation, intensity, and timing from what occurs spontaneously. To meet the need for publicly available corpora of well-labeled video, we collected, ground-truthed, and prepared for distribution the Denver intensity of spontaneous facial action database. Twenty-seven young adults were video recorded by a stereo camera while they viewed video clips intended to elicit spontaneous emotion expression. Each video frame was manually coded for presence, absence, and intensity of facial action units according to the facial action unit coding system. Action units are the smallest visibly discriminable changes in facial action; they may occur individually and in combinations to comprise more molar facial expressions. To provide a baseline for use in future research, protocols and benchmarks for automated action unit intensity measurement are reported. Details are given for accessing the database for research in computer vision, machine learning, and affective and behavioral science.},
  keywords = {action units,Databases,Encoding,Face,Face recognition,facial expression,FACS,Feature extraction,Gold,intensity,Pain,spontaneous facial behavior,video corpus},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\XKACU8XD\\Mavadati et al. - 2013 - DISFA A Spontaneous Facial Action Intensity Datab.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\99XECAUQ\\6475933.html}
}

@inproceedings{mcduffAffectivamitFacialExpression2013,
  title = {Affectiva-Mit Facial Expression Dataset (Am-Fed): {{Naturalistic}} and Spontaneous Facial Expressions Collected},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}}},
  author = {McDuff, Daniel and Kaliouby, Rana and Senechal, Thibaud and Amr, May and Cohn, Jeffrey and Picard, Rosalind},
  year = {2013},
  pages = {881--888},
  keywords = {Datasets},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\3X8YGDVH\\McDuff_Affectiva-MIT_Facial_Expression_2013_CV.pdf}
}

@article{mcneffGlobalPositioningSystem2002,
  title = {The Global Positioning System},
  author = {McNeff, J.G.},
  year = {2002},
  month = mar,
  journal = {IEEE Transactions on Microwave Theory and Techniques},
  volume = {50},
  number = {3},
  pages = {645--652},
  issn = {1557-9670},
  doi = {10.1109/22.989949},
  abstract = {The paper provides a top-level perspective on how the global positioning system works, how its services are used, and delves into the most important technical and geo-political factors affecting its long-term availability in an international setting.},
  keywords = {Atomic measurements,Availability,Extraterrestrial measurements,Global Positioning System,Humans,Position measurement,Satellite navigation systems,Synchronization,Time measurement,Timing},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\L595GVMB\\McNeff - 2002 - The global positioning system.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\LKI3EFQ9\\989949.html}
}

@article{mehrabianPleasurearousaldominanceGeneralFramework1996,
  title = {Pleasure-Arousal-Dominance: {{A}} General Framework for Describing and Measuring Individual Differences in Temperament},
  shorttitle = {Pleasure-Arousal-Dominance},
  author = {Mehrabian, Albert},
  year = {1996},
  journal = {Current Psychology},
  volume = {14},
  number = {4},
  pages = {261--292},
  publisher = {{Springer}},
  note = {Emotion Theory Concepts, for instance ekman and pleasure-arousal-dominance framework (PAD) or newer concepts like Plutchnik model},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\96WP8EUY\\Mehrabian - 1996 - Pleasure-arousal-dominance A general framework fo.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\7WVUG548\\BF02686918.html}
}

@article{melloukFacialEmotionRecognition2020,
  title = {Facial Emotion Recognition Using Deep Learning: Review and Insights},
  shorttitle = {Facial Emotion Recognition Using Deep Learning},
  author = {Mellouk, Wafa and Handouzi, Wahida},
  year = {2020},
  journal = {Procedia Computer Science},
  volume = {175},
  pages = {689--694},
  publisher = {{Elsevier}},
  keywords = {read},
  note = {Review on Machine Learning Models},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\YNK5GB5R\\Mellouk und Handouzi - 2020 - Facial emotion recognition using deep learning re.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\MDLVS3VK\\S1877050920318019.html}
}

@article{menneFacesEmotionInvestigating2018,
  title = {Faces of {{Emotion}}: {{Investigating Emotional Facial Expressions Towards}} a {{Robot}}},
  shorttitle = {Faces of {{Emotion}}},
  author = {Menne, Isabelle M. and Schwab, Frank},
  year = {2018},
  month = apr,
  journal = {International Journal of Social Robotics},
  volume = {10},
  number = {2},
  pages = {199--209},
  issn = {1875-4805},
  doi = {10.1007/s12369-017-0447-2},
  abstract = {Emotions have always been an intriguing topic in everyday life as well as in science. As robots are starting to move from industry halls to our private homes, emotions have become a vital theme for the field of human\textendash robot interaction. Since Darwin, research suggests facial expressions are associated with emotions. Facial expressions could provide an ideal tool for a natural, social human\textendash robot interaction. Despite a growing body of research on the implementation of emotions in robots (mostly based on facial expressions), systematic research on users' emotions and facial expressions towards robots remains largely neglected (cf. Arkin and Moshkina in Calvo R, D'Mello S, Gratch J, Kappas A (eds) The Oxford handbook of affective computing. Oxford University Press, New York, pp 483\textendash 493, 2015 on challenges in effective testing in affective human\textendash robot interaction). We experimentally investigated the multilevel phenomenon of emotions by using a multi-method approach. Since self-reports of emotions are prone to biases such as social desirability, we supplemented it by an objective behavioral measurement. By using the Facial Action Coding System we analyzed the facial expressions of 62 participants who watched the entertainment robot dinosaur Pleo either in a friendly interaction or being tortured. Participants differed in the type and frequency of Action Units displayed as well as in their self-reported feelings depending on the type of treatment they had watched (friendly or torture). In line with a previous study by Rosenthal-von der P\"utten et al. (Int J Soc Robot 5(1):17\textendash 34, 2013. https://doi.org/10.1007/s12369-012-0173-8), participants reported feeling more positive after the friendly video and more negative after the torture video. In the torture condition, participants furthermore showed a wide range of different Action Units primarily associated with negative emotions. For example, the Action Unit 4 (``Brow Lowerer'') that is common in negative emotions such as anger and sadness was displayed more frequently in the torture condition than in the friendly condition. The Action Unit 12 (``Lip Corner Puller'') however, an Action Unit commonly associated with joy, was present in both conditions and thus not necessarily predictive of positive emotions. The findings indicate the importance for a thorough investigation of the variables of emotional facial expressions. In investigating the Action Units participants display due to an emotional situation, we aim to provide information on spontaneous facial expressions towards a robot that could also serve as guidance for automatic approaches.},
  langid = {english},
  keywords = {Emotion,Emotional response,Empathy,Facial Action Coding System,Facial expression,Human–robot interaction},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\KCZKSXKX\\Menne und Schwab - 2018 - Faces of Emotion Investigating Emotional Facial E.pdf}
}

@inproceedings{menneFacingEmotionalReactions2016,
  title = {Facing {{Emotional Reactions Towards}} a {{Robot}} \textendash{} {{An Experimental Study Using FACS}}},
  booktitle = {Social {{Robotics}}},
  author = {Menne, Isabelle M. and Schnellbacher, Christin and Schwab, Frank},
  editor = {Agah, Arvin and Cabibihan, John-John and Howard, Ayanna M. and Salichs, Miguel A. and He, Hongsheng},
  year = {2016},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {372--381},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-47437-3_36},
  abstract = {As robots are starting to enter not only our professional lives but also our domestic lives, new questions arise: What about the emotional impact of getting into contact with this new `species'? Can robots elicit emotional reactions from humans? Assuming that humans may react in the same way towards robots as they do towards humans (Media Equation), it is important to investigate the factors influencing this emotional experience. But systematic research in this area remains scarce. An exception is the study conducted by Rosenthal-von der P\"utten, Kr\"amer, Hoffmann, Sobieraj, and Eimler in 2013 addressing the question of emotional reactions towards a robot experimentally. Taking the study by Rosenthal-von der P\"utten et al. as a starting point and following their suggested multi-method approach as well as Scherer's assumption about the five components of emotions, we added the motor expression component to further investigate the multilevel phenomenon of emotional reactions towards robots. We used the Facial Action Coding System (FACS) to analyze the facial expressions of participants viewing videos of the robot dinosaur Pleo in a friendly interaction or being tortured. Participants showed Action Units associated with intrinsic unpleasantness more frequently in the torture-video-condition than during the reception of the normal video. Participants also reported more negative feelings after the torture video than the normal video. The findings indicate the importance of investigating emotional reactions towards robots for a social robot to be an ideal companion. Furthermore, this paper shows that the application of FACS to research in human-robot interaction is a fruitful and insightful enhancement to commonly used self-report measures. We conclude this paper with some recommendations for improving the design of social robots.},
  isbn = {978-3-319-47437-3},
  langid = {english},
  keywords = {Design guidelines,Emotional response,Facial expression analysis,FACS},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\2XJQUDKT\\Menne et al. - 2016 - Facing Emotional Reactions Towards a Robot – An Ex.pdf}
}

@inproceedings{merloSubsymbolicTechniquesPerception1988,
  title = {Subsymbolic {{Techniques For Perception Systems Control}}},
  booktitle = {Twenty-{{Second Asilomar Conference}} on {{Signals}}, {{Systems}} and {{Computers}}},
  author = {Merlo, X. and Zavidovique, B.},
  year = {1988},
  month = oct,
  volume = {2},
  pages = {981--985},
  issn = {1058-6393},
  doi = {10.1109/ACSSC.1988.754695},
  abstract = {The framework is the perception control problem, ie how to control the sensors and the siggal processing units, in order to achieve a given perception task. We describe here a subsymbolic level, based on probabilistic techniques, but keeping in mind an integration with an artificial intelligence system. After formulating the problem, we derive an algorithm for such a control in simulation, and we show the results of the optimization of the perception task. Then we conclude by describing how to interface this system with a symbolic level.},
  keywords = {Artificial intelligence,Bayesian methods,Cameras,Control systems,Image edge detection,Image processing,Intelligent sensors,Robots,Sensor arrays,Signal processing algorithms},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\2SKP277Z\\Merlo und Zavidovique - 1988 - Subsymbolic Techniques For Perception Systems Cont.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\FFVFTMJV\\754695.html}
}

@inproceedings{miikkulainenIntegratedConnectionistModels1994,
  title = {Integrated Connectionist Models: Building {{AI}} Systems on Subsymbolic Foundations},
  shorttitle = {Integrated Connectionist Models},
  booktitle = {Proceedings {{Sixth International Conference}} on {{Tools}} with {{Artificial Intelligence}}. {{TAI}} 94},
  author = {Miikkulainen, R.},
  year = {1994},
  month = nov,
  pages = {231--232},
  doi = {10.1109/TAI.1994.346489},
  abstract = {Symbolic artificial intelligence is motivated by the hypothesis that symbol manipulation is both necessary and sufficient for intelligence. In symbolic systems, knowledge is encoded in terms of explicit symbolic structures, and inferences are based on handcrafted rules that sequentially manipulate these structures. Such systems have been quite successful, for example, in modeling in-depth natural language processing, episodic memory, and symbolic problem solving. However, much of the inferencing for everyday natural language understanding appears to take place immediately, without conscious control, apparently based on associations with past experience. This type of reasoning is difficult to model in the symbolic framework. In contrast, subsymbolic (distributed connectionist) networks represent knowledge in terms of correlations, coded in the weights of the network. For a given input, the network computes the most likely answer given its past experience. A number of human-like information processing properties such as learning from examples, context sensitivity, generalization, robustness of behavior, and intuitive reasoning emerge automatically in subsymbolic systems. The major motivation for subsymbolic AI, therefore, is to give a better account for cognitive phenomena that are statistical, or intuitive, in nature.{$<>$}},
  keywords = {Artificial intelligence,Buildings,Computer networks,Information processing,Large-scale systems,Natural language processing,Natural languages,Problem-solving,Process control,Robustness},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\HQMSJPCN\\Miikkulainen - 1994 - Integrated connectionist models building AI syste.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\JBTL5UH5\\346489.html}
}

@article{miyazawaSimpleEffectiveMultimodal2022,
  title = {Simple and {{Effective Multimodal Learning Based}} on {{Pre-trained Transformer Models}}},
  author = {Miyazawa, Kazuki and Kyuragi, Yuta and Nagai, Takayuki},
  year = {2022},
  journal = {IEEE Access},
  pages = {1--1},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2022.3159346},
  abstract = {Transformer-based models have been garnering attention in various fields. Beginning with their success in natural language processing, transformer-based models have succeeded in several other fields, such as image and automatic speech recognition. In addition to them being trained on unimodal information, many transformer-based models have been proposed for multimodal information. In the field of multimodal learning, a common problem encountered is that of insufficient multimodal training data. This study attempted to address this problem by proposing a simple albeit effective method using 1) unimodal pre-trained transformer models as encoders for each modal input, and 2) another set of transformer layers to fuse their output representations. Further, the proposed method was evaluated through a series of experiments on two common benchmarks: CMU-MOSI and MM-IMDb. Consequently, the proposed model obtained state-of-the-art performances in both benchmarks, and its robustness in reducing the amount of training data was demonstrated.},
  keywords = {Bit error rate,Data models,emotion recognition,Multimodal machine learning,pre-trained models,Predictive models,Task analysis,Training,Training data,transformers,Transformers},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\X5B2NCNZ\\Miyazawa et al. - 2022 - Simple and Effective Multimodal Learning Based on .pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\JEWMDVKQ\\keywords.html}
}

@article{mollahosseiniAffectnetDatabaseFacial2017,
  title = {Affectnet: {{A}} Database for Facial Expression, Valence, and Arousal Computing in the Wild},
  author = {Mollahosseini, Ali and Hasani, Behzad and Mahoor, Mohammad H.},
  year = {2017},
  journal = {IEEE Transactions on Affective Computing},
  volume = {10},
  number = {1},
  pages = {18--31},
  issn = {1949-3045},
  keywords = {Datasets},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\3SCLH8E4\\AffectNet_oneColumn-2.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\Z54T3C9F\\8013713.html}
}

@article{monkaLearningVisualModels2021,
  title = {Learning {{Visual Models}} Using a {{Knowledge Graph}} as a {{Trainer}}},
  author = {Monka, Sebastian and Halilaj, Lavdim and Schmid, Stefan and Rettinger, Achim},
  year = {2021},
  month = jul,
  journal = {arXiv:2102.08747 [cs]},
  eprint = {2102.08747},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Traditional computer vision approaches, based on neural networks (NN), are typically trained on a large amount of image data. By minimizing the cross-entropy loss between a prediction and a given class label, the NN and its visual embedding space are learned to fulfill a given task. However, due to the sole dependence on the image data distribution of the training domain, these models tend to fail when applied to a target domain that differs from their source domain. To learn a more robust NN to domain shifts, we propose the knowledge graph neural network (KG-NN), a neuro-symbolic approach that supervises the training using image-data-invariant auxiliary knowledge. The auxiliary knowledge is first encoded in a knowledge graph with respective concepts and their relationships, which is then transformed into a dense vector representation via an embedding method. Using a contrastive loss function, KG-NN learns to adapt its visual embedding space and thus its weights according to the image-data invariant knowledge graph embedding space. We evaluate KG-NN on visual transfer learning tasks for classification using the mini-ImageNet dataset and its derivatives, as well as road sign recognition datasets from Germany and China. The results show that a visual model trained with a knowledge graph as a trainer outperforms a model trained with cross-entropy in all experiments, in particular when the domain gap increases. Besides better performance and stronger robustness to domain shifts, these KG-NN adapts to multiple datasets and classes without suffering heavily from catastrophic forgetting.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  note = {Very Important
\par
Comment: ISWC 2021},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\WX225PIH\\Monka et al. - 2021 - Learning Visual Models using a Knowledge Graph as .pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\VBAHAS9F\\2102.html}
}

@article{montemerloFastSLAMFactoredSolution2002,
  title = {{{FastSLAM}}: {{A Factored Solution}} to the {{Simultaneous Localization}} and {{Mapping Problem}}},
  author = {Montemerlo, Michael and Thrun, Sebastian and Koller, Daphne and Wegbreit, Ben},
  year = {2002},
  pages = {6},
  abstract = {The ability to simultaneously localize a robot and accurately map its surroundings is considered by many to be a key prerequisite of truly autonomous robots. However, few approaches to this problem scale up to handle the very large number of landmarks present in real environments. Kalman filter-based algorithms, for example, require time quadratic in the number of landmarks to incorporate each sensor observation. This paper presents FastSLAM, an algorithm that recursively estimates the full posterior distribution over robot pose and landmark locations, yet scales logarithmically with the number of landmarks in the map. This algorithm is based on an exact factorization of the posterior into a product of conditional landmark distributions and a distribution over robot paths. The algorithm has been run successfully on as many as 50,000 landmarks, environments far beyond the reach of previous approaches. Experimental results demonstrate the advantages and limitations of the FastSLAM algorithm on both simulated and realworld data.},
  langid = {english},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\G37CHJKW\\Montemerlo et al. - FastSLAM A Factored Solution to the Simultaneous .pdf}
}

@inproceedings{mSpeechEmotionalRecognition2021,
  title = {Speech {{Emotional Recognition Using CNN Algorithm}}},
  booktitle = {2021 {{IEEE International Conference}} on {{Electronics}}, {{Computing}} and {{Communication Technologies}} ({{CONECCT}})},
  author = {M, Nivethaa and N, Pavithra and {Priyanka} and Sambandam, Palaniappan},
  year = {2021},
  month = jul,
  pages = {1--5},
  issn = {2766-2101},
  doi = {10.1109/CONECCT52877.2021.9622714},
  abstract = {Discourse Emotional Recognition is a method of analyzing the expressions of the human and determining the actual emotion of user through tone analysis and text analysis. Text analysis uses SVM algorithm and tone analysis uses CNN algorithm. Emotion expressed through words differs from emotion expressed through the speaker's tone. This paper focuses on determining the speaker's exact emotions by analyzing both the words and the tone of talk. At the point when the two practices are utilized together, compelling outcomes are acquired. This was intended to help the association in understanding the perspective of the customer on the item dependent on the feeling acknowledgment.},
  keywords = {Audio,CNN algorithm,Conferences,Emotion recognition,Machine Learning,Machine learning algorithms,Polarity,Speech recognition,Support vector machines,SVM algorithm,Text analysis,Text Analysis,Text recognition,Tone Analysis},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\GBVXSU3F\\M et al. - 2021 - Speech Emotional Recognition Using CNN Algorithm.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\53MEC95X\\9622714.html}
}

@misc{MyMobileStore,
  title = {{My Mobile Store von Wanzl \textendash{} einkaufen zu jeder Zeit! | Wanzl}},
  abstract = {Wanzl treibt die Digitalisierung des Handels voran mit Hightech-Ministore},
  howpublished = {https://www.wanzl.com/de\_DE/wanzl-inside/presse-und-news/my-mobile-store-von-wanzl-einkaufen-zu-jeder-zeit\textasciitilde n5415},
  langid = {german},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\9VQPQXDE\\my-mobile-store-von-wanzl-einkaufen-zu-jeder-zeit~n5415.html}
}

@inproceedings{najafiHybridEISmartlyFace2021,
  title = {{{HybridEI}}: {{Smartly Face Detection System}} in {{Resource Constrained Edge Environment}}},
  shorttitle = {{{HybridEI}}},
  booktitle = {2021 {{IEEE International Conference}} on {{Networking}}, {{Sensing}} and {{Control}} ({{ICNSC}})},
  author = {Najafi, Fridoon and Wang, Pengwei and Nyabuga, Douglas O.},
  year = {2021},
  month = dec,
  volume = {1},
  pages = {1--6},
  doi = {10.1109/ICNSC52481.2021.9702180},
  abstract = {With the digital revolution of the Internet of Things (IoT), data is rapidly emerging at the network's edge, where it is collected using artificial intelligence (AI) and processed in a variety of applications, including cybersecurity, image recognition, and human tracking. Despite this, facial expression recognition (FER) remains a challenging problem in computer vision. Inconsistencies in the sophistication of FER and variations among expression classes are often ignored in most facial expression detection systems due to the various expressions of emotion and unpredictable environmental influences. An HybridEI (Hybrid Edge Intelligence), a smartly face detection system trained in a more powerful cloud environment and inferenced at the resource-constrained edge environment, is proposed for FER in this study. Face classification complexity, memory minimization, and computational time were all well addressed by our proposed HybridEI approach. With an accuracy of 82.78\% and 1.27 frames per second (FPS), the experimental results on FER-2013 dataset demonstrated the effectiveness and superiority over other state-of-the-art (SOTA) methods. Furthermore, both the memory capacity and computational time were significantly decreased.},
  keywords = {Edge intelligence,Emotion recognition,face detection,Face detection,Face recognition,Image edge detection,Minimization,post-quantization,raspberry pi v4,Real-time systems,Sensors,weight pruning},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\SS9T5XVB\\Najafi et al. - 2021 - HybridEI Smartly Face Detection System in Resourc.pdf}
}

@inproceedings{nantyFuzzyControlledPAD2013,
  title = {Fuzzy {{Controlled PAD Emotional State}} of a {{NAO Robot}}},
  booktitle = {2013 {{Conference}} on {{Technologies}} and {{Applications}} of {{Artificial Intelligence}}},
  author = {Nanty, Alban and Gelin, Rodolphe},
  year = {2013},
  month = dec,
  pages = {90--96},
  issn = {2376-6824},
  doi = {10.1109/TAAI.2013.30},
  abstract = {Various Emotion Models (e.g. Circumplex Model [1], Vector Model [2], PANA (Positive Activation - Negative Activation) Model [3], PAD (Pleasure Arousal Dominance) Model [4], etc...) can be used to represent the different emotional states of a robot. In this paper we chose to use a PAD Emotional Space [5] to simulate the emotional state of a NAO Robot (NAO is the name of a humanoid robot developed and commercialized by Aldebaran Robotics). But one difficulty is to write an algorithm that makes evolve the emotional state according to external and internal stimuli. Simulating a coherent Emotional State over the time is a real challenge for designing a virtual personality. As human emotions are fuzzy by nature, many researchers explored using Fuzzy State Machine [6], [7] or Fuzzy Logic [8], [9], [10], [11], [12] to control the emotions of a virtual character or robot. Following the same approach, in this paper we propose to use a Fuzzy Control System [13] to control the emotional state of a NAO robot by injecting attractors in his PAD Emotional Space. Fuzzy Control Systems are usually good controller of uncertainty and easy to write thanks to their syntax close to the human language. The result of our experiments showed that the evolution of the Emotional State of our NAO robot was smooth and coherent with the situation met when interacting with humans. Moreover with different set of rules it becomes easy to switch the personality of the robot (meaning the robot can react differently to the same stimuli according to the different set of rules used).},
  keywords = {Aerospace electronics,emotion,emotion controller,Fuzzy control,Fuzzy Control System,Input variables,Merging,NAO,PAD,robot,Robots,Speech,Writing},
  note = {Idea to use Fuzzy Logic for decision-making, instead of multi-classification based on binary values},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\NDZPZNGQ\\Nanty und Gelin - 2013 - Fuzzy Controlled PAD Emotional State of a NAO Robo.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\QFCHDV3F\\6783849.html}
}

@inproceedings{neathiContextAwareAttention2022,
  title = {Context {{Aware Attention Learning For Person Re-Identification}} ({{A Data Driven Approach}})},
  booktitle = {2022 2nd {{International Conference}} on {{Artificial Intelligence}} and {{Signal Processing}} ({{AISP}})},
  author = {Neathi, R and Kavitha, D. and Chinnasamy, A. and Devi, A.Sangeerani and Devi, V. Kanchana and Kumar, D.Sathish},
  year = {2022},
  month = feb,
  pages = {1--5},
  issn = {2640-5768},
  doi = {10.1109/AISP53593.2022.9760563},
  abstract = {Deep learning approaches have started to conquer the research progress of video-based person re-identification (re-id). However, comprehensive manual efforts are required for existing approaches which is substantially consider supervised learning. The proposed context aware learning system that considers human body features and also considers the feature points extracted from the surrounding. It extracts the feature points and matches the person from videos captured across network of surveillance cameras. For example, People are identified with biometrics like face, iris, or gait. A high level of segmentation (such as at airports and subway stations) and/or poor camera quality make it impossible to get specific information in most video surveillance setups. For re-identification, a strong modeling of an individual's overall look (apparel) is necessary. This is a particularly difficult challenge because of the large appearance changes produced by differences in view angle, lighting circumstances, and the individual's varied postures. This approach is very much robust in pedestrian videos.},
  keywords = {Cameras,Clothing,context aware learning,Context-aware services,Deep learning,Feature extraction,Fully Convolution Network (FCN),re-identification,Supervised learning,Video surveillance},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\2MLIEZ3G\\Neathi et al. - 2022 - Context Aware Attention Learning For Person Re-Ide.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\45Z6THGE\\9760563.html}
}

@article{ngoOAKOntologyBasedKnowledge2020,
  title = {{{OAK}}: {{Ontology-Based Knowledge Map Model}} for {{Digital Agriculture}}},
  shorttitle = {{{OAK}}},
  author = {Ngo, Quoc Hung and Kechadi, Tahar and {Le-Khac}, Nhien-An},
  year = {2020},
  journal = {arXiv:2011.11442 [cs]},
  volume = {12466},
  eprint = {2011.11442},
  eprinttype = {arxiv},
  primaryclass = {cs},
  pages = {245--259},
  doi = {10.1007/978-3-030-63924-2_14},
  abstract = {Nowadays, a huge amount of knowledge has been amassed in digital agriculture. This knowledge and know-how information are collected from various sources, hence the question is how to organise this knowledge so that it can be efficiently exploited. Although this knowledge about agriculture practices can be represented using ontology, rule-based expert systems, or knowledge model built from data mining processes, the scalability still remains an open issue. In this study, we propose a knowledge representation model, called an ontology-based knowledge map, which can collect knowledge from different sources, store it, and exploit either directly by stakeholders or as an input to the knowledge discovery process (Data Mining). The proposed model consists of two stages, 1) build an ontology as a knowledge base for a specific domain and data mining concepts, and 2) build the ontology-based knowledge map model for representing and storing the knowledge mined on the crop datasets. A framework of the proposed model has been implemented in agriculture domain. It is an efficient and scalable model, and it can be used as knowledge repository a digital agriculture.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Databases},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\C9JAFRR5\\Ngo et al. - 2020 - OAK Ontology-Based Knowledge Map Model for Digita.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\IKJYNJGV\\2011.html}
}

@article{ngoSemanticSearchLarge2022,
  title = {Semantic {{Search}} for {{Large Scale Clinical Ontologies}}},
  author = {Ngo, Duy-Hoa and Kemp, Madonna and Truran, Donna and Koopman, Bevan and {Metke-Jimenez}, Alejandro},
  year = {2022},
  month = jan,
  journal = {arXiv:2201.00118 [cs]},
  eprint = {2201.00118},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Finding concepts in large clinical ontologies can be challenging when queries use different vocabularies. A search algorithm that overcomes this problem is useful in applications such as concept normalisation and ontology matching, where concepts can be referred to in different ways, using different synonyms. In this paper, we present a deep learning based approach to build a semantic search system for large clinical ontologies. We propose a Triplet-BERT model and a method that generates training data directly from the ontologies. The model is evaluated using five real benchmark data sets and the results show that our approach achieves high results on both free text to concept and concept to concept searching tasks, and outperforms all baseline methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\8M9HB62A\\Ngo et al. - 2022 - Semantic Search for Large Scale Clinical Ontologie.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\58JILY8Z\\2201.html}
}

@article{northcuttPervasiveLabelErrors2021,
  title = {Pervasive {{Label Errors}} in {{Test Sets Destabilize Machine Learning Benchmarks}}},
  author = {Northcutt, Curtis G. and Athalye, Anish and Mueller, Jonas},
  year = {2021},
  month = nov,
  journal = {arXiv:2103.14749 [cs, stat]},
  eprint = {2103.14749},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We identify label errors in the test sets of 10 of the most commonly-used computer vision, natural language, and audio datasets, and subsequently study the potential for these label errors to affect benchmark results. Errors in test sets are numerous and widespread: we estimate an average of at least 3.3\% errors across the 10 datasets, where for example label errors comprise at least 6\% of the ImageNet validation set. Putative label errors are identified using confident learning algorithms and then human-validated via crowdsourcing (51\% of the algorithmically-flagged candidates are indeed erroneously labeled, on average across the datasets). Traditionally, machine learning practitioners choose which model to deploy based on test accuracy - our findings advise caution here, proposing that judging models over correctly labeled test sets may be more useful, especially for noisy real-world datasets. Surprisingly, we find that lower capacity models may be practically more useful than higher capacity models in real-world datasets with high proportions of erroneously labeled data. For example, on ImageNet with corrected labels: ResNet-18 outperforms ResNet-50 if the prevalence of originally mislabeled test examples increases by just 6\%. On CIFAR-10 with corrected labels: VGG-11 outperforms VGG-19 if the prevalence of originally mislabeled test examples increases by just 5\%. Test set errors across the 10 datasets can be viewed at https://labelerrors.com and all label errors can be reproduced by https://github.com/cleanlab/label-errors.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: Demo available at https://labelerrors.com/ and source code available at https://github.com/cleanlab/label-errors},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\NUZ4DYZW\\Northcutt et al. - 2021 - Pervasive Label Errors in Test Sets Destabilize Ma.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\CJXVQUDQ\\2103.html}
}

@misc{NowYouCan,
  title = {Now {{You Can Order Domino}}'s\textregistered{} {{Pizza}} \ldots{} {{With Your Mind}} | {{Domino}}'s {{Pizza}}},
  howpublished = {https://ir.dominos.com/news-releases/news-release-details/now-you-can-order-dominosr-pizza-your-mind},
  langid = {english},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\247J3HNZ\\now-you-can-order-dominosr-pizza-your-mind.html}
}

@misc{NVIDIADataScience2021,
  title = {{{NVIDIA Data Science Stack}}},
  year = {2021},
  month = dec,
  abstract = {NVIDIA Data Science stack tools},
  copyright = {Apache-2.0},
  howpublished = {NVIDIA Corporation}
}

@inproceedings{okaHybridCognitiveModel1991,
  title = {Hybrid Cognitive Model of Conscious Level Processing and Unconscious Level Processing},
  booktitle = {[{{Proceedings}}] 1991 {{IEEE International Joint Conference}} on {{Neural Networks}}},
  author = {Oka, N.},
  year = {1991},
  month = nov,
  pages = {485-490 vol.1},
  doi = {10.1109/IJCNN.1991.170448},
  abstract = {Human intelligence has been modeled in two ways: modeling based on central symbolic processing, and modeling based on distributed subsymbolic processing. This paper points out the limitations of these two approaches, and proposes a hybrid cognitive model of central symbolic processing on the conscious level, and distributed subsymbolic processing on the unconscious level (C/U model). The advantages of the C/U model are clarified by explaining various functions realized by the model: multistage knowledge retrieval, recognition and inference with situated knowledge, inductive learning, and creative inference. Those functions are realized by utilizing the close interaction between the two levels. Finally, this paper describes an implementation method that utilizes the characteristics of a parallel logic programming language, and also describes a knowledge acquisition system necessary for building practical hybrid systems.{$<>$}},
  keywords = {Artificial intelligence,Buildings,Distributed control,Humans,Intelligent networks,Intelligent structures,Knowledge acquisition,Law,Legal factors,Logic programming},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\RSGUL3V7\\Oka - 1991 - Hybrid cognitive model of conscious level processi.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\L72F7GZP\\170448.html}
}

@article{oltramariNeurosymbolicArchitecturesContext2020,
  title = {Neuro-Symbolic {{Architectures}} for {{Context Understanding}}},
  author = {Oltramari, Alessandro and Francis, Jonathan and Henson, Cory and Ma, Kaixin and Wickramarachchi, Ruwan},
  year = {2020},
  month = mar,
  journal = {arXiv:2003.04707 [cs]},
  eprint = {2003.04707},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Computational context understanding refers to an agent's ability to fuse disparate sources of information for decision-making and is, therefore, generally regarded as a prerequisite for sophisticated machine reasoning capabilities, such as in artificial intelligence (AI). Data-driven and knowledge-driven methods are two classical techniques in the pursuit of such machine sense-making capability. However, while data-driven methods seek to model the statistical regularities of events by making observations in the real-world, they remain difficult to interpret and they lack mechanisms for naturally incorporating external knowledge. Conversely, knowledge-driven methods, combine structured knowledge bases, perform symbolic reasoning based on axiomatic principles, and are more interpretable in their inferential processing; however, they often lack the ability to estimate the statistical salience of an inference. To combat these issues, we propose the use of hybrid AI methodology as a general framework for combining the strengths of both approaches. Specifically, we inherit the concept of neuro-symbolism as a way of using knowledge-bases to guide the learning progress of deep neural networks. We further ground our discussion in two applications of neuro-symbolism and, in both cases, show that our systems maintain interpretability while achieving comparable performance, relative to the state-of-the-art.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Symbolic Computation},
  note = {Comment: In: Ilaria Tiddi, Freddy Lecue, Pascal Hitzler (eds.), Knowledge Graphs for eXplainable AI -- Foundations, Applications and Challenges. Studies on the Semantic Web, IOS Press, Amsterdam, 2020. arXiv admin note: text overlap with arXiv:1910.14087},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\Q6LCWYSE\\Oltramari et al. - 2020 - Neuro-symbolic Architectures for Context Understan.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\ZSAKKHZ5\\2003.html}
}

@misc{organizationWorldIntellectualProperty,
  title = {World {{Intellectual Property Indicators}} 2021},
  author = {Organization, World Intellectual Property},
  doi = {10.34667/tind.44461},
  abstract = {This authoritative report analyzes IP activity around the globe. Drawing on 2020 filing, registration and renewals statistics from national and regional IP offices and WIPO, it covers patents, utility models, trademarks, industrial designs, microorganisms, plant variety protection and geographical indications. The report also draws on survey data and industry sources to give a picture of activity in the publishing industry.},
  howpublished = {https://www.wipo.int/edocs/pubdocs/en/wipo\_pub\_941\_2021.pdf},
  isbn = {9789280533293},
  langid = {english},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\45ED4GAH\\details.html}
}

@inproceedings{ousmaneAutomaticRecognitionSystem2019,
  title = {Automatic Recognition System of Emotions Expressed through the Face Using Machine Learning: {{Application}} to Police Interrogation Simulation},
  shorttitle = {Automatic Recognition System of Emotions Expressed through the Face Using Machine Learning},
  booktitle = {2019 3rd {{International Conference}} on {{Bio-engineering}} for {{Smart Technologies}} ({{BioSMART}})},
  author = {Ousmane, Abdoul Matine and Djara, Tahirou and Zoumarou W., Faizath J{\'e}dida and Vianou, Antoine},
  year = {2019},
  month = apr,
  pages = {1--4},
  doi = {10.1109/BIOSMART.2019.8734245},
  abstract = {During our work, we developed Automatic recognition system of emotions expressed through the face. This system will be adapted to judicial police interrogation or interviews simulation. This system is based on a functional division: detection and follow-up the face, acquisition of facial expressions starting from video images sequences, and finally, characteristics extraction and emotions expressed recognition. The deep convolution network model is formed primarily on FER2013. It is used to train and validate the model. We use OpenCV library and its implementation of the Viola and Jones algorithm for face detection.},
  keywords = {Convolution,Convolutional neural networks,data sets,deep convolutional networks,emotion recognition,Emotion recognition,Face,Face recognition,facial expressions,Law enforcement,Training},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\AADDKHUB\\Ousmane et al. - 2019 - Automatic recognition system of emotions expressed.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\BWU4PW6V\\Ousmane et al. - 2019 - Automatic recognition system of emotions expressed.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\EQKSUE4F\\8734245.html;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\H4H5ZEG4\\8734245.html}
}

@article{ozakiLearningDescriptionLogic2020,
  title = {Learning {{Description Logic Ontologies}}. {{Five Approaches}}. {{Where Do They Stand}}?},
  author = {Ozaki, Ana},
  year = {2020},
  month = sep,
  journal = {KI - K\"unstliche Intelligenz},
  volume = {34},
  number = {3},
  eprint = {2104.01193},
  eprinttype = {arxiv},
  pages = {317--327},
  issn = {0933-1875, 1610-1987},
  doi = {10.1007/s13218-020-00656-9},
  abstract = {The quest for acquiring a formal representation of the knowledge of a domain of interest has attracted researchers with various backgrounds into a diverse field called ontology learning. We highlight classical machine learning and data mining approaches that have been proposed for (semi-)automating the creation of description logic (DL) ontologies. These are based on association rule mining, formal concept analysis, inductive logic programming, computational learning theory, and neural networks. We provide an overview of each approach and how it has been adapted for dealing with DL ontologies. Finally, we discuss the benefits and limitations of each of them for learning DL ontologies.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Logic in Computer Science,Computer Science - Machine Learning},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\WGTLAF82\\Ozaki - 2020 - Learning Description Logic Ontologies. Five Approa.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\G4K78IR7\\2104.html}
}

@inproceedings{ozdemirRealTimeEmotion2019,
  title = {Real Time Emotion Recognition from Facial Expressions Using {{CNN}} Architecture},
  booktitle = {2019 Medical Technologies Congress (Tiptekno)},
  author = {Ozdemir, Mehmet Akif and Elagoz, Berkay and Alaybeyoglu, Aysegul and Sadighzadeh, Reza and Akan, Aydin},
  year = {2019},
  pages = {1--4},
  publisher = {{IEEE}},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\3WSHU35J\\Ozdemir et al. - 2019 - Real time emotion recognition from facial expressi.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\F564ZFIE\\8895215.html}
}

@article{ozsoyIndoorPositioningBased2013,
  title = {Indoor Positioning Based on Global Positioning System Signals},
  author = {Ozsoy, Kerem and Bozkurt, Ayhan and Tekin, Ibrahim},
  year = {2013},
  journal = {Microwave and Optical Technology Letters},
  volume = {55},
  number = {5},
  pages = {1091--1097},
  issn = {1098-2760},
  doi = {10.1002/mop.27520},
  abstract = {The Global Positioning System (GPS) is highly reliable and accurate when used outdoors.However, in indoor environments, due to the additional signal loss incurred by the walls of the buildings, the detection and decoding of GPS signals becomes a difficult task. As a solution to the indoor area coverage problem, an indoor positioning system based on GPS repeaters and a modified positioning algorithm is proposed, designed, and tested. A prototype indoor positioning system for 1D/2D positioning is built using directional GPS antennas and low-noise amplifiers (LNA). The modified positioning algorithm is used for the real time processing of captured live GPS data. All the system components are integrated and positioning is obtained for the evaluation of the system performance. Results of the experiments show that the proposed system can be used for indoor positioning in locations where there is no GPS signal reception. The proposed system facilitates the continuation of GPS services indoors with hardware additions to the buildings and only a software update to a standard GPS receiver. \textcopyright{} 2013 Wiley Periodicals, Inc. Microwave Opt Technol Lett 55:1091\textendash 1097, 2013; View this article online at wileyonlinelibrary.com. DOI 10.1002/mop.27520},
  langid = {english},
  keywords = {GPS directional antenna,GPS repeater,GPS RF low-noise amplifier,indoor positioning},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/mop.27520},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\ARMAWFMY\\mop.html}
}

@inproceedings{p.luceyExtendedCohnKanadeDataset2010,
  title = {The {{Extended Cohn-Kanade Dataset}} ({{CK}}+): {{A}} Complete Dataset for Action Unit and Emotion-Specified Expression},
  booktitle = {2010 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}} - {{Workshops}}},
  author = {P. Lucey and J. F. Cohn and T. Kanade and J. Saragih and Z. Ambadar and I. Matthews},
  year = {2010},
  month = jun,
  pages = {94--101},
  doi = {10.1109/CVPRW.2010.5543262},
  abstract = {In 2000, the Cohn-Kanade (CK) database was released for the purpose of promoting research into automatically detecting individual facial expressions. Since then, the CK database has become one of the most widely used test-beds for algorithm development and evaluation. During this period, three limitations have become apparent: 1) While AU codes are well validated, emotion labels are not, as they refer to what was requested rather than what was actually performed, 2) The lack of a common performance metric against which to evaluate new algorithms, and 3) Standard protocols for common databases have not emerged. As a consequence, the CK database has been used for both AU and emotion detection (even though labels for the latter have not been validated), comparison with benchmark algorithms is missing, and use of random subsets of the original database makes meta-analyses difficult. To address these and other concerns, we present the Extended Cohn-Kanade (CK+) database. The number of sequences is increased by 22\% and the number of subjects by 27\%. The target expression for each sequence is fully FACS coded and emotion labels have been revised and validated. In addition to this, non-posed sequences for several types of smiles and their associated metadata have been added. We present baseline results using Active Appearance Models (AAMs) and a linear support vector machine (SVM) classifier using a leave-one-out subject cross-validation for both AU and emotion detection for the posed data. The emotion and AU labels, along with the extended image data and tracked landmarks will be made available July 2010.},
  isbn = {2160-7516},
  keywords = {Datasets},
  note = {The following values have no corresponding Zotero field:\\
alt-title: 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\95E56DPY\\The_Extended_Cohn-Kanade_Dataset_CK_A_complete.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\55UICJHD\\5543262.html;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\PI7ED9HW\\5543262.html}
}

@article{palffyMultiClassRoadUser2022,
  title = {Multi-{{Class Road User Detection With}} 3+{{1D Radar}} in the {{View-of-Delft Dataset}}},
  author = {Palffy, Andras and Pool, Ewoud and Baratam, Srimannarayana and Kooij, Julian F. P. and Gavrila, Dariu M.},
  year = {2022},
  month = apr,
  journal = {IEEE Robotics and Automation Letters},
  volume = {7},
  number = {2},
  pages = {4961--4968},
  issn = {2377-3766},
  doi = {10.1109/LRA.2022.3147324},
  abstract = {Next-generation automotive radars provide elevation data in addition to range-, azimuth- and Doppler velocity. In this experimental study, we apply a state-of-the-art object detector (PointPillars), previously used for LiDAR 3D data, to such 3+1D radar data (where 1D refers to Doppler). In ablation studies, we first explore the benefits of the additional elevation information, together with that of Doppler, radar cross section and temporal accumulation, in the context of multi-class road user detection. We subsequently compare object detection performance on the radar and LiDAR point clouds, object class-wise and as a function of distance. To facilitate our experimental study, we present the novel View-of-Delft (VoD) automotive dataset. It contains 8693 frames of synchronized and calibrated 64-layer LiDAR-, (stereo) camera-, and 3+1D radar-data acquired in complex, urban traffic. It consists of 123106 3D bounding box annotations of both moving and static objects, including 26587 pedestrian, 10800 cyclist and 26949 car labels. Our results show that object detection on 64-layer LiDAR data still outperforms that on 3+1D radar data, but the addition of elevation information and integration of successive radar scans helps close the gap. The VoD dataset is made freely available for scientific benchmarking at {$<$}uri{$>$}https://intelligent-vehicles.org/datasets/view-of-delft/{$<$}/uri{$>$}.},
  keywords = {Annotations,automotive radars,data sets for robotic vision,Doppler effect,Doppler radar,Laser radar,Object detection,Radar,Radar detection,segmentation and catego- rization,Three-dimensional displays},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\SJUZJBSV\\9699098.html}
}

@article{pankseppBasicsBasicEmotion1994,
  title = {The Basics of Basic Emotion},
  author = {Panksepp, Jaak},
  year = {1994},
  journal = {The nature of emotion: Fundamental questions},
  pages = {20--24},
  publisher = {{Oxford University Press New York}}
}

@article{panticAutomaticAnalysisFacial2000,
  title = {Automatic Analysis of Facial Expressions: {{The}} State of the Art},
  shorttitle = {Automatic Analysis of Facial Expressions},
  author = {Pantic, Maja and Rothkrantz, Leon J. M.},
  year = {2000},
  journal = {IEEE Transactions on pattern analysis and machine intelligence},
  volume = {22},
  number = {12},
  pages = {1424--1445},
  publisher = {{IEEE}},
  keywords = {read},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\FH4U2HK5\\Pantic und Rothkrantz - 2000 - Automatic analysis of facial expressions The stat.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\VYKUZKVT\\895976.html}
}

@article{panticDynamicsFacialExpression2006,
  title = {Dynamics of Facial Expression: Recognition of Facial Actions and Their Temporal Segments from Face Profile Image Sequences},
  shorttitle = {Dynamics of Facial Expression},
  author = {Pantic, M. and Patras, I.},
  year = {2006},
  month = apr,
  journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},
  volume = {36},
  number = {2},
  pages = {433--449},
  issn = {1941-0492},
  doi = {10.1109/TSMCB.2005.859075},
  abstract = {Automatic analysis of human facial expression is a challenging problem with many applications. Most of the existing automated systems for facial expression analysis attempt to recognize a few prototypic emotional expressions, such as anger and happiness. Instead of representing another approach to machine analysis of prototypic facial expressions of emotion, the method presented in this paper attempts to handle a large range of human facial behavior by recognizing facial muscle actions that produce expressions. Virtually all of the existing vision systems for facial muscle action detection deal only with frontal-view face images and cannot handle temporal dynamics of facial actions. In this paper, we present a system for automatic recognition of facial action units (AUs) and their temporal models from long, profile-view face image sequences. We exploit particle filtering to track 15 facial points in an input face-profile sequence, and we introduce facial-action-dynamics recognition from continuous video input using temporal rules. The algorithm performs both automatic segmentation of an input video into facial expressions pictured and recognition of temporal segments (i.e., onset, apex, offset) of 27 AUs occurring alone or in a combination in the input face-profile video. A recognition rate of 87\% is achieved.},
  keywords = {Computer vision,Emotion recognition,Face detection,Face recognition,facial action units,facial expression analysis,facial expression dynamics analysis,Facial muscles,Humans,Image recognition,Image segmentation,Image sequences,Machine vision,particle filtering,Prototypes,rule-based reasoning,spatial reasoning,temporal reasoning},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\ZFIBHVXK\\Pantic und Patras - 2006 - Dynamics of facial expression recognition of faci.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\HMMAVBKD\\1605389.html}
}

@inproceedings{paredisLearningCrossroadsBiology1995,
  title = {Learning at the Crossroads of Biology and Computation},
  booktitle = {Proceedings {{First International Symposium}} on {{Intelligence}} in {{Neural}} and {{Biological Systems}}. {{INBS}}'95},
  author = {Paredis, J.},
  year = {1995},
  month = may,
  pages = {56--63},
  doi = {10.1109/INBS.1995.404279},
  abstract = {Discusses various avenues for exploiting biological learning mechanisms within machine learning. Special attention is given to the following issues: (a) the reasons for the wide variety of biological learning mechanisms; (b) the relation between lifetime and genetic learning; (c) a description of the driving forces of genetic learning and their use in evolutionary computation. Various symbolic machine learning and reasoning techniques can be used to complement (genetic and/or neural) sub-symbolic learning. A first approach uses symbolic induction for explaining the behavior of (genetically evolved) neural nets. Next, a general framework for the use of (symbolic) domain knowledge during genetic learning is introduced.{$<>$}},
  keywords = {Animals,Biology computing,Birds,Evolution (biology),Evolutionary computation,Genetics,Learning systems,Machine learning,Machine learning algorithms,Neural networks},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\D3PSFCDT\\Paredis - 1995 - Learning at the crossroads of biology and computat.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\GFUGRQNS\\404279.html}
}

@article{parkAnalysisEmotionAuthenticity2021,
  title = {The {{Analysis}} of {{Emotion Authenticity Based}} on {{Facial Micromovements}}},
  author = {Park, Sung and Lee, Seong Won and Whang, Mincheol},
  year = {2021},
  month = jul,
  journal = {Sensors (Basel, Switzerland)},
  volume = {21},
  number = {13},
  pages = {4616},
  issn = {1424-8220},
  doi = {10.3390/s21134616},
  abstract = {People tend to display fake expressions to conceal their true feelings. False expressions are observable by facial micromovements that occur for less than a second. Systems designed to recognize facial expressions (e.g., social robots, recognition systems for the blind, monitoring systems for drivers) may better understand the user's intent by identifying the authenticity of the expression. The present study investigated the characteristics of real and fake facial expressions of representative emotions (happiness, contentment, anger, and sadness) in a two-dimensional emotion model. Participants viewed a series of visual stimuli designed to induce real or fake emotions and were signaled to produce a facial expression at a set time. From the participant's expression data, feature variables (i.e., the degree and variance of movement, and vibration level) involving the facial micromovements at the onset of the expression were analyzed. The results indicated significant differences in the feature variables between the real and fake expression conditions. The differences varied according to facial regions as a function of emotions. This study provides appraisal criteria for identifying the authenticity of facial expressions that are applicable to future research and the design of emotion recognition systems.},
  pmcid = {PMC8271774},
  pmid = {34283146},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\R8AYICM5\\Park et al. - 2021 - The Analysis of Emotion Authenticity Based on Faci.pdf}
}

@article{parkHowVisionTransformers2022,
  title = {How {{Do Vision Transformers Work}}?},
  author = {Park, Namuk and Kim, Songkuk},
  year = {2022},
  month = feb,
  journal = {arXiv:2202.06709 [cs]},
  eprint = {2202.06709},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The success of multi-head self-attentions (MSAs) for computer vision is now indisputable. However, little is known about how MSAs work. We present fundamental explanations to help better understand the nature of MSAs. In particular, we demonstrate the following properties of MSAs and Vision Transformers (ViTs): (1) MSAs improve not only accuracy but also generalization by flattening the loss landscapes. Such improvement is primarily attributable to their data specificity, not long-range dependency. On the other hand, ViTs suffer from non-convex losses. Large datasets and loss landscape smoothing methods alleviate this problem; (2) MSAs and Convs exhibit opposite behaviors. For example, MSAs are low-pass filters, but Convs are high-pass filters. Therefore, MSAs and Convs are complementary; (3) Multi-stage neural networks behave like a series connection of small individual models. In addition, MSAs at the end of a stage play a key role in prediction. Based on these insights, we propose AlterNet, a model in which Conv blocks at the end of a stage are replaced with MSA blocks. AlterNet outperforms CNNs not only in large data regimes but also in small data regimes. The code is available at https://github.com/xxxnell/how-do-vits-work.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  note = {Comment: ICLR 2022 (Spotlight)},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\2DTAB4AJ\\Park und Kim - 2022 - How Do Vision Transformers Work.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\9I7GZZGU\\2202.html}
}

@inproceedings{parkLiteratureRepresentationUsing2022,
  title = {Literature {{Representation}} Using {{Character Networks}} Based on {{Sentiment Analysis}}},
  booktitle = {2022 {{IEEE International Conference}} on {{Big Data}} and {{Smart Computing}} ({{BigComp}})},
  author = {Park, Myeonggeon and Park, Sunghong and Shin, Hyunjung},
  year = {2022},
  month = jan,
  pages = {190--193},
  issn = {2375-9356},
  doi = {10.1109/BigComp54360.2022.00044},
  abstract = {In this study, we propose a method for expressing literary works using machine learning. The proposed method expresses novels by composing a network of characters appearing in the novel and identifying the flow of sentiment scores between characters who agree or oppose the main character. Characters, represented as nodes on the constructed network, are extracted via the name entity recognizer, whereas edges are constructed based on the emotional words described in the novel. Protagonist and antagonist groups on the character network are classified via signed graph clustering. The novel proceeds through the interaction between the characters, and the groups are segmented to emphasize this through character grouping and network construction. A novel is classified into four acts, and the emotions of each group are emphasized and expressed in each act. In this study, 20 novels are clustered using the proposed method; subsequently, they are compared and tested using other expression methods.},
  keywords = {Big Data,Character Networks,Computational modeling,Conferences,Emotion recognition,Machine learning,Name Entity Recognizer,Novel Representation,Sentiment analysis,Text Embedding,Text Mining,Text recognition,Text Representation},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\UVAN8F2E\\Park et al. - 2022 - Literature Representation using Character Networks.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\I9N7TI3W\\9736547.html}
}

@book{parrottEmotionsSocialPsychology2001,
  title = {Emotions in Social Psychology: {{Essential}} Readings},
  shorttitle = {Emotions in Social Psychology},
  author = {Parrott, W. Gerrod},
  year = {2001},
  publisher = {{psychology press}},
  note = {Linked to Emotion Detection from Text, Parrot covers emotion theories, for instance, basic emotions and neglected emotions},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\9ICBS84B\\books.html}
}

@article{patelFacialSentimentAnalysis2020a,
  title = {Facial {{Sentiment Analysis Using AI Techniques}}: {{State-of-the-Art}}, {{Taxonomies}}, and {{Challenges}}},
  shorttitle = {Facial {{Sentiment Analysis Using AI Techniques}}},
  author = {Patel, Keyur and Mehta, Dev and Mistry, Chinmay and Gupta, Rajesh and Tanwar, Sudeep and Kumar, Neeraj and Alazab, Mamoun},
  year = {2020},
  journal = {IEEE Access},
  volume = {8},
  pages = {90495--90519},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.2993803},
  abstract = {With the advancements in machine and deep learning algorithms, the envision of various critical real-life applications in computer vision becomes possible. One of the applications is facial sentiment analysis. Deep learning has made facial expression recognition the most trending research fields in computer vision area. Recently, deep learning-based FER models have suffered from various technological issues like under-fitting or over-fitting. It is due to either insufficient training and expression data. Motivated from the above facts, this paper presents a systematic and comprehensive survey on current state-of-art Artificial Intelligence techniques (datasets and algorithms) that provide a solution to the aforementioned issues. It also presents a taxonomy of existing facial sentiment analysis strategies in brief. Then, this paper reviews the existing novel machine and deep learning networks proposed by researchers that are specifically designed for facial expression recognition based on static images and present their merits and demerits and summarized their approach. Finally, this paper also presents the open issues and research challenges for the design of a robust facial expression recognition system.},
  keywords = {artificial intelligence,convolutional neural network,deep belief network,deep learning,Face,Face recognition,Facial sentiment analysis,Feature extraction,machine learning,Machine learning,Sentiment analysis,Taxonomy},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\T8RV5R8D\\Patel et al. - 2020 - Facial Sentiment Analysis Using AI Techniques Sta.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\CH3ZH5HW\\9091188.html}
}

@misc{PatentanmeldungenBeimEuropaischen,
  title = {{Patentanmeldungen beim Europ\"aischen Patentamt nach L\"andern 2020}},
  journal = {Statista},
  abstract = {Die Statistik zeigt die Anzahl der Patentanmeldungen* beim Europ\"aischen Patentamt nach Ursprungsl\"andern im Jahr 2020.},
  howpublished = {https://de-statista-com.ezproxy.hs-neu-ulm.de/statistik/daten/studie/684123/umfrage/anzahl-der-patentanmeldungen-beim-europaeischen-patentamt-nach-laendern/},
  langid = {german},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\4NBUH7DX\\anzahl-der-patentanmeldungen-beim-europaeischen-patentamt-nach-laendern.html}
}

@inproceedings{patilEmotionLinkedAIoT2021,
  title = {Emotion {{Linked AIoT Based Cognitive Home Automation System}} with {{Sensovisual Method}}},
  booktitle = {2021 {{IEEE Pune Section International Conference}} ({{PuneCon}})},
  author = {Patil, Varsha K and Hadawale, Omkar and Pawar, Vijaya R and Gijre, Mayank},
  year = {2021},
  month = dec,
  pages = {1--7},
  doi = {10.1109/PuneCon52575.2021.9686498},
  abstract = {This paper focuses on research done for implementation of emotion linked Artificial Intelligence of Things based smart Home Automation System. Previously, Home automation research scope was limited to either remote controlled systems or voice operated commands-based systems or virtual assistant-based systems. The problem with these systems was they were not self-adapting systems nor emotion-based mood enhancing systems. Our Proposed system combines real-world data for emotion awareness, sensing, learning, and decision-making. The proposed system is milestone in Smart home journey because, it is enhancing the systems security, efficiency and user experience. Thus, we present a cognitive intelligent home automation system featuring computer vision, environmental sensing, data processing, learning, and adaptation capabilities. Face recognition-based door access control used in the proposed system allows access only to authorized user. Further, Convolutional Neural Network and support vector machine algorithm-based emotion detection is implemented. The system recognizes emotions on the user's face and enhances user experience by controlling the ambient light as per the mood/emotion of the user. To implement system, we have used the methodology with different modes of operations such as privileged mode, non-privileged mode, intelligent mode, manual mode. We are proposing the ``sensovisual'' term in this paper which is combination of the sensor and visual method for smart home automation.},
  keywords = {Artificial Intelligence of Things,Automation,cognitive intelligence,computer vision,Computer vision,Convolutional Neural Network (CNN),Door access,Emotion detection,Emotion recognition,face recognition,Face recognition,Home Automation system,Internet of Things,non-privileged mode,PIR sensor,privileged mode,Raspberry pi,self-adapting system,sensovisual,Smart homes,SVM,Visual systems,Visualization},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\JZCSIBD6\\Patil et al. - 2021 - Emotion Linked AIoT Based Cognitive Home Automatio.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\NL6XI3WN\\9686498.html}
}

@inproceedings{pengMacroMicroExpression2018,
  title = {From {{Macro}} to {{Micro Expression Recognition}}: {{Deep Learning}} on {{Small Datasets Using Transfer Learning}}},
  shorttitle = {From {{Macro}} to {{Micro Expression Recognition}}},
  booktitle = {2018 13th {{IEEE International Conference}} on {{Automatic Face Gesture Recognition}} ({{FG}} 2018)},
  author = {Peng, Min and Wu, Zhan and Zhang, Zhihao and Chen, Tong},
  year = {2018},
  month = may,
  pages = {657--661},
  doi = {10.1109/FG.2018.00103},
  abstract = {This paper presents the methods used in our submission to 2018 Facial Micro-Expression Grand Challenge (MEGC). The object of the challenge is to recognize micro-expression in two provided databases, including holdout-database recognition and composite database recognition. Considering the small size of the databases, we follow a rout of transfer learning to implement convolutional neural network to recognize the micro-expression. ResNet10 pre-trained on ImageNet dataset was fine-tuned on macro-expression datasets with large size and then on the provided micro-expression datasets. Experimental results show that the method can achieve weighted average recall (WAR) of 0.561 and unweighted average recall (UAR) of 0.389 in Holdout-database Evaluation Task, and F1 Score of 0.64 in Composite Database Evaluation Task, which are much higher than what baseline methods (LBP-TOP, HOOF, HOG3D) can achieve.},
  keywords = {Databases,deep learning,Face,Face recognition,Feature extraction,Image recognition,micro expression recognition,Task analysis,Training,trasnfer learning},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\3WJFX78L\\Peng et al. - 2018 - From Macro to Micro Expression Recognition Deep L.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\3YI9K732\\8373896.html}
}

@article{pengOGSSLSemiSupervisedClassification2022,
  title = {{{OGSSL}}: {{A Semi-Supervised Classification Model Coupled With Optimal Graph Learning}} for {{EEG Emotion Recognition}}},
  shorttitle = {{{OGSSL}}},
  author = {Peng, Yong and Jin, Fengzhe and Kong, Wanzeng and Nie, Feiping and Lu, Bao-Liang and Cichocki, Andrzej},
  year = {2022},
  journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
  volume = {30},
  pages = {1288--1297},
  issn = {1558-0210},
  doi = {10.1109/TNSRE.2022.3175464},
  abstract = {Electroencephalogram(EEG) signals are generated from central nervous system which are difficult to disguise, leading to its popularity in emotion recognition. Recently,semi-supervisedlearning exhibits promisingemotion recognition performance by involving unlabeled EEG data into model training. However, if we first build a graph to characterize the sample similarities and then perform label propagation on this graph, these two steps cannotwell collaborate with each other. In this paper, we propose an OptimalGraph coupledSemi-Supervised Learning (OGSSL) model for EEG emotion recognition by unifying the adaptive graph learning and emotion recognition into a single objective. Besides, we improve the label indicator matrix of unlabeledsamples in order to directly obtain theiremotional states. Moreover, the key EEG frequency bands and brain regions in emotion expression are automatically recognized by the projectionmatrix of OGSSL. Experimental results on the SEED-IV data set demonstrate that 1) OGSSL achieves excellent average accuracies of 76.51\%, 77.08\% and 81.29\% in three cross-sessionemotion recognition tasks, 2) OGSSL is competent for discriminative EEG feature selection in emotion recognition, and 3) the Gamma frequency band, the left/righttemporal, prefrontal,and (central) parietal lobes are identified to be more correlated with the occurrence of emotions.},
  keywords = {Adaptation models,Brain modeling,Data models,Electroencephalogram (EEG),Electroencephalography,emotion recognition,Emotion recognition,feature selection,graph learning,semi-supervised learning,Semisupervised learning,Task analysis},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\R6IKJRLY\\Peng et al. - 2022 - OGSSL A Semi-Supervised Classification Model Coup.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\LT4C82CY\\9775684.html}
}

@article{pepaAutomaticEmotionRecognition2021a,
  title = {Automatic Emotion Recognition in Clinical Scenario: A Systematic Review of Methods},
  shorttitle = {Automatic Emotion Recognition in Clinical Scenario},
  author = {Pepa, Lucia and Spalazzi, Luca and Capecci, Marianna and Ceravolo, Maria Gabriella},
  year = {2021},
  journal = {IEEE Transactions on Affective Computing},
  pages = {1--1},
  issn = {1949-3045},
  doi = {10.1109/TAFFC.2021.3128787},
  abstract = {Automatic emotion recognition has powerful opportunities in the clinical field, but several critical aspects are still open, such as heterogeneity of methodologies or technologies tested mainly on healthy people. This systematic review aims to survey automatic emotion recognition systems applied in real clinical contexts, to deeply analyse clinical and technical aspects, how they were addressed, and relationships among them. The literature review was conducted on: IEEEXplore, ScienceDirect, Scopus, PubMed, ACM. Inclusion criteria were the presence of an automatic emotion recognition algorithm and the enrollment of at least 2 patients in the experimental protocol. The review process followed the Preferred Reporting Items for Systematic Reviews and Meta-Analyses guidelines. Moreover, the works were analysed according to a reference model to deeply examine both clinical and technical topics. 52 scientific papers passed inclusion criteria. Most clinical scenarios involved neurodevelopmental, neurological and psychiatric disorders with the aims of diagnosing, monitoring, or treating emotional symptoms. The most adopted signals are video and audio, while supervised shallow learning is mostly used for emotion recognition. A poor study design, tiny samples, and the absence of a control group emerged as methodological weaknesses. Heterogeneity of performance metrics, datasets and algorithms challenges results comparability, robustness, reliability and reproducibility.},
  keywords = {Artificial Intelligence,Clinical applications,Diseases,Emotion recognition,Guidelines,Machine Learning,Neurological disorders,Pathology,Protocols,Psychiatric disorders,Systematics,Tools},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\Y3VRA6MR\\Pepa et al. - 2021 - Automatic emotion recognition in clinical scenario.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\Q7A4L4MX\\9618863.html}
}

@article{perrettRepresentationsFacialExpressions2022,
  title = {Representations of Facial Expressions since {{Darwin}}},
  author = {Perrett, David},
  year = {2022/ed},
  journal = {Evolutionary Human Sciences},
  volume = {4},
  publisher = {{Cambridge University Press}},
  issn = {2513-843X},
  doi = {10.1017/ehs.2022.10},
  abstract = {, Darwin's book on expressions of emotion was one of the first publications to include photographs (Darwin, The expression of the emotions in Man and animals, 1872). The inclusion of expression photographs meant that readers could form their own opinions and could, like Darwin, survey others for their interpretations. As such, the images provided an evidence base and an `open source'. Since Darwin, increases in the representativeness and realism of emotional expressions have come from the use of composite images, colour, multiple views and dynamic displays. Research on understanding emotional expressions has been aided by the use of computer graphics to interpolate parametrically between different expressions and to extrapolate exaggerations. This review tracks the developments in how emotions are illustrated and studied and considers where to go next.},
  langid = {english},
  keywords = {Expression,face,representation},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\RU32FZHK\\Perrett - 2022 - Representations of facial expressions since Darwin.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\3362I7BP\\3360FB31C8AE5CC31A0C290ADA92A8B1.html}
}

@inproceedings{pilatoSubSymbolicSemanticLayer2007,
  title = {Sub-{{Symbolic Semantic Layer}} in {{Cyc}} for {{Intuitive Chat-Bots}}},
  booktitle = {International {{Conference}} on {{Semantic Computing}} ({{ICSC}} 2007)},
  author = {Pilato, G. and Augello, A. and Vassallo, G. and Gaglio, S.},
  year = {2007},
  month = sep,
  pages = {121--128},
  doi = {10.1109/ICSC.2007.37},
  abstract = {The work presented in this paper aims to combine Latent Semantic Analysis methodology, common sense and traditional knowledge representation in order to improve the dialogue capabilities of a conversational agent. In our approach the agent brain is characterized by two areas: a "rational area", composed by a structured, rule-based knowledge base, and an "associative area", obtained through a data- driven semantic space. Concepts are mapped in this space and their mutual geometric distance is related to their conceptual similarity. The geometric distance between concepts implicitly defines a sub-symbolic relationship net, which can be seen as a new "sub- symbolic semantic layer" automatically added to the Cyc ontology. Users queries can also be mapped in the same conceptual space, and evoke similar ontology concepts. As a result the agent can exploit this feature, attempting to retrieve ontological concepts that are not easily reachable by means of the traditional ontology reasoning engine.},
  keywords = {Buildings,Councils,Engines,Humans,Joining processes,Knowledge representation,Natural languages,Ontologies,Pattern matching,User interfaces},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\SDBT3XID\\Pilato et al. - 2007 - Sub-Symbolic Semantic Layer in Cyc for Intuitive C.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\DFZ5YDAK\\4338340.html}
}

@incollection{plutchikGeneralPsychoevolutionaryTheory1980,
  title = {A General Psychoevolutionary Theory of Emotion},
  booktitle = {Theories of Emotion},
  author = {Plutchik, Robert},
  year = {1980},
  pages = {3--33},
  publisher = {{Elsevier}},
  note = {Emotion Theory Concepts, for instance ekman and pleasure-arousal-dominance framework (PAD) or newer concepts like Plutchnik model},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\YZNYL5YC\\B9780125587013500077.html}
}

@inproceedings{prabhaDesignAutomatedRecurrent2022,
  title = {Design of an {{Automated Recurrent Neural Network}} for {{Emotional Intelligence Using Deep Neural Networks}}},
  booktitle = {2022 4th {{International Conference}} on {{Smart Systems}} and {{Inventive Technology}} ({{ICSSIT}})},
  author = {Prabha, R. and A, Mr. Senthil G. and Anandan, P. and Sivarajeswari, S. and Saravanakumar, C. and Vijendra Babu, D.},
  year = {2022},
  month = jan,
  pages = {1061--1067},
  doi = {10.1109/ICSSIT53264.2022.9716420},
  abstract = {Emotional intelligence (EI) is a collection of quasi skills, attitudes, and talents that influence one's ability to respond quickly to environmental changes and stresses. Nevertheless, it is not always possible to monitor the effect of a multitude of variables involved in behavioral phenomena. In several different industries, Emotion Perception or Artificial Emotional Intelligence is now a \$20 billion research area with applications. In a variety of ways, artificial emotional intelligence can operate through industries. Emotional readings may also be used by AI as relating to decision, such as in advertising campaigns. In terms of temporal dynamics, a powerful learning method is required to extract high-level representations of emotional responses. In terms of spatial dispersion, a learning process strategy is required to extract high-level assessment of emotional states. The recurrent neural network outperforms linear models in terms of prediction accuracy. A recurrent model is proposed in this paper to forecast the pattern between the variables of age, gender, occupation, marital status, and education in order to predict the EI. The appropriate recurrent model is capable of predicting EI with important correlations in most of its dimensions and could demonstrate the advantage over regression models in predicting EI using sociological parameters. This model will estimate the level of EI in the various occupational, professional, gender and age groups and provide a planning basis for addressing possible deficiencies in each group.},
  keywords = {Anxiety disorders,Artificial Intelligence,Correlation,Deep neural network,Education,Emotion recognition,Emotional Intelligence,Facial recognition,Industries,Learning systems,Recurrent network,Recurrent neural networks},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\WE4B7P8X\\Prabha et al. - 2022 - Design of an Automated Recurrent Neural Network fo.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\LVQ8TUJJ\\9716420.html}
}

@inproceedings{priyaEmotionRecognitionUsing2021,
  title = {Emotion {{Recognition Using Deep Learning}}},
  booktitle = {2021 {{IEEE Applied Imagery Pattern Recognition Workshop}} ({{AIPR}})},
  author = {Priya, R.N. Beena and Hanmandlu, M. and Vasikarla, Shantaram},
  year = {2021},
  month = oct,
  pages = {1--5},
  issn = {2332-5615},
  doi = {10.1109/AIPR52630.2021.9762207},
  abstract = {Recognition of human emotions such as anger, disgust, fear, happiness, sadness, surprise and neutrality through facial expression is one of the important research topics in human computer interaction. There are several methods in the literature to identify emotions using machine learning and Artificial Intelligence techniques. This paper investigates the suitability of deep learning methods for the classification of emotions. Two classes of deep learning models: one belonging to non-pretrained models such as ConvNet, LeNet and the other belonging to pretrained models such as VGG 19 and MobileNet have been experimented for the extraction of features followed by Softmax for the classification. The classification accuracy of 95\% is obtained with LeNet whereas MobileNeT gives the best accuracy of 96\% on the database.},
  keywords = {Computational modeling,Computer architecture,ConvNet,Convolutional neural network (CNN),Databases,Deep learning,Emotion recognition,Emotions,Face recognition,LeNet,MobileNet,Neural networks,VGG19},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\X9H3RGFY\\Priya et al. - 2021 - Emotion Recognition Using Deep Learning.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\M38ZMCZW\\9762207.html}
}

@article{quinnRealtimeEmotionRecognition2017,
  title = {Real-Time Emotion Recognition from Facial Expressions},
  author = {Quinn, Minh-An and Sivesind, Grant and Reis, Guilherme},
  year = {2017},
  journal = {Standford University},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\PUMBLEVY\\Quinn et al. - 2017 - Real-time emotion recognition from facial expressi.pdf}
}

@article{r.a.calvoAffectDetectionInterdisciplinary2010,
  title = {Affect {{Detection}}: {{An Interdisciplinary Review}} of {{Models}}, {{Methods}}, and {{Their Applications}}},
  author = {R. A. Calvo and S. D' Mello},
  year = {2010},
  journal = {IEEE Transactions on Affective Computing},
  volume = {1},
  number = {1},
  pages = {18--37},
  issn = {1949-3045},
  doi = {10.1109/T-AFFC.2010.1},
  abstract = {This survey describes recent progress in the field of Affective Computing (AC), with a focus on affect detection. Although many AC researchers have traditionally attempted to remain agnostic to the different emotion theories proposed by psychologists, the affective technologies being developed are rife with theoretical assumptions that impact their effectiveness. Hence, an informed and integrated examination of emotion theories from multiple areas will need to become part of computing practice if truly effective real-world systems are to be achieved. This survey discusses theoretical perspectives that view emotions as expressions, embodiments, outcomes of cognitive appraisal, social constructs, products of neural circuitry, and psychological interpretations of basic feelings. It provides meta-analyses on existing reviews of affect detection systems that focus on traditional affect detection modalities like physiology, face, and voice, and also reviews emerging research on more novel channels such as text, body language, and complex multimodal systems. This survey explicitly explores the multidisciplinary foundation that underlies all AC applications by describing how AC researchers have incorporated psychological theories of emotion and how these theories affect research questions, methods, results, and their interpretations. In this way, models and methods can be compared, and emerging insights from various disciplines can be more expertly integrated.},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\6HGY87HY\\Affect_Detection_An_Interdisciplinary_Review_o.pdf}
}

@article{r.subramanianASCERTAINEmotionPersonality2018,
  title = {{{ASCERTAIN}}: {{Emotion}} and {{Personality Recognition Using Commercial Sensors}}},
  author = {R. Subramanian and J. Wache and M. K. Abadi and R. L. Vieriu and S. Winkler and N. Sebe},
  year = {2018},
  journal = {IEEE Transactions on Affective Computing},
  volume = {9},
  number = {2},
  pages = {147--160},
  issn = {1949-3045},
  doi = {10.1109/TAFFC.2016.2625250},
  abstract = {We present ASCERTAIN-a multimodal databaASe for impliCit pERsonaliTy and Affect recognitIoN using commercial physiological sensors. To our knowledge, ASCERTAIN is the first database to connect personality traits and emotional states via physiological responses. ASCERTAIN contains big-five personality scales and emotional self-ratings of 58 users along with their Electroencephalogram (EEG), Electrocardiogram (ECG), Galvanic Skin Response (GSR) and facial activity data, recorded using off-the-shelf sensors while viewing affective movie clips. We first examine relationships between users' affective ratings and personality scales in the context of prior observations, and then study linear and non-linear physiological correlates of emotion and personality. Our analysis suggests that the emotion-personality relationship is better captured by non-linear rather than linear statistics. We finally attempt binary emotion and personality trait recognition using physiological features. Experimental results cumulatively confirm that personality differences are better revealed while comparing user responses to emotionally homogeneous videos, and above-chance recognition is achieved for both affective and personality dimensions.},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\FCPUEEF6\\ASCERTAIN_Emotion_and_Personality_Recognition_.pdf}
}

@article{rabahConvergenceAIIoT2018,
  title = {Convergence of {{AI}}, {{IoT}}, Big Data and Blockchain: A Review},
  shorttitle = {Convergence of {{AI}}, {{IoT}}, Big Data and Blockchain},
  author = {Rabah, Kefa},
  year = {2018},
  journal = {The lake institute Journal},
  volume = {1},
  number = {1},
  pages = {1--18},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\E3PFWZD9\\Rabah - 2018 - Convergence of AI, IoT, big data and blockchain a.pdf}
}

@inproceedings{rajuContinuousMultimodalEmotion2021,
  title = {Continuous {{Multi-modal Emotion Prediction}} in {{Video}} Based on {{Recurrent Neural Network Variants}} with {{Attention}}},
  booktitle = {2021 20th {{IEEE International Conference}} on {{Machine Learning}} and {{Applications}} ({{ICMLA}})},
  author = {Raju, Joyal and Gaus, Yona Falinie A. and Breckon, Toby P.},
  year = {2021},
  month = dec,
  pages = {688--693},
  doi = {10.1109/ICMLA52953.2021.00115},
  abstract = {Automatic perception and understanding of human emotion is becoming an increasingly attractive research field in artificial intelligence and human-computer interaction. Emotion portrayal within conversation plays a significant role in the semantics of a sentence. However, emotion is not only biologically determined but is also influenced by the environment. Therefore, cultural differences exist in some aspects of emotions, and it is important for the next generation of computer systems to adapt the cross-cultural difference in order to enable more naturalistic interactions between humans and machines. In this paper, we investigate the suitability of state-of-the-art deep learning architectures based on recurrent neural network (RNN) variants with explicit attention modelling to bridge the gap across different cultures (German and Hungarian) for emotion prediction in video. Three different attention based network architectures are proposed in this work:- early attention fusion, extended multi-attention fusion and attention-based encoder-decoder. Our RNN variants with explicit attention modelling approach achieves very promising Concordance Correlation Coefficient results, which outperform the baseline on Arousal of 0.637 vs. 0.614 (baseline), for Valence of 0.689 vs. 0.615 and for Liking of 0.625 vs. 0.222.},
  keywords = {affective computing,attention network,Computer architecture,cross-cultural,Deep learning,emotion,emotion recognition,Human computer interaction,motion detection,multi-modal,Predictive models,Recurrent neural networks,Semantics,Text recognition},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\L39GNV5I\\Raju et al. - 2021 - Continuous Multi-modal Emotion Prediction in Video.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\TYVLST6S\\9680001.html}
}

@inproceedings{ramalingamFacialExpressionRecognition2018,
  title = {Facial {{Expression Recognition}} Using {{Transfer Learning}}},
  booktitle = {2018 {{International Carnahan Conference}} on {{Security Technology}} ({{ICCST}})},
  author = {Ramalingam, Soodamani and Garzia, Fabio},
  year = {2018},
  month = oct,
  pages = {1--5},
  issn = {2153-0742},
  doi = {10.1109/CCST.2018.8585504},
  abstract = {In this paper, we investigate Deep Learning architectures for the recognition of facial expressions. In particular, we consider the concept of Transfer Learning whereby features learnt from generic images of large scale datasets can be used to train models of smaller databases without losing the generalization ability.},
  keywords = {Computer architecture,Convolutional neural networks,Convolutional Neural Networks,Databases,deep learning,Emotion recognition,Face recognition,facial expression recognition,Task analysis,Training,transfer learning},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\8PSUTUSE\\Ramalingam und Garzia - 2018 - Facial Expression Recognition using Transfer Learn.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\L4HEHCR8\\8585504.html}
}

@inproceedings{raniMachineLearningModel2021,
  title = {A {{Machine Learning Model}} for {{Kids}}' {{Behavior Analysis}} from {{Facial Emotions}} Using {{Principal Component Analysis}}},
  booktitle = {2021 5th {{Asian Conference}} on {{Artificial Intelligence Technology}} ({{ACAIT}})},
  author = {Rani, Sita and Bhambri, Pankaj and Chauhan, Meetali},
  year = {2021},
  month = oct,
  pages = {522--525},
  doi = {10.1109/ACAIT53529.2021.9731203},
  abstract = {Identification of the emotional state of humans, especially kids', is a very complex activity. Different types of emotions contribute to the behavior of kids. There are various methods to recognize the emotional state like verbal communication, non-verbal gestures like movement of hands, voice tone and facial expressions. Among these, recognition of the facial expressions is the most widely used method to characterize human emotions further to predict human behavior. In this work, a machine learning model is proposed to recognize the emotional state of the kids', i.e., toddlers and preschoolers. Proposed model is based on PCA technique and MLP classifier. Data set is pre-processed using gradient filtering and extracted features are optimized using PSO. Training data used in this work, comprise of 273 facial images of the kids in the age group of 2 to 5 years. Dataset belonged to four facial expressions, i.e., happy, sad, neutral and thoughtful. Proposed model gave better results in comparison to two existing model with an accuracy of 95.63\%. The proposed model can further be enhanced for emotion recognition and behavior analysis of mentally retarded kids.},
  keywords = {Analytical models,Behavior Analysis,Emotion recognition,Face recognition,Facial Expression Recognition (FER),Filtering,Kids’ Emotion Recognition,Machine learning,Pediatrics,Principal Component Analysis (PCA),Training data},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\QH2HHZ9K\\Rani et al. - 2021 - A Machine Learning Model for Kids’ Behavior Analys.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\Q2FKBQ48\\9731203.html}
}

@misc{Replika2021,
  title = {Replika},
  year = {2021},
  month = nov,
  journal = {replika.com},
  abstract = {Always here to listen and talk. Always on your side. Join the millions growing with their AI~friends~now!},
  howpublished = {https://replika.com},
  langid = {english},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\WKCJ4K6G\\replika.ai.html}
}

@inproceedings{romero-morenoAIFacialRecognition2021,
  title = {{{AI}} Facial Recognition and Biometric Detection: Balancing Consumer Rights and Corporate Interests},
  shorttitle = {{{AI}} Facial Recognition and Biometric Detection},
  booktitle = {2021 {{International Carnahan Conference}} on {{Security Technology}} ({{ICCST}})},
  author = {{Romero-Moreno}, Felipe},
  year = {2021},
  month = oct,
  pages = {1--5},
  issn = {2153-0742},
  doi = {10.1109/ICCST49569.2021.9717403},
  abstract = {The purpose of this study is two-fold. Firstly, to critically assess the extent to which corporate actors can lawfully use artificial intelligence (AI) technology for real-time facial recognition biometric detection. Secondly, to suggest and appraise some procedural safeguards to make the use of these systems by private actors compatible with consumers' right to protection of their personal data under the General Data Protection Regulation (GDPR). This study seeks to fill an existing gap in the literature. It concludes that unless, the three variables suggested in the study are considered, that is, `whether', `when' and `how' corporate actors can legally use AI for real-time facial recognition biometric detection, the use of this technology will violate consumers' data protection rights.},
  keywords = {Artificial intelligence,Artificial Intelligence,Biometric Detection,Complexity theory,Emotion recognition,Face recognition,Facial Recognition,General Data Protection Regulation,Real-time systems,Security},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\C83TJH3N\\Romero-Moreno - 2021 - AI facial recognition and biometric detection bal.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\4VGQ3HN2\\9717403.html}
}

@article{rosenthal-vonderputtenExperimentalStudyEmotional2013,
  title = {An {{Experimental Study}} on {{Emotional Reactions Towards}} a {{Robot}}},
  author = {{Rosenthal-von~der~P{\"u}tten}, Astrid M. and Kr{\"a}mer, Nicole C. and Hoffmann, Laura and Sobieraj, Sabrina and Eimler, Sabrina C.},
  year = {2013},
  month = jan,
  journal = {International Journal of Social Robotics},
  volume = {5},
  number = {1},
  pages = {17--34},
  issn = {1875-4805},
  doi = {10.1007/s12369-012-0173-8},
  abstract = {Although robots are starting to enter into our professional and private lives, little is known about the emotional effects which robots elicit. However, insights into this topic are an important prerequisite when discussing, for example, ethical issues regarding the question of what role we (want to) allow robots to play in our lives. In line with the Media Equation, humans may react towards robots as they do towards humans, making it all the more important to carefully investigate the preconditions and consequences of contact with robots. Based on assumptions on the socialness of reactions towards robots and anecdotal evidence of emotional attachments to robots (e.g. Klamer and BenAllouch in Trappl R. (ed.), Proceedings of EMCSR 2010, Vienna, 2010; Klamer and BenAllouch in Proceedings of the 27th International Conference on Human Factors in Computing Systems (CHI-2010), Atlanta, GA. ACM, New York, 2010; Kr\"amer et al. in Appl. Artif. Intell. 25(6):474\textendash 502, 2011), we conducted a study that provides further insights into the question of whether humans show emotional reactions towards Ugobe's Pleo, which is shown in different situations. We used a 2\texttimes 2 design with one between-subjects factor ``prior interaction with the robot'' (never seen the robot before vs. 10-minute interaction with the robot) and a within-subject factor ``type of video'' (friendly interaction video vs. torture video). Following a multi-method approach, we assessed participants' physiological arousal and self-reported emotions as well as their general evaluation of the videos and the robot. In line with our hypotheses, participants showed increased physiological arousal during the reception of the torture video as compared to the normal video. They also reported fewer positive and more negative feelings after the torture video and expressed empathic concern for the robot. It appears that the acquaintance with the robot does not play a role, as ``prior interaction with the robot'' showed no effect.},
  langid = {english},
  keywords = {Emotional response,Empathy,Experimental study,Human-robot interaction,Psychophysiological measures},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\EAMRCFER\\Rosenthal-von der Pütten et al. - 2013 - An Experimental Study on Emotional Reactions Towar.pdf}
}

@article{rouastDeepLearningHuman2019,
  title = {Deep Learning for Human Affect Recognition: {{Insights}} and New Developments},
  shorttitle = {Deep Learning for Human Affect Recognition},
  author = {Rouast, Philipp V. and Adam, Marc and Chiong, Raymond},
  year = {2019},
  journal = {IEEE Transactions on Affective Computing},
  publisher = {{IEEE}},
  keywords = {read},
  note = {Review on Machine Learning Models},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\HBP4UT8V\\Rouast et al. - 2019 - Deep learning for human affect recognition Insigh.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\9KPPIERV\\8598999.html}
}

@article{roySurveyMachineLearning2021,
  title = {A {{Survey}} of {{Machine Learning Techniques}} for {{Indoor Localization}} and {{Navigation Systems}}},
  author = {Roy, Priya and Chowdhury, Chandreyee},
  year = {2021},
  month = mar,
  journal = {Journal of Intelligent \& Robotic Systems},
  volume = {101},
  number = {3},
  pages = {63},
  issn = {1573-0409},
  doi = {10.1007/s10846-021-01327-z},
  abstract = {In the recent past, we have witnessed the adoption of different machine learning techniques for indoor positioning applications using WiFi, Bluetooth and other technologies. The techniques range from heuristically derived hand-crafted feature-based traditional machine learning algorithms, feature selection algorithms to the hierarchically self-evolving feature-based Deep Learning algorithms. The transient and chaotic nature of the WiFi/Bluetooth fingerprint data along with different signal sensitivity of different device configurations presents numerous challenges that influence the performance of the indoor localization system in the wild. This article is intended to offer a comprehensive state-of-the-art survey on machine learning techniques that have recently been adopted for localization purposes. Hence, we review the applicability of machine learning techniques in this domain along with basic localization principles, applications, and the underlying problems and challenges associated with the existing systems. We also articulate the recent advances and state-of-the-art machine learning techniques to visualize the possible future directions in the research field of indoor localization.},
  langid = {english},
  keywords = {Deep learning,Extreme learning machine,Fingerprinting,Indoor localization,Mobile robot,SLAM,Supervised learning,Transfer learning},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\SA4XWCDC\\Roy und Chowdhury - 2021 - A Survey of Machine Learning Techniques for Indoor.pdf}
}

@inproceedings{rupanagudiNovelVideoProcessing2015,
  title = {A Novel Video Processing Based Cost Effective Smart Trolley System for Supermarkets Using {{FPGA}}},
  booktitle = {2015 {{International Conference}} on {{Communication}}, {{Information Computing Technology}} ({{ICCICT}})},
  author = {Rupanagudi, Sudhir Rao and Jabeen, Fathima and R, Vaishnav Ram Savarni K. and Adinarayana, Sindhu and Bharadwaj, Vinay K and R, Karishma and Bhat, Varsha G.},
  year = {2015},
  month = jan,
  pages = {1--6},
  doi = {10.1109/ICCICT.2015.7045723},
  abstract = {One of the major problems faced by consumers while shopping at a supermarket is the inability to locate items and also to carry goods to the billing counter. In this paper, we describe a novel cost-effective method to overcome these issues by creating a smart trolley using a web camera along with video processing to complete the tasks. In comparison with previous methods which utilize RFID transceivers, our solution costs 10 times lesser than its predecessors and is environment friendly as well.},
  keywords = {basket,Cameras,Color,Computers,FPGA,Image color analysis,Prototypes,Radiofrequency identification,RFID,robotic assistant,Robots,shopping assistant,shopping cart,smart shopping system,smart trolley,supermarket,trolley,video processing},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\5C9G9I28\\Rupanagudi et al. - 2015 - A novel video processing based cost effective smar.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\W5MJXM5K\\7045723.html}
}

@article{sabourDynamicRoutingCapsules2017,
  title = {Dynamic {{Routing Between Capsules}}},
  author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E.},
  year = {2017},
  month = nov,
  journal = {arXiv:1710.09829 [cs]},
  eprint = {1710.09829},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\SY9VVRRX\\Sabour et al. - 2017 - Dynamic Routing Between Capsules.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\P4F2FD7G\\1710.html}
}

@inproceedings{saganowskiColdStartProblem2022,
  title = {The {{Cold Start Problem}} and {{Per-Group Personalization}} in {{Real-Life Emotion Recognition With Wearables}}},
  booktitle = {2022 {{IEEE International Conference}} on {{Pervasive Computing}} and {{Communications Workshops}} and Other {{Affiliated Events}} ({{PerCom Workshops}})},
  author = {Saganowski, Stanis{\l}aw and Kunc, Dominika and Perz, Bartosz and Komoszy{\'n}ska, Joanna and Behnke, Maciej and Kazienko, Przemys{\l}aw},
  year = {2022},
  month = mar,
  pages = {812--817},
  doi = {10.1109/PerComWorkshops53856.2022.9767233},
  abstract = {Emotion recognition in real life from physiological signals provided by wrist worn devices still remains a great challenge especially due to difficulties with gathering annotated emotional events. For that purpose, we suggest building pre-trained machine learning models capable of detecting intense emotional states. This work aims to explore the cold start problem, where no data from the target subjects (users) are available at the beginning of the experiment to train the reasoning model. To address this issue, we investigate the potential of per-group personalization and the amount of data needed to perform it. Our results on real-life data indicate that even a week's worth of personalized data improves the model performance.},
  keywords = {Analytical models,cold start,Computational modeling,Conferences,Emognition,emotion recognition,Emotion recognition,field studies,personalization,physiological signals,smartwatch,Training,Wearable computers,Wrist},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\57CELB8B\\Saganowski et al. - 2022 - The Cold Start Problem and Per-Group Personalizati.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\AQ985VSV\\9767233.html}
}

@article{saganowskiEmotionRecognitionEveryday2022,
  title = {Emotion {{Recognition}} for {{Everyday Life Using Physiological Signals}} from {{Wearables}}: {{A Systematic Literature Review}}},
  shorttitle = {Emotion {{Recognition}} for {{Everyday Life Using Physiological Signals}} from {{Wearables}}},
  author = {Saganowski, Stanislaw and Perz, Bartosz and Polak, Adam and Kazienko, Przemyslaw},
  year = {2022},
  journal = {IEEE Transactions on Affective Computing},
  pages = {1--1},
  issn = {1949-3045},
  doi = {10.1109/TAFFC.2022.3176135},
  abstract = {Smart wearables, equipped with sensors monitoring physiological parameters, are becoming an integral part of our life. In this work, we investigate the possibility of utilizing such wearables to recognize emotions in the wild. In most reviewed papers, the authors apply a similar procedure consisting of participant recruitment, stimuli preparation and annotation, signal collection and processing, self-assessment, and machine learning model learning and validation. Besides, we identified seven emotion recognition scenarios and analyzed the transition from psychological models to machine learning tasks.},
  keywords = {affective computing,Bibliographies,Biomedical monitoring,emotion recognition,Emotion recognition,field studies,Monitoring,personal device,review,Sensors,smart band,smartwatch,survey,systematic literature review,Systematics,validation,wearable,Wearable sensors},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\HSV6Y27H\\Saganowski et al. - 2022 - Emotion Recognition for Everyday Life Using Physio.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\LL6ZAPGL\\9779458.html}
}

@article{sarkerNeuroSymbolicArtificialIntelligence2021,
  title = {Neuro-{{Symbolic Artificial Intelligence}}: {{Current Trends}}},
  shorttitle = {Neuro-{{Symbolic Artificial Intelligence}}},
  author = {Sarker, Md Kamruzzaman and Zhou, Lu and Eberhart, Aaron and Hitzler, Pascal},
  year = {2021},
  month = may,
  journal = {arXiv:2105.05330 [cs]},
  eprint = {2105.05330},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Neuro-Symbolic Artificial Intelligence -- the combination of symbolic methods with methods that are based on artificial neural networks -- has a long-standing history. In this article, we provide a structured overview of current trends, by means of categorizing recent publications from key conferences. The article is meant to serve as a convenient starting point for research on the general topic.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  note = {Very Important
\par
Current Trends: Five different meaning of Neuro-symbolic AI
\par
Comment: under review},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\GQ84DHTS\\Sarker et al. - 2021 - Neuro-Symbolic Artificial Intelligence Current Tr.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\LN7L4N5R\\2105.html}
}

@inproceedings{savranBosphorusDatabase3D2008,
  title = {Bosphorus Database for {{3D}} Face Analysis},
  booktitle = {European Workshop on Biometrics and Identity Management},
  author = {Savran, Arman and Aly{\"u}z, Ne{\c s}e and Dibeklio{\u g}lu, Hamdi and {\c C}eliktutan, Oya and G{\"o}kberk, Berk and Sankur, B{\"u}lent and Akarun, Lale},
  year = {2008},
  pages = {47--56},
  publisher = {{Springer}},
  keywords = {Datasets},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\2V5L842J\\Bosphorus.pdf}
}

@article{saxenaDepthEstimationUsing,
  title = {Depth {{Estimation}} Using {{Monocular}} and {{Stereo Cues}}},
  author = {Saxena, Ashutosh and Schulte, Jamie and Ng, Andrew Y},
  pages = {7},
  abstract = {Depth estimation in computer vision and robotics is most commonly done via stereo vision (stereopsis), in which images from two cameras are used to triangulate and estimate distances. However, there are also numerous monocular visual cues\textemdash such as texture variations and gradients, defocus, color/haze, etc.\textemdash that have heretofore been little exploited in such systems. Some of these cues apply even in regions without texture, where stereo would work poorly. In this paper, we apply a Markov Random Field (MRF) learning algorithm to capture some of these monocular cues, and incorporate them into a stereo system. We show that by adding monocular cues to stereo (triangulation) ones, we obtain significantly more accurate depth estimates than is possible using either monocular or stereo cues alone. This holds true for a large variety of environments, including both indoor environments and unstructured outdoor environments containing trees/forests, buildings, etc. Our approach is general, and applies to incorporating monocular cues together with any off-the-shelf stereo system.},
  langid = {english},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\GYC3YFNL\\Saxena et al. - Depth Estimation using Monocular and Stereo Cues.pdf}
}

@article{schererAppraisalTheory1999,
  title = {Appraisal Theory.},
  author = {Scherer, Klaus R.},
  year = {1999},
  publisher = {{John Wiley \& Sons Ltd}},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\WIDXHVIL\\1999-04021-030.html}
}

@article{schererWhatAreEmotions2005,
  title = {What Are Emotions? {{And}} How Can They Be Measured?},
  shorttitle = {What Are Emotions?},
  author = {Scherer, Klaus R.},
  year = {2005},
  journal = {Social science information},
  volume = {44},
  number = {4},
  pages = {695--729},
  publisher = {{Sage Publications Sage CA: Thousand Oaks, CA}},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\GFXMFAJP\\Scherer - 2005 - What are emotions And how can they be measured.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\C3Q9GRCT\\0539018405058216.html}
}

@article{schiestlWasVerratUns2016,
  title = {Was Verr\"at Uns Unsere {{Mimik}}? {{Ein Streifzug}} Durch Die Aktuelle {{Mimikforschung}}},
  shorttitle = {Was Verr\"at Uns Unsere {{Mimik}}?},
  author = {Schiestl, Cathrin and Beermann, Ursula},
  year = {2016},
  month = dec,
  journal = {Psychologie in \"Osterreich},
  volume = {5},
  pages = {300--307},
  abstract = {Serien wie ,,Lie to Me`` vermitteln, dass Mimik genau entschl\"usselt werden kann. Wie die Forschungslage dazu tats\"achlich aussieht, soll im folgenden Artikel gezeigt werden. Zu Beginn wird eine Methode der Mimikforschung, das Facial Action Coding System (FACS), vorgestellt, welches auch in ,,Lie to Me`` zum Einsatz kommt. Anschlie\ss end wird erl\"autert, welche Funktionen mimische Ausdr\"ucke haben k\"onnen. Danach wird aufgezeigt, warum man L\"ugen nur bedingt an der Mimik erkennen kann. Als Beispiele f\"ur spezifische mimische Ausdr\"ucken wurden Weinen und Lachen ausgew\"ahlt, anhand derer erkl\"art wird, warum mimische Zeichen nicht immer eindeutig sind. Am Ende wird auf Zusammenh\"ange zwischen den menschlichen Genen und der Mimik eingegangen.},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\QHXFIY9L\\Schiestl_Beermann_MimikartikelBPZeitschriftDezember2016.pdf}
}

@article{schlosbergThreeDimensionsEmotion1954,
  title = {Three Dimensions of Emotion.},
  author = {Schlosberg, Harold},
  year = {1954},
  journal = {Psychological review},
  volume = {61},
  number = {2},
  pages = {81},
  publisher = {{American Psychological Association}},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\YVNF5HWV\\1954-08509-001.html}
}

@misc{schwarzMetaLearningSparseCompression2022,
  title = {Meta-{{Learning Sparse Compression Networks}}},
  author = {Schwarz, Jonathan Richard and Teh, Yee Whye},
  year = {2022},
  month = may,
  number = {arXiv:2205.08957},
  eprint = {2205.08957},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  institution = {{arXiv}},
  abstract = {Recent work in Deep Learning has re-imagined the representation of data as functions mapping from a coordinate space to an underlying continuous signal. When such functions are approximated by neural networks this introduces a compelling alternative to the more common multi-dimensional array representation. Recent work on such Implicit Neural Representations (INRs) has shown that - following careful architecture search - INRs can outperform established compression methods such as JPEG (e.g. Dupont et al., 2021). In this paper, we propose crucial steps towards making such ideas scalable: Firstly, we employ stateof-the-art network sparsification techniques to drastically improve compression. Secondly, introduce the first method allowing for sparsification to be employed in the inner-loop of commonly used Meta-Learning algorithms, drastically improving both compression and the computational cost of learning INRs. The generality of this formalism allows us to present results on diverse data modalities such as images, manifolds, signed distance functions, 3D shapes and scenes, several of which establish new state-of-the-art results.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\3UUBZSND\\Schwarz und Teh - 2022 - Meta-Learning Sparse Compression Networks.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\KL4DCUIZ\\2205.html}
}

@article{setchiArtificialIntelligencePatent2021,
  title = {Artificial Intelligence for Patent Prior Art Searching},
  author = {Setchi, Rossitza and Spasi{\'c}, Irena and Morgan, Jeffrey and Harrison, Christopher and Corken, Richard},
  year = {2021},
  month = mar,
  journal = {World Patent Information},
  volume = {64},
  pages = {102021},
  issn = {0172-2190},
  doi = {10.1016/j.wpi.2021.102021},
  abstract = {This study explored how artificial intelligence (AI) could assist patent examiners as part of the prior art search process. The proof-of-concept allowed experimentation with different AI techniques to suggest search terms, retrieve most relevant documents, rank them and visualise their content. The study suggested that AI is less effective in formulating search queries but can reduce the time and cost of the process of sifting through a large number of patents. The study highlighted the importance of the humanin-the-loop approach and the need for better tools for human-centred decision and performance support in prior art searching.},
  langid = {english},
  keywords = {Artificial intelligence,Human-machine systems,Information retrieval,Patents,Prior art search},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\NYDSBBTW\\Setchi et al. - 2021 - Artificial intelligence for patent prior art searc.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\5T34T8FJ\\S017221902100003X.html;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\FPUQABUE\\S017221902100003X.html}
}

@article{shaoThreeConvolutionalNeural2019,
  title = {Three Convolutional Neural Network Models for Facial Expression Recognition in the Wild},
  author = {Shao, Jie and Qian, Yongsheng},
  year = {2019},
  journal = {Neurocomputing},
  volume = {355},
  pages = {82--92},
  issn = {0925-2312},
  keywords = {Convolutional neural network,DeepLearning,Dual-branch CNN,FER in the wild,Pretrained CNN,Shallow network},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\R4U9EKFY\\Shao und Qian - 2019 - Three convolutional neural network models for faci.pdf}
}

@inproceedings{sharmaFacialEmotionBased2020,
  title = {Facial {{Emotion Based Review Accumulation System}}},
  booktitle = {2020 {{IEEE}} 17th {{India Council International Conference}} ({{INDICON}})},
  author = {Sharma, Himanshu and Sharma, Devang and Bhatt, Krutarth and Shah, Bhavya},
  year = {2020},
  month = dec,
  pages = {1--6},
  issn = {2325-9418},
  doi = {10.1109/INDICON49873.2020.9342467},
  abstract = {Multiple cases of fake reviews and false ratings concerning any place on Google Maps or any other advertising platform have been recorded over the past few years. In this paper, we develop a Visitor Rating System by employing a Raspberry Pi camera integrated with Raspberry Pi. The system mainly depends on the AWS cloud services which are utilized for storing and analyzing the collected ratings. For recording authentic ratings of actual visitors, the system uses Facial Emotion Recognition model to generate the rating data. The experimental results state that the system can recognize the emotions with 75\% accuracy having more than 90\% probability of recognition for any particular emotion. The proposed system which overcomes the limitation of false ratings is estimated to cost around 7,000 INR.},
  keywords = {AWS IAM,AWS QuickSight,AWS S3,Business,Cameras,Conferences,Customer services,Data models,Emotion recognition,Facial Emotion Recognition,Google Maps,Internet,Raspberry Pi},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\A44L97U7\\Sharma et al. - 2020 - Facial Emotion Based Review Accumulation System.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\5XEI93TY\\9342467.html}
}

@misc{shengDeepLearningVisual2022,
  title = {Deep {{Learning}} for {{Visual Speech Analysis}}: {{A Survey}}},
  shorttitle = {Deep {{Learning}} for {{Visual Speech Analysis}}},
  author = {Sheng, Changchong and Kuang, Gangyao and Bai, Liang and Hou, Chenping and Guo, Yulan and Xu, Xin and Pietik{\"a}inen, Matti and Liu, Li},
  year = {2022},
  month = may,
  number = {arXiv:2205.10839},
  eprint = {2205.10839},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  abstract = {Visual speech, referring to the visual domain of speech, has attracted increasing attention due to its wide applications, such as public security, medical treatment, military defense, and film entertainment. As a powerful AI strategy, deep learning techniques have extensively promoted the development of visual speech learning. Over the past five years, numerous deep learning based methods have been proposed to address various problems in this area, especially automatic visual speech recognition and generation. To push forward future research on visual speech, this paper aims to present a comprehensive review of recent progress in deep learning methods on visual speech analysis. We cover different aspects of visual speech, including fundamental problems, challenges, benchmark datasets, a taxonomy of existing methods, and state-of-the-art performance. Besides, we also identify gaps in current research and discuss inspiring future research directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: 20 pages, 8 figures},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\SRGDIXAV\\Sheng et al. - 2022 - Deep Learning for Visual Speech Analysis A Survey.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\26E348XE\\2205.html}
}

@article{shindoNeuroSymbolicForwardReasoning2021,
  title = {Neuro-{{Symbolic Forward Reasoning}}},
  author = {Shindo, Hikaru and Dhami, Devendra Singh and Kersting, Kristian},
  year = {2021},
  month = oct,
  journal = {arXiv:2110.09383 [cs]},
  eprint = {2110.09383},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Reasoning is an essential part of human intelligence and thus has been a long-standing goal in artificial intelligence research. With the recent success of deep learning, incorporating reasoning with deep learning systems, i.e., neuro-symbolic AI has become a major field of interest. We propose the Neuro-Symbolic Forward Reasoner (NSFR), a new approach for reasoning tasks taking advantage of differentiable forward-chaining using first-order logic. The key idea is to combine differentiable forward-chaining reasoning with object-centric (deep) learning. Differentiable forward-chaining reasoning computes logical entailments smoothly, i.e., it deduces new facts from given facts and rules in a differentiable manner. The object-centric learning approach factorizes raw inputs into representations in terms of objects. Thus, it allows us to provide a consistent framework to perform the forward-chaining inference from raw inputs. NSFR factorizes the raw inputs into the object-centric representations, converts them into probabilistic ground atoms, and finally performs differentiable forward-chaining inference using weighted rules for inference. Our comprehensive experimental evaluations on object-centric reasoning data sets, 2D Kandinsky patterns and 3D CLEVR-Hans, and a variety of tasks show the effectiveness and advantage of our approach.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  note = {Comment: Preprint},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\BY6M9LGN\\Shindo et al. - 2021 - Neuro-Symbolic Forward Reasoning.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\ATJKBR3J\\2110.html}
}

@inproceedings{shivhareEmotionFinderDetectingEmotion2015,
  title = {{{EmotionFinder}}: {{Detecting}} Emotion from Blogs and Textual Documents},
  shorttitle = {{{EmotionFinder}}},
  booktitle = {Communication {{Automation International Conference}} on {{Computing}}},
  author = {Shivhare, Shiv Naresh and Garg, Shakun and Mishra, Anitesh},
  year = {2015},
  month = may,
  pages = {52--57},
  doi = {10.1109/CCAA.2015.7148343},
  abstract = {Emotion Detection is one of the most emerging issues in human machine interaction. Detecting emotional state of a person from textual data is an active research field along with recognizing emotions from facial and audio information. Several methods were given to recognize emotion from text in previous years. This paper proposed a new architecture (a keyword based approach) to recognize emotions from text. In case of recognizing emotion from a piece of text document or a blog, any human can do this better than a machine only problem is he/she takes time. Proposed emotion detector system takes a text document and the emotion word ontology as inputs and produces one of the six emotion classes (i.e. love, sadness, joy, fear and surprise, anger) as the output. Every input text contains some short stories which are firstly read and assigned an emotion class manually and then that emotion class is compared to the output of the proposed system to check the accuracy of the Proposed Emotion Detector System. It is found that the Proposed Emotion Detector System produces output with the accuracy of more than 75\%.},
  keywords = {Accuracy,Blogs,Detectors,Emotion recognition,Emotion Word Ontology,Human-Computer Interaction,Manuals,Ontologies,Textual Emotion Recognition,XML},
  note = {Emotion Detection from Text, a combination of keyword spotting technique and an emotion ontology.},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\8VMZZTHB\\Shivhare et al. - 2015 - EmotionFinder Detecting emotion from blogs and te.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\QK9VHVCH\\7148343.html}
}

@article{shojaeilangariRobustRepresentationRecognition2015,
  title = {Robust {{Representation}} and {{Recognition}} of {{Facial Emotions Using Extreme Sparse Learning}}},
  author = {Shojaeilangari, Seyedehsamaneh and Yau, Wei-Yun and Nandakumar, Karthik and Li, Jun and Teoh, Eam Khwang},
  year = {2015},
  month = jul,
  journal = {IEEE Transactions on Image Processing},
  volume = {24},
  number = {7},
  pages = {2140--2152},
  issn = {1941-0042},
  doi = {10.1109/TIP.2015.2416634},
  abstract = {Recognition of natural emotions from human faces is an interesting topic with a wide range of potential applications, such as human-computer interaction, automated tutoring systems, image and video retrieval, smart environments, and driver warning systems. Traditionally, facial emotion recognition systems have been evaluated on laboratory controlled data, which is not representative of the environment faced in real-world applications. To robustly recognize the facial emotions in real-world natural situations, this paper proposes an approach called extreme sparse learning, which has the ability to jointly learn a dictionary (set of basis) and a nonlinear classification model. The proposed approach combines the discriminative power of extreme learning machine with the reconstruction property of sparse representation to enable accurate classification when presented with noisy signals and imperfect data recorded in natural settings. In addition, this paper presents a new local spatio-temporal descriptor that is distinctive and pose-invariant. The proposed framework is able to achieve the state-of-the-art recognition accuracy on both acted and spontaneous facial emotion databases.},
  keywords = {Dictionaries,dictionary learning,Dictionary learning,Emotion recognition,extreme learning machine,Extreme learning machine,extreme sparse learning,Extreme sparse learning,facial emotion,Facial emotion,Feature extraction,Head,Image reconstruction,pose-invariance,Pose-invariance,Robustness,sparse representation,Sparse representation,Vectors},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\PRNLKYFX\\Shojaeilangari et al. - 2015 - Robust Representation and Recognition of Facial Em.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\XTN85HHI\\7067419.html}
}

@article{shoumyMultimodalBigData2020,
  title = {Multimodal Big Data Affective Analytics: {{A}} Comprehensive Survey Using Text, Audio, Visual and Physiological Signals},
  author = {Shoumy, Nusrat J. and Ang, Li-Minn and Seng, Kah Phooi and Rahaman, DM Motiur and Zia, Tanveer},
  year = {2020},
  journal = {Journal of Network and Computer Applications},
  volume = {149},
  pages = {102447},
  issn = {1084-8045},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\KTCADJ7K\\Multimodal big data affective analytics.pdf}
}

@inproceedings{shoushtariEmotionRecognitionUsing2021,
  title = {Emotion {{Recognition Using Sparse Graph Analysis}} of {{Brain Connectivity}}},
  booktitle = {2021 28th {{National}} and 6th {{International Iranian Conference}} on {{Biomedical Engineering}} ({{ICBME}})},
  author = {Shoushtari, Shirin and Mohammadzadeh, Hoda and Amini, Arash},
  year = {2021},
  month = nov,
  pages = {182--187},
  doi = {10.1109/ICBME54433.2021.9750382},
  abstract = {Emotion recognition has gained more importance in recent years due to its various applications in artificial intelligence (AI). Because of high temporal resolution and low acquisition costs, EEG signals have become one of the dominant brain signals for the analysis and recognition of the emotions induced by the nervous system. In this study, we aim to explain brain connectivity using graph models and assess the performance of graph features extracted from brain connectivity. We propose two models to build graphs for brain connectivity and compare their capabilities in expressing the emotional state of a subject. We have used the DEAP dataset for this experiment. Our proposed models show an accuracy increase of approximately 5\% compared to solely using brain connectivity features. Another advantage of this approach is that by making the connectivity graph sparser, we could considerably reduce the size of the feature vector compared to the conventional brain connectivity feature vector.},
  keywords = {Artificial intelligence,Brain connectivity,Brain modeling,Computational efficiency,Costs,DEAP,EEG signals,Electroencephalography,Emotion recognition,Feature extraction,Graph analysis},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\KBLJTB5D\\Shoushtari et al. - 2021 - Emotion Recognition Using Sparse Graph Analysis of.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\IRIKSLTP\\9750382.html}
}

@article{simonyanVeryDeepConvolutional2015,
  title = {Very {{Deep Convolutional Networks}} for {{Large-Scale Image Recognition}}},
  author = {Simonyan, Karen and Zisserman, Andrew},
  year = {2015},
  month = apr,
  journal = {arXiv:1409.1556 [cs]},
  eprint = {1409.1556},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\KCQ7CZ43\\Simonyan und Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale I.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\7L2BPTYS\\1409.html}
}

@misc{SmarteEinkaufswagenVerbessern,
  title = {{Smarte Einkaufswagen verbessern die Customer Journey | Wanzl}},
  abstract = {Digitale Pfandschl\"osser k\"onnen weit mehr als nur Einkaufswagen entsperren},
  howpublished = {https://www.wanzl.com/de\_DE/wanzl-inside/presse-und-news/smarte-einkaufswagen-verbessern-die-customer-journey\textasciitilde n5505},
  langid = {german},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\XXK5Y53X\\smarte-einkaufswagen-verbessern-die-customer-journey~n5505.html}
}

@article{srihariExplainableArtificialIntelligence2020,
  title = {Explainable {{Artificial Intelligence}}: {{An Overview}}},
  shorttitle = {Explainable {{Artificial Intelligence}}},
  author = {Srihari, S.},
  year = {2020},
  journal = {J. Wash. Acad. Sci},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\MMHA4DVF\\Srihari - Explainable Artificial Intelligence An Overview.pdf}
}

@article{stappenMultimodalSentimentAnalysis2021,
  title = {The {{Multimodal Sentiment Analysis}} in {{Car Reviews}} ({{MuSe-CaR}}) {{Dataset}}: {{Collection}}, {{Insights}} and {{Improvements}}},
  author = {Stappen, Lukas and Baird, Alice and Schumann, Lea and Schuller, Bj{\"o}rn},
  year = {2021},
  journal = {arXiv preprint arXiv:2101.06053},
  eprint = {2101.06053},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\Y33QPVT7\\Multimodal Sentiment Analysis.pdf}
}

@article{stappenMuSe2020First2020,
  title = {{{MuSe}} 2020--{{The First International Multimodal Sentiment Analysis}} in {{Real-life Media Challenge}} and {{Workshop}}},
  author = {Stappen, Lukas and Baird, Alice and Rizos, Georgios and Tzirakis, Panagiotis and Du, Xinchen and Hafner, Felix and Schumann, Lea and {Mallol-Ragolta}, Adria and Schuller, Bj{\"o}rn W. and Lefter, Iulia},
  year = {2020},
  journal = {arXiv preprint arXiv:2004.14858},
  eprint = {2004.14858},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\DBRWIPJI\\First International Multimodal Sentiment Analy.pdf}
}

@inproceedings{suhPartalignedBilinearRepresentations2018,
  title = {Part-Aligned Bilinear Representations for Person Re-Identification},
  booktitle = {Proceedings of the {{European}} Conference on Computer Vision ({{ECCV}})},
  author = {Suh, Yumin and Wang, Jingdong and Tang, Siyu and Mei, Tao and Lee, Kyoung Mu},
  year = {2018},
  pages = {402--419},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\YT5ZS25Q\\Suh et al. - 2018 - Part-aligned bilinear representations for person r.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\WF5AKX62\\Yumin_Suh_Part-Aligned_Bilinear_Representations_ECCV_2018_paper.html}
}

@article{sunInterpretingDeepLearning2021,
  title = {Interpreting {{Deep Learning Models}} in {{Natural Language Processing}}: {{A Review}}},
  shorttitle = {Interpreting {{Deep Learning Models}} in {{Natural Language Processing}}},
  author = {Sun, Xiaofei and Yang, Diyi and Li, Xiaoya and Zhang, Tianwei and Meng, Yuxian and Qiu, Han and Wang, Guoyin and Hovy, Eduard and Li, Jiwei},
  year = {2021},
  month = oct,
  journal = {arXiv:2110.10470 [cs]},
  eprint = {2110.10470},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Neural network models have achieved state-of-the-art performances in a wide range of natural language processing (NLP) tasks. However, a long-standing criticism against neural network models is the lack of interpretability, which not only reduces the reliability of neural NLP systems but also limits the scope of their applications in areas where interpretability is essential (e.g., health care applications). In response, the increasing interest in interpreting neural NLP models has spurred a diverse array of interpretation methods over recent years. In this survey, we provide a comprehensive review of various interpretation methods for neural models in NLP. We first stretch out a high-level taxonomy for interpretation methods in NLP, i.e., training-based approaches, test-based approaches, and hybrid approaches. Next, we describe sub-categories in each category in detail, e.g., influence-function based methods, KNN-based methods, attention-based models, saliency-based methods, perturbation-based methods, etc. We point out deficiencies of current methods and suggest some avenues for future research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\AC5SY2JG\\Sun et al. - 2021 - Interpreting Deep Learning Models in Natural Langu.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\5MN2AQV9\\2110.html}
}

@inproceedings{sunSimpleRuleExtraction2000,
  title = {Beyond Simple Rule Extraction: The Extraction of Planning Knowledge from Reinforcement Learners},
  shorttitle = {Beyond Simple Rule Extraction},
  booktitle = {Proceedings of the {{IEEE-INNS-ENNS International Joint Conference}} on {{Neural Networks}}. {{IJCNN}} 2000. {{Neural Computing}}: {{New Challenges}} and {{Perspectives}} for the {{New Millennium}}},
  author = {Sun, R.},
  year = {2000},
  month = jul,
  volume = {2},
  pages = {105-110 vol.2},
  issn = {1098-7576},
  doi = {10.1109/IJCNN.2000.857882},
  abstract = {This paper discusses learning in hybrid models that goes beyond simple rule extraction from backpropagation networks. Although simple rule extraction has received a lot of research attention, to further develop hybrid learning models that include both symbolic and subsymbolic knowledge and that learn autonomously, it is necessary to study autonomous learning of both subsymbolic and symbolic knowledge in integrated architectures. This paper describes knowledge extraction from neural reinforcement learning. It includes two approaches towards extracting plan knowledge: the extraction of explicit, symbolic rules from neural reinforcement learning, and the extraction of complete plans. This work points to the creation of a general framework for achieving the subsymbolic to symbolic transition in an integrated autonomous learning framework.},
  keywords = {Backpropagation,Boltzmann distribution,Collaborative work,Learning,State estimation,Stochastic processes,Sun,Usability},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\K3532AI9\\Sun - 2000 - Beyond simple rule extraction the extraction of p.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\HBW64H9A\\857882.html}
}

@misc{SusskindFacialExpression,
  title = {Susskind: {{Facial}} Expression Form and Function - {{Google Scholar}}},
  howpublished = {https://scholar.google.com/scholar\_lookup?title=\&journal=Commun.\%20Integrat.\%20Biol.\&volume=1\&pages=148-149\&publication\_year=2008\&author=Susskind\%2CJ.\%20M.\&author=Anderson\%2CA.\%20K.},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\2Y6H54W2\\Susskind Facial expression form and function - Go.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\H4HEWA84\\scholar_lookup.html}
}

@article{susskindFacialExpressionForm2008,
  title = {Facial Expression Form and Function},
  author = {Susskind, Joshua M. and Anderson, Adam K.},
  year = {2008},
  month = oct,
  journal = {Communicative \& Integrative Biology},
  volume = {1},
  number = {2},
  pages = {148--149},
  publisher = {{Taylor \& Francis}},
  issn = {null},
  doi = {10.4161/cib.1.2.6999},
  abstract = {From an evolutionary perspective, facial expressions would serve adaptive functions that promote genetic fitness. While many ideas have been proposed,1 the specific adaptive functions of expressing emotion on the face have largely remained untested since Darwin proposed a set of expressive functional principles over 130 years ago.2 Recently, we showed that expressions of fear and disgust alter the biomechanical properties of the face, such that fear increases while disgust decreases sensory exposure.3 Additional vector flow analyses presented here reveal that anger and surprise expressions may similarly be shaped by sensory contraction and expansion action tendencies. An examination of the temporal dynamics of sensory modulation may reveal a general principle shaping expressive form rather than a specific adaptation shaping fear and disgust. Furthermore, if sensory modulation is a general principle, this function should be present across species rather than only in humans. Although facial morphology differs across species, detailed examination of sensory intake in different species may reveal the origins of facial expressions inherited by humans.},
  pmid = {19704875},
  annotation = {\_eprint: https://doi.org/10.4161/cib.1.2.6999},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\32FQ9F5S\\Susskind und Anderson - 2008 - Facial expression form and function.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\QGP6ND7P\\cib.1.2.html}
}

@article{susskindNeuroSymbolicAIEmerging2021,
  title = {Neuro-{{Symbolic AI}}: {{An Emerging Class}} of {{AI Workloads}} and Their {{Characterization}}},
  shorttitle = {Neuro-{{Symbolic AI}}},
  author = {Susskind, Zachary and Arden, Bryce and John, Lizy K. and Stockton, Patrick and John, Eugene B.},
  year = {2021},
  month = sep,
  journal = {arXiv:2109.06133 [cs]},
  eprint = {2109.06133},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Neuro-symbolic artificial intelligence is a novel area of AI research which seeks to combine traditional rules-based AI approaches with modern deep learning techniques. Neuro-symbolic models have already demonstrated the capability to outperform state-of-the-art deep learning models in domains such as image and video reasoning. They have also been shown to obtain high accuracy with significantly less training data than traditional models. Due to the recency of the field's emergence and relative sparsity of published results, the performance characteristics of these models are not well understood. In this paper, we describe and analyze the performance characteristics of three recent neuro-symbolic models. We find that symbolic models have less potential parallelism than traditional neural models due to complex control flow and low-operational-intensity operations, such as scalar multiplication and tensor addition. However, the neural aspect of computation dominates the symbolic part in cases where they are clearly separable. We also find that data movement poses a potential bottleneck, as it does in many ML workloads.},
  archiveprefix = {arXiv},
  keywords = {C.4,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Performance,I.2.m},
  note = {Comment: 11 pages, 7 figures
\par
Comment: 11 pages, 7 figures},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\52C6SANF\\Susskind et al. - 2021 - Neuro-Symbolic AI An Emerging Class of AI Workloa.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\B6UE9436\\2109.html}
}

@article{suUnsupervisedCrossCorpusSpeech2022,
  title = {Unsupervised {{Cross-Corpus Speech Emotion Recognition Using}} a {{Multi-Source Cycle-GAN}}},
  author = {Su, Bo-Hao and Lee, Chi-Chun},
  year = {2022},
  journal = {IEEE Transactions on Affective Computing},
  pages = {1--1},
  issn = {1949-3045},
  doi = {10.1109/TAFFC.2022.3146325},
  abstract = {Speech emotion recognition (SER) plays a crucial role in understanding user feelings when developing artificial intelligence services. However, the data mismatch and label distortion between the training (source) set and the testing (target) set significantly degrade the performances when developing the SER systems. Additionally, most emotion-related speech datasets are highly contextualized and limited in size. The manual annotation cost is often too high leading to an active investigation of unsupervised cross-corpus SER techniques. In this paper, we propose a framework in unsupervised cross-corpus emotion recognition using multi-source corpus in a data augmentation manner. We introduced Corpus-Aware Emotional CycleGAN (CAEmoCyGAN) including a corpus-aware attention mechanism to aggregate each source datasets to generate the synthetic target sample. We choose the widely used speech emotion corpora the IEMOCAP and the VAM as sources and the MSP-Podcast as the target. By generating synthetic target-aware samples to augment source datasets and by directly training on this augmented dataset, our proposed multi-source target-aware augmentation method outperforms other baseline models in activation and valence classification.},
  keywords = {Adaptation models,cross corpus,data augmentation,Data models,Databases,Emotion recognition,multi-sources attention,Speech emotion recognition,Target recognition,Task analysis,Training,unsupervised learning},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\7TAL4R2L\\Su und Lee - 2022 - Unsupervised Cross-Corpus Speech Emotion Recogniti.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\MVCHIM5D\\9695273.html}
}

@incollection{swanBlockchainBusinessNextgeneration2018,
  title = {Blockchain for Business: {{Next-generation}} Enterprise Artificial Intelligence Systems},
  shorttitle = {Blockchain for Business},
  booktitle = {Advances in Computers},
  author = {Swan, Melanie},
  year = {2018},
  volume = {111},
  pages = {121--162},
  publisher = {{Elsevier}},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\NF9QC3LD\\S0065245818300287.html}
}

@inproceedings{szegedyGoingDeeperConvolutions2015a,
  title = {Going Deeper with Convolutions},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  year = {2015},
  pages = {1--9},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\UK4HFBX3\\Szegedy et al. - 2015 - Going deeper with convolutions.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\ZTXTADMV\\Szegedy_Going_Deeper_With_2015_CVPR_paper.html}
}

@article{tamkinUnderstandingCapabilitiesLimitations,
  title = {Understanding the {{Capabilities}}, {{Limitations}}, and {{Societal Impact}} of {{Large Language Models}}},
  author = {Tamkin, Alex and Brundage, Miles and Clark, Jack and Ganguli, Deep},
  pages = {8},
  langid = {english},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\EZ6MH9NA\\Tamkin et al. - Understanding the Capabilities, Limitations, and S.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\PD3KLMQD\\2102.html}
}

@inproceedings{tanakaPBLEducationSupport2022,
  title = {{{PBL Education Support System}} to {{Visualize Students}}' {{Attitudes}}},
  booktitle = {2022 {{IEEE}} 4th {{Global Conference}} on {{Life Sciences}} and {{Technologies}} ({{LifeTech}})},
  author = {Tanaka, Mikiko Sode and Miyazaki, Keisuke and Shin, Masako and Ito, Takao},
  year = {2022},
  month = mar,
  pages = {196--197},
  doi = {10.1109/LifeTech53646.2022.9754787},
  abstract = {We have built a system that supports PBL education called AI teachers. In this paper, we will introduce the mechanism and effect of group activities supported by AI teachers. In particular, we will explain the advice function for students using an emotion analysis. The problem of group activity was that when the students were divided into multiple groups and did group work in breakout rooms, the teacher could not grasp the situation of the students. Teachers can check by entering the breakout room of each group in turn, but teachers cannot see the whole at once. AI teachers have the ability to leave the voice of group activities as text. By utilizing this function, teachers can now give appropriate advice to each group. However, it takes some time to understand the student situation from the texts. Therefore, we added the emotion analysis to this function to improve the understanding the situation so that teachers can visually grasp the student's situation. In this paper we reports the details of this function and the results of its use.},
  keywords = {AI,Artificial intelligence,Conferences,Education,Emotional analysis,Grasping,Group activity,Life sciences,PBL,Speech recognition,Visualization},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\VI5L8QIM\\Tanaka et al. - 2022 - PBL Education Support System to Visualize Students.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\PE4B32X6\\9754787.html}
}

@article{tarnowskiEyetrackingAnalysisEmotion2020,
  title = {Eye-Tracking Analysis for Emotion Recognition},
  author = {Tarnowski, Pawe{\l} and Ko{\l}odziej, Marcin and Majkowski, Andrzej and Rak, Remigiusz Jan},
  year = {2020},
  journal = {Computational Intelligence and Neuroscience},
  volume = {2020},
  publisher = {{Hindawi Limited}},
  address = {{United Kingdom}},
  issn = {1687-5273},
  doi = {10.1155/2020/2909267},
  abstract = {This article reports the results of the study related to emotion recognition by using eye-tracking. Emotions were evoked by presenting a dynamic movie material in the form of 21 video fragments. Eye-tracking signals recorded from 30 participants were used to calculate 18 features associated with eye movements (fixations and saccades) and pupil diameter. To ensure that the features were related to emotions, we investigated the influence of luminance and the dynamics of the presented movies. Three classes of emotions were considered: high arousal and low valence, low arousal and moderate valence, and high arousal and high valence. A maximum of 80\% classification accuracy was obtained using the support vector machine (SVM) classifier and leaveone- subject-out validation method. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  keywords = {Affective Valence,Emotion Recognition,Eye Fixation,Eye Movements,Physiological Arousal,Visual Tracking},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\PC37C8R9\\Tarnowski et al. - 2020 - Eye-tracking analysis for emotion recognition.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\8KBQXVAF\\2020-68547-001.html}
}

@inproceedings{tatenoCNNSLAMRealTimeDense2017,
  title = {{{CNN-SLAM}}: {{Real-Time Dense Monocular SLAM}} with {{Learned Depth Prediction}}},
  shorttitle = {{{CNN-SLAM}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Tateno, Keisuke and Tombari, Federico and Laina, Iro and Navab, Nassir},
  year = {2017},
  month = jul,
  pages = {6565--6574},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2017.695},
  abstract = {Given the recent advances in depth prediction from Convolutional Neural Networks (CNNs), this paper investigates how predicted depth maps from a deep neural network can be deployed for the goal of accurate and dense monocular reconstruction. We propose a method where CNN-predicted dense depth maps are naturally fused together with depth measurements obtained from direct monocular SLAM, based on a scheme that privileges depth prediction in image locations where monocular SLAM approaches tend to fail, e.g. along low-textured regions, and vice-versa. We demonstrate the use of depth prediction to estimate the absolute scale of the reconstruction, hence overcoming one of the major limitations of monocular SLAM. Finally, we propose a framework to efficiently fuse semantic labels, obtained from a single frame, with dense SLAM, so to yield semantically coherent scene reconstruction from a single view. Evaluation results on two benchmark datasets show the robustness and accuracy of our approach.},
  keywords = {Cameras,Image reconstruction,Pose estimation,Semantics,Simultaneous localization and mapping,Three-dimensional displays},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\BWZ679QM\\Tateno et al. - 2017 - CNN-SLAM Real-Time Dense Monocular SLAM with Lear.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\GMVXSGQY\\8100178.html}
}

@article{teyeEvaluationConversationalAgents2022,
  title = {Evaluation of {{Conversational Agents}}: {{Understanding Culture}}, {{Context}} and {{Environment}} in {{Emotion Detection}}},
  shorttitle = {Evaluation of {{Conversational Agents}}},
  author = {Teye, Martha T. and Missah, Yaw Marfo and Ahene, Emmanuel and Frimpong, Twum},
  year = {2022},
  journal = {IEEE Access},
  volume = {10},
  pages = {24976--24984},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2022.3153787},
  abstract = {Valuable decisions and highly prioritized analysis now depend on applications such as facial biometrics, social media photo tagging, and human robots interactions. However, the ability to successfully deploy such applications is based on their efficiencies on tested use cases taking into consideration possible edge cases. Over the years, lots of generalized solutions have been implemented to mimic human emotions including sarcasm. However, factors such as geographical location or cultural difference have not been explored fully amidst its relevance in resolving ethical issues and improving conversational AI (Artificial Intelligence). In this paper, we seek to address the potential challenges in the usage of conversational AI within Black African society. We develop an emotion prediction model with accuracies ranging between 85\% and 96\%. Our model combines both speech and image data to detect the seven basic emotions with a focus on also identifying sarcasm. It uses 3-layers of the Convolutional Neural Network in addition to a new Audio-Frame Mean Expression (AFME) algorithm and focuses on model pre-processing and post-processing stages. In the end, our proposed solution contributes to maintaining the credibility of an emotion recognition system in conversational AIs.},
  keywords = {AI ethics,Biological system modeling,convolutional neural network,Convolutional neural networks,Data models,emotion recognition,Emotion recognition,Face recognition,human-AI interaction,Social networking (online),speech recognition,Speech recognition},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\U6VQ8KKL\\Teye et al. - 2022 - Evaluation of Conversational Agents Understanding.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\FYKSXRBH\\9718284.html}
}

@inproceedings{tianEliminatingBackgroundbiasRobust2018,
  title = {Eliminating Background-Bias for Robust Person Re-Identification},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Tian, Maoqing and Yi, Shuai and Li, Hongsheng and Li, Shihua and Zhang, Xuesen and Shi, Jianping and Yan, Junjie and Wang, Xiaogang},
  year = {2018},
  pages = {5794--5803},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\3Y9SZS54\\Tian et al. - 2018 - Eliminating background-bias for robust person re-i.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\3XXY7J9V\\Tian_Eliminating_Background-Bias_for_CVPR_2018_paper.html}
}

@article{tianRecognizingActionUnits2001,
  title = {Recognizing Action Units for Facial Expression Analysis},
  author = {Tian, Y.-I. and Kanade, Takeo and Cohn, Jeffrey F.},
  year = {2001},
  journal = {IEEE Transactions on pattern analysis and machine intelligence},
  volume = {23},
  number = {2},
  pages = {97--115},
  issn = {0162-8828}
}

@article{tolstikhinMLPMixerAllMLPArchitecture2021,
  title = {{{MLP-Mixer}}: {{An}} All-{{MLP Architecture}} for {{Vision}}},
  shorttitle = {{{MLP-Mixer}}},
  author = {Tolstikhin, Ilya and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Steiner, Andreas and Keysers, Daniel and Uszkoreit, Jakob and Lucic, Mario and Dosovitskiy, Alexey},
  year = {2021},
  month = jun,
  journal = {arXiv:2105.01601 [cs]},
  eprint = {2105.01601},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Convolutional Neural Networks (CNNs) are the go-to model for computer vision. Recently, attention-based networks, such as the Vision Transformer, have also become popular. In this paper we show that while convolutions and attention are both sufficient for good performance, neither of them are necessary. We present MLP-Mixer, an architecture based exclusively on multi-layer perceptrons (MLPs). MLP-Mixer contains two types of layers: one with MLPs applied independently to image patches (i.e. "mixing" the per-location features), and one with MLPs applied across patches (i.e. "mixing" spatial information). When trained on large datasets, or with modern regularization schemes, MLP-Mixer attains competitive scores on image classification benchmarks, with pre-training and inference cost comparable to state-of-the-art models. We hope that these results spark further research beyond the realms of well established CNNs and Transformers.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  note = {Comment: v2: Fixed parameter counts in Table 1. v3: Added results on JFT-3B in Figure 2(right); Added Section 3.4 on the input permutations. v4: Updated the x label in Figure 2(right)},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\CA6AWZQL\\Tolstikhin et al. - 2021 - MLP-Mixer An all-MLP Architecture for Vision.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\QNY42P2G\\2105.html}
}

@inproceedings{touzetReinforcementFunctionDesign1998,
  title = {Reinforcement Function Design and Bias for Efficient Learning in Mobile Robots},
  booktitle = {1998 {{IEEE International Conference}} on {{Fuzzy Systems Proceedings}}. {{IEEE World Congress}} on {{Computational Intelligence}} ({{Cat}}. {{No}}.{{98CH36228}})},
  author = {Touzet, C. and Santos, J.M.},
  year = {1998},
  month = may,
  volume = {1},
  pages = {153-158 vol.1},
  issn = {1098-7584},
  doi = {10.1109/FUZZY.1998.687475},
  abstract = {The main paradigm in sub-symbolic learning robot domain is the reinforcement learning method. Various techniques have been developed to deal with the memorization/generalization problem, demonstrating the superior ability of artificial neural network implementations. In this paper, we address the issue of designing the reinforcement so as to optimize the exploration part of the learning. We also present and summarize works relative to the use of bias intended to achieve the effective synthesis of the desired behavior. Demonstrative experiments involving a self-organizing map implementation of the Q-learning and real mobile robots (Nomad 200 and Khepera) in a task of obstacle avoidance behavior synthesis are described.},
  keywords = {Artificial neural networks,Computer networks,Design optimization,Human robot interaction,Laboratories,Learning,Mobile robots,Orbital robotics,Robot sensing systems,Signal synthesis},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\FCSWH7BX\\Touzet und Santos - 1998 - Reinforcement function design and bias for efficie.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\3P3HIXLW\\687475.html}
}

@article{trockmanPatchesAreAll2022,
  title = {Patches {{Are All You Need}}?},
  author = {Trockman, Asher and Kolter, J. Zico},
  year = {2022},
  month = jan,
  journal = {arXiv:2201.09792 [cs]},
  eprint = {2201.09792},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Although convolutional networks have been the dominant architecture for vision tasks for many years, recent experiments have shown that Transformer-based models, most notably the Vision Transformer (ViT), may exceed their performance in some settings. However, due to the quadratic runtime of the self-attention layers in Transformers, ViTs require the use of patch embeddings, which group together small regions of the image into single input features, in order to be applied to larger image sizes. This raises a question: Is the performance of ViTs due to the inherently-more-powerful Transformer architecture, or is it at least partly due to using patches as the input representation? In this paper, we present some evidence for the latter: specifically, we propose the ConvMixer, an extremely simple model that is similar in spirit to the ViT and the even-more-basic MLP-Mixer in that it operates directly on patches as input, separates the mixing of spatial and channel dimensions, and maintains equal size and resolution throughout the network. In contrast, however, the ConvMixer uses only standard convolutions to achieve the mixing steps. Despite its simplicity, we show that the ConvMixer outperforms the ViT, MLP-Mixer, and some of their variants for similar parameter counts and data set sizes, in addition to outperforming classical vision models such as the ResNet. Our code is available at https://github.com/locuslab/convmixer.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\IXUSU3RY\\Trockman und Kolter - 2022 - Patches Are All You Need.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\C2AN4LTD\\2201.html}
}

@inproceedings{ultschIntegrationConnectionistModels1998,
  title = {The Integration of Connectionist Models with Knowledge-Based Systems: Hybrid Systems},
  shorttitle = {The Integration of Connectionist Models with Knowledge-Based Systems},
  booktitle = {{{SMC}}'98 {{Conference Proceedings}}. 1998 {{IEEE International Conference}} on {{Systems}}, {{Man}}, and {{Cybernetics}} ({{Cat}}. {{No}}.{{98CH36218}})},
  author = {Ultsch, A.},
  year = {1998},
  month = oct,
  volume = {2},
  pages = {1530-1535 vol.2},
  issn = {1062-922X},
  doi = {10.1109/ICSMC.1998.728103},
  abstract = {The main difference between connectionist models and technologies of symbolic artificial intelligence is the form in which knowledge is represented i.e. symbolic vs. subsymbolic methods are used to represent knowledge. The paper clarifies and emphasizes this paradigmatic difference, in particular with respect to the so called hybrid systems. The terms symbolic and subsymbolic knowledge representation are specified. Furthermore, the central constituents of a subsymbolic representation are named. Necessary requirements for connectionist models are clarified. As hybrid systems in a strict sense those systems are identified that realize a conversion between subsymbolic and symbolic knowledge.},
  keywords = {Artificial intelligence,Computer errors,Dictionaries,Knowledge based systems,Knowledge representation,Neurons,Numerical models,Power system modeling,Robustness,Stress},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\SRLBYSEM\\Ultsch - 1998 - The integration of connectionist models with knowl.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\LQ56L3KQ\\728103.html}
}

@article{vanschorenMetalearningSurvey2018,
  title = {Meta-Learning: {{A}} Survey},
  shorttitle = {Meta-Learning},
  author = {Vanschoren, Joaquin},
  year = {2018},
  journal = {arXiv preprint arXiv:1810.03548},
  eprint = {1810.03548},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\ZM8LI3H6\\Vanschoren - 2018 - Meta-learning A survey.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\Z2XKEPHB\\1810.html}
}

@article{vaswaniAttentionAllYou2017a,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  month = dec,
  journal = {arXiv:1706.03762 [cs]},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  note = {Comment: 15 pages, 5 figures},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\DPUF7FTU\\Vaswani et al. - 2017 - Attention Is All You Need.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\LJ2RPC5S\\1706.html}
}

@article{villaSequentialPatentSearch2022,
  title = {A Sequential Patent Search Approach Combining Semantics and Artificial Intelligence to Identify Initial {{State-of-the-Art}} Documents},
  author = {Villa, Anna Maria and Wirz, Manuel},
  year = {2022},
  journal = {World Patent Information},
  volume = {68},
  pages = {102096},
  publisher = {{Elsevier}},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\7ZBV5LU7\\Villa und Wirz - 2022 - A sequential patent search approach combining sema.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\GANGH6FR\\S0172219022000035.html}
}

@article{vonruedenInformedMachineLearning2021,
  title = {Informed {{Machine Learning}} - {{A Taxonomy}} and {{Survey}} of {{Integrating Prior Knowledge}} into {{Learning Systems}}},
  author = {{von Rueden}, Laura and Mayer, Sebastian and Beckh, Katharina and Georgiev, Bogdan and Giesselbach, Sven and Heese, Raoul and Kirsch, Birgit and Walczak, Michal and Pfrommer, Julius and Pick, Annika and Ramamurthy, Rajkumar and Garcke, Jochen and Bauckhage, Christian and Schuecker, Jannis},
  year = {2021},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  pages = {1--1},
  issn = {1558-2191},
  doi = {10.1109/TKDE.2021.3079836},
  abstract = {Despite its great success, machine learning can have its limits when dealing with insufficient training data. A potential solution is the additional integration of prior knowledge into the training process which leads to the notion of informed machine learning. In this paper, we present a structured overview of various approaches in this field. We provide a definition and propose a concept for informed machine learning which illustrates its building blocks and distinguishes it from conventional machine learning. We introduce a taxonomy that serves as a classification framework for informed machine learning approaches. It considers the source of knowledge, its representation, and its integration into the machine learning pipeline. Based on this taxonomy, we survey related research and describe how different knowledge representations such as algebraic equations, logic rules, or simulation results can be used in learning systems. This evaluation of numerous papers on the basis of our taxonomy uncovers key methods in the field of informed machine learning.},
  keywords = {Expert Knowledge,Hybrid,Informed,Machine learning,Machine Learning,Mathematical model,Neuro-Symbolic,Pipelines,Prior Knowledge,Survey,Systematics,Taxonomy,Training,Training data},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\IRCM6SPJ\\von Rueden et al. - 2021 - Informed Machine Learning - A Taxonomy and Survey .pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\2KS9EYCT\\9429985.html}
}

@article{wangKnowledgeDistillationStudentTeacher2021,
  title = {Knowledge {{Distillation}} and {{Student-Teacher Learning}} for {{Visual Intelligence}}: {{A Review}} and {{New Outlooks}}},
  shorttitle = {Knowledge {{Distillation}} and {{Student-Teacher Learning}} for {{Visual Intelligence}}},
  author = {Wang, Lin and Yoon, Kuk-Jin},
  year = {2021},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  eprint = {2004.05937},
  eprinttype = {arxiv},
  pages = {1--1},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2021.3055564},
  abstract = {Deep neural models in recent years have been successful in almost every field, including extremely complex problem statements. However, these models are huge in size, with millions (and even billions) of parameters, thus demanding more heavy computation power and failing to be deployed on edge devices. Besides, the performance boost is highly dependent on redundant labeled data. To achieve faster speeds and to handle the problems caused by the lack of data, knowledge distillation (KD) has been proposed to transfer information learned from one model to another. KD is often characterized by the so-called `Student-Teacher' (S-T) learning framework and has been broadly applied in model compression and knowledge transfer. This paper is about KD and S-T learning, which are being actively studied in recent years. First, we aim to provide explanations of what KD is and how/why it works. Then, we provide a comprehensive survey on the recent progress of KD methods together with S-T frameworks typically for vision tasks. In general, we consider some fundamental questions that have been driving this research area and thoroughly generalize the research progress and technical details. Additionally, we systematically analyze the research status of KD in vision applications. Finally, we discuss the potentials and open challenges of existing methods and prospect the future directions of KD and S-T learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  note = {Comment: Accepted to IEEE Transactions on Pattern Analysis and Machine Intelligence(TPAMI),2021. Some references are updated in this version},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\CFQ2W8JH\\Wang und Yoon - 2021 - Knowledge Distillation and Student-Teacher Learnin.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\AKCS6HHK\\2004.html}
}

@article{wangLinformerSelfAttentionLinear2020,
  title = {Linformer: {{Self-Attention}} with {{Linear Complexity}}},
  shorttitle = {Linformer},
  author = {Wang, Sinong and Li, Belinda Z. and Khabsa, Madian and Fang, Han and Ma, Hao},
  year = {2020},
  month = jun,
  journal = {arXiv:2006.04768 [cs, stat]},
  eprint = {2006.04768},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Large transformer models have shown extraordinary success in achieving state-of-the-art results in many natural language processing applications. However, training and deploying these models can be prohibitively costly for long sequences, as the standard self-attention mechanism of the Transformer uses \$O(n\^2)\$ time and space with respect to sequence length. In this paper, we demonstrate that the self-attention mechanism can be approximated by a low-rank matrix. We further exploit this finding to propose a new self-attention mechanism, which reduces the overall self-attention complexity from \$O(n\^2)\$ to \$O(n)\$ in both time and space. The resulting linear transformer, the \textbackslash textit\{Linformer\}, performs on par with standard Transformer models, while being much more memory- and time-efficient.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\G3WQ2S6T\\Wang et al. - 2020 - Linformer Self-Attention with Linear Complexity.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\HHE6MUJY\\2006.html}
}

@inproceedings{wangPyramidSpatialTemporalAggregation2021,
  title = {Pyramid {{Spatial-Temporal Aggregation}} for {{Video-based Person Re-Identification}}},
  booktitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Wang, Yingquan and Zhang, Pingping and Gao, Shang and Geng, Xia and Lu, Hu and Wang, Dong},
  year = {2021},
  month = oct,
  pages = {12006--12015},
  issn = {2380-7504},
  doi = {10.1109/ICCV48922.2021.01181},
  abstract = {Video-based person re-identification aims to associate the video clips of the same person across multiple non-overlapping cameras. Spatial-temporal representations can provide richer and complementary information between frames, which are crucial to distinguish the target person when occlusion occurs. This paper proposes a novel Pyramid Spatial-Temporal Aggregation (PSTA) framework to aggregate the frame-level features progressively and fuse the hierarchical temporal features into a final video-level representation. Thus, short-term and long-term temporal information could be well exploited by different hierarchies. Furthermore, a Spatial-Temporal Aggregation Module (STAM) is proposed to enhance the aggregation capability of PSTA. It mainly consists of two novel attention blocks: Spatial Reference Attention (SRA) and Temporal Reference Attention (TRA). SRA explores the spatial correlations within a frame to determine the attention weight of each location. While TRA extends SRA with the correlations between adjacent frames, temporal consistency information can be fully explored to suppress the interference features and strengthen the discriminative ones. Extensive experiments on several challenging benchmarks demonstrate the effectiveness of the proposed PSTA, and our full model reaches 91.5\% and 98.3\% Rank-1 accuracy on MARS and DukeMTMC-VID benchmarks. The source code is available at https://github.com/WangYQ9/VideoReID-PSTA.},
  keywords = {Aggregates,Codes,Computer vision,Correlation,Fuses,Image and video retrieval,Interference,Mars},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\M336MHFZ\\Wang et al. - 2021 - Pyramid Spatial-Temporal Aggregation for Video-bas.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\QV42TEIJ\\9710753.html}
}

@misc{wangYouOnlyLearn2021,
  title = {You {{Only Learn One Representation}}: {{Unified Network}} for {{Multiple Tasks}}},
  shorttitle = {You {{Only Learn One Representation}}},
  author = {Wang, Chien-Yao and Yeh, I.-Hau and Liao, Hong-Yuan Mark},
  year = {2021},
  month = may,
  number = {arXiv:2105.04206},
  eprint = {2105.04206},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  abstract = {People ``understand'' the world via vision, hearing, tactile, and also the past experience. Human experience can be learned through normal learning (we call it explicit knowledge), or subconsciously (we call it implicit knowledge). These experiences learned through normal learning or subconsciously will be encoded and stored in the brain. Using these abundant experience as a huge database, human beings can effectively process data, even they were unseen beforehand. In this paper, we propose a unified network to encode implicit knowledge and explicit knowledge together, just like the human brain can learn knowledge from normal learning as well as subconsciousness learning. The unified network can generate a unified representation to simultaneously serve various tasks. We can perform kernel space alignment, prediction refinement, and multi-task learning in a convolutional neural network. The results demonstrate that when implicit knowledge is introduced into the neural network, it benefits the performance of all tasks. We further analyze the implicit representation learnt from the proposed unified network, and it shows great capability on catching the physical meaning of different tasks. The source code of this work is at : https://github.com/WongKinYiu/yolor.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\ZLWV922I\\Wang et al. - 2021 - You Only Learn One Representation Unified Network.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\M9FS6K8N\\2105.html}
}

@article{weiPersonTransferGAN2017,
  title = {Person {{Transfer GAN}} to {{Bridge Domain Gap}} for {{Person Re-Identification}}},
  author = {Wei, Longhui and Zhang, Shiliang and Gao, Wen and Tian, Qi},
  year = {2017},
  month = nov,
  abstract = {Although the performance of person Re-Identification (ReID) has been significantly boosted, many challenging issues in real scenarios have not been fully investigated, e.g., the complex scenes and lighting variations, viewpoint and pose changes, and the large number of identities in a camera network. To facilitate the research towards conquering those issues, this paper contributes a new dataset called MSMT17 with many important features, e.g., 1) the raw videos are taken by an 15-camera network deployed in both indoor and outdoor scenes, 2) the videos cover a long period of time and present complex lighting variations, and 3) it contains currently the largest number of annotated identities, i.e., 4,101 identities and 126,441 bounding boxes. We also observe that, domain gap commonly exists between datasets, which essentially causes severe performance drop when training and testing on different datasets. This results in that available training data cannot be effectively leveraged for new testing domains. To relieve the expensive costs of annotating new training samples, we propose a Person Transfer Generative Adversarial Network (PTGAN) to bridge the domain gap. Comprehensive experiments show that the domain gap could be substantially narrowed-down by the PTGAN.},
  langid = {english},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\3BBFRE4S\\Wei et al. - 2017 - Person Transfer GAN to Bridge Domain Gap for Perso.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\2UG2JS85\\1711.html}
}

@inproceedings{weserOntologybasedMetamodelCapability2020,
  title = {An {{Ontology-based Metamodel}} for {{Capability Descriptions}}},
  booktitle = {2020 25th {{IEEE International Conference}} on {{Emerging Technologies}} and {{Factory Automation}} ({{ETFA}})},
  author = {Weser, Michael and Bock, J{\"u}rgen and Schmitt, Siwara and Perzylo, Alexander and Evers, Kathrin},
  year = {2020},
  month = sep,
  volume = {1},
  pages = {1679--1686},
  issn = {1946-0759},
  doi = {10.1109/ETFA46521.2020.9212104},
  abstract = {This paper presents an approach to describe abilities of manufacturing resources by a formal description of capabilities using Semantic Web technologies. A hierarchical ontology architecture is proposed to represent, publish, and extend knowledge on capabilities for different application domains and use cases. Furthermore, the paper describes patterns of how the underlying formal logic can be used in taxonomy modeling and the inference of implicit capability facts. The usability and performance of the approach was validated by formalizing capability knowledge of related work and evaluated in benchmarking a prototypical implemented tool for managing and querying catalogs of resources and their capabilities. The proposed concept is intended to be used as a foundation for a future multi-layered feasibility checking, which evaluates the compatibility of resources and their offered skills with the requirements of manufacturing tasks at symbolic and subsymbolic levels. Extended evaluations might be based on parameters, analytics, simulation, and other means.},
  keywords = {capabilities,manufacturing,Ontologies,ontology,Production systems,semantic web,Semantic Web,Semantics,skills,Taxonomy,Temperature,Vocabulary},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\G5WJQI45\\Weser et al. - 2020 - An Ontology-based Metamodel for Capability Descrip.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\4NTNR2ZD\\9212104.html}
}

@article{witkowerBodilyCommunicationEmotion2019,
  title = {Bodily Communication of Emotion: {{Evidence}} for Extrafacial Behavioral Expressions and Available Coding Systems},
  shorttitle = {Bodily Communication of Emotion},
  author = {Witkower, Zachary and Tracy, Jessica L.},
  year = {2019},
  journal = {Emotion Review},
  volume = {11},
  number = {2},
  pages = {184--193},
  publisher = {{Sage Publications Sage UK: London, England}},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\APVQVWCY\\Witkower und Tracy - 2019 - Bodily communication of emotion Evidence for extr.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\D2H2IRZD\\1754073917749880.html}
}

@inproceedings{wuClimateWeatherInspecting2022,
  title = {Climate and {{Weather}}: {{Inspecting Depression Detection}} via {{Emotion Recognition}}},
  shorttitle = {Climate and {{Weather}}},
  booktitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Wu, Wen and Wu, Mengyue and Yu, Kai},
  year = {2022},
  month = may,
  pages = {6262--6266},
  issn = {2379-190X},
  doi = {10.1109/ICASSP43922.2022.9746634},
  abstract = {Automatic depression detection has attracted increasing amount of attention but remains a challenging task. Psychological research suggests that depressive mood is closely related with emotion expression and perception, which motivates the investigation of whether knowledge of emotion recognition can be transferred for depression detection. This paper uses pretrained features extracted from the emotion recognition model for depression detection, further fuses emotion modality with audio and text to form multimodal depression detection. The proposed emotion transfer improves depression detection performance on DAIC-WOZ as well as increases the training stability. The analysis of how the emotion expressed by de-pressed individuals is further perceived provides clues for further understanding of the relationship between depression and emotion.},
  keywords = {Depression,depression detection,emotion,Emotion recognition,Feature extraction,Mood,Signal processing,Stability analysis,Training,transfer learning},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\WQZW5KSM\\Wu et al. - 2022 - Climate and Weather Inspecting Depression Detecti.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\34ZSZABR\\9746634.html}
}

@article{xieNeuroSymbolicVerificationDeep2022,
  title = {Neuro-{{Symbolic Verification}} of {{Deep Neural Networks}}},
  author = {Xie, Xuan and Kersting, Kristian and Neider, Daniel},
  year = {2022},
  month = mar,
  journal = {arXiv:2203.00938 [cs]},
  eprint = {2203.00938},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Formal verification has emerged as a powerful approach to ensure the safety and reliability of deep neural networks. However, current verification tools are limited to only a handful of properties that can be expressed as first-order constraints over the inputs and output of a network. While adversarial robustness and fairness fall under this category, many real-world properties (e.g., "an autonomous vehicle has to stop in front of a stop sign") remain outside the scope of existing verification technology. To mitigate this severe practical restriction, we introduce a novel framework for verifying neural networks, named neuro-symbolic verification. The key idea is to use neural networks as part of the otherwise logical specification, enabling the verification of a wide variety of complex, real-world properties, including the one above. Moreover, we demonstrate how neuro-symbolic verification can be implemented on top of existing verification infrastructure for neural networks, making our framework easily accessible to researchers and practitioners alike.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Logic in Computer Science,Computer Science - Machine Learning,F.3.1,I.2.0},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\Q3PD2G22\\Xie et al. - 2022 - Neuro-Symbolic Verification of Deep Neural Network.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\VLU77HHI\\2203.html}
}

@article{xuBioInspiredDeepAttribute2021,
  title = {Bio-{{Inspired Deep Attribute Learning Towards Facial Aesthetic Prediction}}},
  author = {Xu, Mingliang and Chen, Fuhai and Li, Lu and Shen, Chen and Lv, Pei and Zhou, Bing and Ji, Rongrong},
  year = {2021},
  month = jan,
  journal = {IEEE Transactions on Affective Computing},
  volume = {12},
  number = {1},
  pages = {227--238},
  issn = {1949-3045},
  doi = {10.1109/TAFFC.2018.2868651},
  abstract = {Computational prediction of facial aesthetics has attracted ever-increasing research focus, which has wide range of prospects in multimedia applications. The key challenge lies in extracting discriminative and perception-aware features to characterize the facial beautifulness. To this end, the existing schemes simply adopt a direct feature mapping, which relies on handcraft-designed low-level features that cannot reflect human-level aesthetic perception. In this paper, we present a systematic framework towards designing biology-inspired, discriminative representation for facial aesthetic prediction. First, we design a group of biological experiments that adopt eye tracker to identify spatial regions of interest during the facial aesthetic judgments of subjects, which forms a Bio-inspired Facial Aesthetic Ontology (Bio-FAO) and is made public available. Second, we adopt the cutting-edge convolutional neural network to train a set of Bio-inspired Attribute features, termed Bio-AttriBank, which forms a mid-level interpretable representation corresponding to the aforementioned Bio-FAO. For a given image, the facial aesthetic prediction is then formulated as a classification problem over the Bio-AttriBank descriptor responses, which well bridges the affective gap, and provides explainable evidences on why/how a face is beautiful or not. We have carried out extensive experiments on both JAFFE and FaceWarehouse datasets, with comparisons to a set of state-of-the-art and alternative approaches. Superior performance gains in the experiments have demonstrated the merits of the proposed scheme.},
  keywords = {aesthetic concept,bio-inspired attention,Bridges,deep learning,Detectors,Face,Facial aesthetic,Feature extraction,Machine learning,Ontologies,Visualization},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\YNZAGPP3\\Xu et al. - 2021 - Bio-Inspired Deep Attribute Learning Towards Facia.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\B8K9Q6A3\\8454803.html}
}

@misc{xuComprehensiveSurveyImage2022,
  title = {A {{Comprehensive Survey}} of {{Image Augmentation Techniques}} for {{Deep Learning}}},
  author = {Xu, Mingle and Yoon, Sook and Fuentes, Alvaro and Park, Dong Sun},
  year = {2022},
  month = may,
  number = {arXiv:2205.01491},
  eprint = {2205.01491},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  abstract = {Deep learning has been achieving decent performance in computer vision requiring a large volume of images, however, collecting images is expensive and difficult in many scenarios. To alleviate this issue, many image augmentation algorithms have been proposed as effective and efficient strategies. Understanding current algorithms is essential to find suitable methods or develop novel techniques for given tasks. In this paper, we perform a comprehensive survey on image augmentation for deep learning with a novel informative taxonomy. To get the basic idea why we need image augmentation, we introduce the challenges in computer vision tasks and vicinity distribution. Then, the algorithms are split into three categories; model-free, model-based, and optimizing policy-based. The model-free category employs image processing methods while the model-based method leverages trainable image generation models. In contrast, the optimizing policy-based approach aims to find the optimal operations or their combinations. Furthermore, we discuss the current trend of common applications with two more active topics, leveraging different ways to understand image augmentation, such as group and kernel theory, and deploying image augmentation for unsupervised learning. Based on the analysis, we believe that our survey gives a better understanding helpful to choose suitable methods or design novel algorithms for practical applications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: 41 pages},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\G4XQMW3F\\Xu et al. - 2022 - A Comprehensive Survey of Image Augmentation Techn.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\DH3JYRM2\\2205.html}
}

@article{xuScalingEdgeInference2018,
  title = {Scaling for Edge Inference of Deep Neural Networks},
  author = {Xu, Xiaowei and Ding, Yukun and Hu, Sharon and Niemier, Michael and Cong, Jason and Hu, Yu and Shi, Yiyu},
  year = {2018},
  month = apr,
  journal = {Nature Electronics},
  volume = {1},
  doi = {10.1038/s41928-018-0059-3},
  abstract = {Deep neural networks offer considerable potential across a range of applications, from advanced manufacturing to autonomous cars. A clear trend in deep neural networks is the exponential growth of network size and the associated increases in computational complexity and memory consumption. However, the performance and energy efficiency of edge inference, in which the inference (the application of a trained network to new data) is performed locally on embedded platforms that have limited area and power budget, is bounded by technology scaling. Here we analyse recent data and show that there are increasing gaps between the computational complexity and energy efficiency required by data scientists and the hardware capacity made available by hardware architects. We then discuss various architecture and algorithm innovations that could help to bridge the gaps. This Perspective highlights the existence of gaps between the computational complexity and energy efficiency required for the continued scaling of deep neural networks and the hardware capacity actually available with current CMOS technology scaling, in situations where edge inference is required; it then discusses various architecture and algorithm innovations that could help to bridge these gaps.},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\EEUEZHUP\\Xu et al. - 2018 - Scaling for edge inference of deep neural networks.pdf}
}

@misc{yangDatasetPruningReducing2022,
  title = {Dataset {{Pruning}}: {{Reducing Training Data}} by {{Examining Generalization Influence}}},
  shorttitle = {Dataset {{Pruning}}},
  author = {Yang, Shuo and Xie, Zeke and Peng, Hanyu and Xu, Min and Sun, Mingming and Li, Ping},
  year = {2022},
  month = may,
  number = {arXiv:2205.09329},
  eprint = {2205.09329},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  abstract = {The great success of deep learning heavily relies on increasingly larger training data, which comes at a price of huge computational and infrastructural costs. This poses crucial questions that, do all training data contribute to model's performance? How much does each individual training sample or a sub-training-set affect the model's generalization, and how to construct a smallest subset from the entire training data as a proxy training set without significantly sacrificing the model's performance? To answer these, we propose dataset pruning, an optimization-based sample selection method that can (1) examine the influence of removing a particular set of training samples on model's generalization ability with theoretical guarantee, and (2) construct a smallest subset of training data that yields strictly constrained generalization gap. The empirically observed generalization gap of dataset pruning is substantially consistent with our theoretical expectations. Furthermore, the proposed method prunes 40\% training examples on the CIFAR-10 dataset, halves the convergence time with only 1.3\% test accuracy decrease, which is superior to previous score-based sample selection methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\ZHSUG5E2\\Yang et al. - 2022 - Dataset Pruning Reducing Training Data by Examini.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\N57UIESU\\2205.html}
}

@inproceedings{yangFacialExpressionRecognition2018,
  title = {Facial {{Expression Recognition}} by {{De-expression Residue Learning}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Yang, Huiyuan and Ciftci, Umur and Yin, Lijun},
  year = {2018},
  month = jun,
  pages = {2168--2177},
  issn = {2575-7075},
  doi = {10.1109/CVPR.2018.00231},
  abstract = {A facial expression is a combination of an expressive component and a neutral component of a person. In this paper, we propose to recognize facial expressions by extracting information of the expressive component through a de-expression learning procedure, called De-expression Residue Learning (DeRL). First, a generative model is trained by cGAN. This model generates the corresponding neutral face image for any input face image. We call this procedure de-expression because the expressive information is filtered out by the generative model; however, the expressive information is still recorded in the intermediate layers. Given the neutral face image, unlike previous works using pixel-level or feature-level difference for facial expression classification, our new method learns the deposition (or residue) that remains in the intermediate layers of the generative model. Such a residue is essential as it contains the expressive component deposited in the generative model from any input facial expression images. Seven public facial expression databases are employed in our experiments. With two databases (BU-4DFE and BP4D-spontaneous) for pre-training, the DeRL method has been evaluated on five databases, CK+, Oulu-CASIA, MMI, BU-3DFE, and BP4D+. The experimental results demonstrate the superior performance of the proposed method.},
  keywords = {Databases,Face,Face recognition,Gallium nitride,Generators,Training,Training data},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\BRA3AEBI\\Yang et al. - 2018 - Facial Expression Recognition by De-expression Res.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\ZKR2IW6Y\\8578329.html}
}

@inproceedings{yangIdentityAdaptiveFacialExpression2018,
  title = {Identity-{{Adaptive Facial Expression Recognition}} through {{Expression Regeneration Using Conditional Generative Adversarial Networks}}},
  booktitle = {2018 13th {{IEEE International Conference}} on {{Automatic Face Gesture Recognition}} ({{FG}} 2018)},
  author = {Yang, Huiyuan and Zhang, Zheng and Yin, Lijun},
  year = {2018},
  month = may,
  pages = {294--301},
  doi = {10.1109/FG.2018.00050},
  abstract = {Subject variation is a challenging issue for fa- cial expression recognition, especially when handling unseen subjects with small-scale lableled facial expression databases. Although transfer learning has been widely used to tackle the problem, the performance degrades on new data. In this paper, we present a novel approach (so-called IA-gen) to alleviate the issue of subject variations by regenerating expressions from any input facial images. First of all, we train conditional generative models to generate six prototypic facial expressions from any given query face image while keeping the identity related information unchanged. Generative Adversarial Networks are employed to train the conditional generative models, and each of them is designed to generate one of the prototypic facial expression images. Second, a regular CNN (FER-Net) is fine- tuned for expression classification. After the corresponding prototypic facial expressions are regenerated from each facial image, we output the last FC layer of FER-Net as features for both the input image and the generated images. Based on the minimum distance between the input image and the generated expression images in the feature space, the input image is classified as one of the prototypic expressions consequently. Our proposed method can not only alleviate the influence of inter-subject variations, but will also be flexible enough to integrate with any other FER CNNs for person-independent facial expression recognition. Our method has been evaluated on CK+, Oulu-CASIA, BU-3DFE and BU-4DFE databases, and the results demonstrate the effectiveness of our proposed method.},
  keywords = {CNN,Databases,Face,Face recognition,FER,Gallium nitride,GAN,Generators,Identity-adaptive,Image recognition,Training},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\M4KBV7A3\\Yang et al. - 2018 - Identity-Adaptive Facial Expression Recognition th.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\2F7Y6S3T\\8373843.html}
}

@article{yangImageDataAugmentation2022,
  title = {Image {{Data Augmentation}} for {{Deep Learning}}: {{A Survey}}},
  shorttitle = {Image {{Data Augmentation}} for {{Deep Learning}}},
  author = {Yang, Suorong and Xiao, Weikang and Zhang, Mengcheng and Guo, Suhan and Zhao, Jian and Shen, Furao},
  year = {2022},
  month = apr,
  journal = {arXiv:2204.08610 [cs]},
  eprint = {2204.08610},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Deep learning has achieved remarkable results in many computer vision tasks. Deep neural networks typically rely on large amounts of training data to avoid overfitting. However, labeled data for real-world applications may be limited. By improving the quantity and diversity of training data, data augmentation has become an inevitable part of deep learning model training with image data. As an effective way to improve the sufficiency and diversity of training data, data augmentation has become a necessary part of successful application of deep learning models on image data. In this paper, we systematically review different image data augmentation methods. We propose a taxonomy of reviewed methods and present the strengths and limitations of these methods. We also conduct extensive experiments with various data augmentation methods on three typical computer vision tasks, including semantic segmentation, image classification and object detection. Finally, we discuss current challenges faced by data augmentation and future research directions to put forward some useful research guidance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\LWRLNNXJ\\Yang et al. - 2022 - Image Data Augmentation for Deep Learning A Surve.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\3KINZS9K\\2204.html}
}

@inproceedings{yanResearchEmotionRecognition2021,
  title = {Research on {{Emotion Recognition Based}} on {{Facial Expression}} and {{EEG}}},
  booktitle = {2021 {{International Conference}} on {{Networking Systems}} of {{AI}} ({{INSAI}})},
  author = {Yan, Na and Zeng, Xinhua and Chen, Lei},
  year = {2021},
  month = nov,
  pages = {118--122},
  doi = {10.1109/INSAI54028.2021.00031},
  abstract = {With the development of artificial intelligence technology, emotion recognition has become an increasingly important research topic. Recognizing emotions only from the data with a single modality has its drawbacks. In this paper, the two modalities of facial expressions and EEG are integrated to realize the recognition of five types of emotions such as happiness, and the accuracy rate has reached a relatively satisfactory result. For facial expression modalities, this paper uses histogram equalization for preprocessing, then use LBP algorithm to extract facial expression features, and finally use SVM for expression recognition; for EEG modalities, this paper uses wavelet threshold denoising for preprocessing, and then use fractal dimension and multi-scale entropy algorithm to extract EEG signal features. This paper classifies EEG signals in the DEAP data set for emotion classification. Under the condition of using only one EEG channel FP1, the accuracy of SVM classification can reach 75.0\%.},
  keywords = {EEG signal,Electroencephalography,emotion recognition,Emotion recognition,Face recognition,facial expression,Feature extraction,Histograms,multi-modality,Noise reduction,Support vector machines},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\X6YUQF5K\\Yan et al. - 2021 - Research on Emotion Recognition Based on Facial Ex.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\5Q9IXUYX\\9757969.html}
}

@misc{yarbroughTrebleDamagesWilful2019,
  title = {Treble {{Damages}} for {{Wilful Patent}} Infringment},
  author = {Yarbrough, Robert},
  year = {2019},
  month = jan,
  abstract = {The patent statute provides that the judge has the discretion to award enhanced damages of up to three times actual damages for patent infringement.},
  langid = {american},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\HLUQILB9\\treble-damages-for-willful-patent-infringement.html}
}

@article{yigitcanlarHowAreSmart2021,
  title = {How {{Are Smart City Concepts}} and {{Technologies Perceived}} and {{Utilized}}? {{A Systematic Geo-Twitter Analysis}} of {{Smart Cities}} in {{Australia}}},
  shorttitle = {How {{Are Smart City Concepts}} and {{Technologies Perceived}} and {{Utilized}}?},
  author = {Yigitcanlar, Tan and Kankanamge, Nayomi and Vella, Karen},
  year = {2021},
  month = apr,
  journal = {Journal of Urban Technology},
  volume = {28},
  number = {1-2},
  pages = {135--154},
  publisher = {{Routledge}},
  issn = {1063-0732},
  doi = {10.1080/10630732.2020.1753483},
  abstract = {``Smart cities'' is a hot topic in debates about urban policy and practice across the globe. There is, however, limited knowledge and understanding about trending smart city concepts and technologies; relationships between popular smart city concepts and technologies; policies that influence the perception and utilization of smart city concepts and technologies. The aim of this study is to evaluate how smart city concepts and technologies are perceived and utilized in cities. The methodology involves a social media analysis approach\textemdash i.e., systematic geo-Twitter analysis\textemdash that contains descriptive, content, policy, and spatial analyses. For the empirical investigation, the Australian context is selected as the testbed. The results reveal that: (a) innovation, sustainability, and governance are the most popular smart city concepts; (b) internet-of-things, artificial intelligence, and autonomous vehicle technology are the most popular technologies; (c) a balanced view exists on the importance of both smart city concepts and technologies; (d) Sydney, Melbourne, and Brisbane are the leading Australian smart cities; (e) systematic geo-Twitter analysis is a useful methodological approach for investigating perceptions and utilization of smart city concepts and technologies. The findings provide a clear snapshot of community perceptions on smart city concepts and technologies, and can inform smart city policymaking.},
  keywords = {Australian cities,data analytics,Smart cities,smart city policy,social media,Twitter},
  annotation = {\_eprint: https://doi.org/10.1080/10630732.2020.1753483},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\CS6SPVVQ\\Yigitcanlar et al. - 2021 - How Are Smart City Concepts and Technologies Perce.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\S7H8DGXL\\10630732.2020.html}
}

@article{yousefiriziAIBasedDetectionClassification2022,
  title = {{{AI-Based Detection}}, {{Classification}} and {{Prediction}}/{{Prognosis}} in {{Medical Imaging}}: {{Towards Radiophenomics}}},
  shorttitle = {{{AI-Based Detection}}, {{Classification}} and {{Prediction}}/{{Prognosis}} in {{Medical Imaging}}},
  author = {Yousefirizi, Fereshteh and Decazes, Pierre and Amyar, Amine and Ruan, Su and Saboury, Babak and Rahmim, Arman},
  year = {2022},
  month = jan,
  journal = {arXiv:2110.10332 [physics]},
  eprint = {2110.10332},
  eprinttype = {arxiv},
  primaryclass = {physics},
  abstract = {Artificial intelligence (AI) techniques have significant potential to enable effective, robust and automated image phenotyping including identification of subtle patterns. AI-based detection searches the image space to find the regions of interest based on patterns and features. There is a spectrum of tumor histologies from benign to malignant that can be identified by AI-based classification approaches using image features. The extraction of minable information from images gives way to the field of radiomics and can be explored via explicit (handcrafted/engineered) and deep radiomics frameworks. Radiomics analysis has the potential to be utilized as a noninvasive technique for the accurate characterization of tumors to improve diagnosis and treatment monitoring. This work reviews AI-based techniques, with a special focus on oncological PET and PET/CT imaging, for different detection, classification, and prediction/prognosis tasks. We also discuss needed efforts to enable the translation of AI techniques to routine clinical workflows, and potential improvements and complementary techniques such as the use of natural language processing on electronic health records and neuro-symbolic AI techniques.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing,Physics - Medical Physics},
  note = {see only 6.4 Neuro-Symbolic AI Techniques},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\KPFMZLWK\\Yousefirizi et al. - 2022 - AI-Based Detection, Classification and Prediction.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\QJHN54E3\\2110.html}
}

@inproceedings{yuGaitbasedEmotionRecognition2021,
  title = {Gait-Based {{Emotion Recognition Using Spatial Temporal Graph Convolutional Networks}}},
  booktitle = {2021 {{International Conference}} on {{Computer Information Science}} and {{Artificial Intelligence}} ({{CISAI}})},
  author = {Yu, Shiyi and Li, Xinde},
  year = {2021},
  month = sep,
  pages = {190--193},
  doi = {10.1109/CISAI54367.2021.00043},
  abstract = {Most of the existing gait emotion recognition methods use spatial convolution or temporal convolution separately. Without integrating these two dimensions, it is easy to loss valid information for emotion analysis. In order to solve this problem, we propose a supervised learning method using Spatial Temporal Graph Convolutional Networks (ST-GCN) to classify emotions into positive, neutral and negative patterns. In addition, we made a new gait dataset called EmoGait3d including 742 real gait skeleton sequences extracted from videos, also annotated with emotion labels. According to the experiment results, our method achieves the accuracy of 92.2\% which outperforms current state-of-art algorithms for gait emotion recognition.},
  keywords = {Convolution,emotion recognition,Emotion recognition,Fuses,gait,Information science,Linguistics,Skeleton,ST-GCN,Supervised learning},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\JE55B8DB\\Yu und Li - 2021 - Gait-based Emotion Recognition Using Spatial Tempo.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\KHSNBM64\\9718956.html}
}

@inproceedings{yuImageBasedStatic2015,
  title = {Image Based Static Facial Expression Recognition with Multiple Deep Network Learning},
  booktitle = {Proceedings of the 2015 {{ACM}} on International Conference on Multimodal Interaction},
  author = {Yu, Zhiding and Zhang, Cha},
  year = {2015},
  pages = {435--442},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\6Q69G3AT\\Yu und Zhang - 2015 - Image based static facial expression recognition w.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\PK38WYUX\\2818346.html}
}

@article{zawadzkaOntologicalModelContextual2021,
  title = {Ontological {{Model}} for {{Contextual Data Defining Time Series}} for {{Emotion Recognition}} and {{Analysis}}},
  author = {Zawadzka, Teresa and Waloszek, Wojciech and Karpus, Aleksandra and Zapa{\l}owska, Sara and Wr{\'o}bel, Micha{\l} R.},
  year = {2021},
  journal = {IEEE Access},
  volume = {9},
  pages = {166674--166694},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2021.3132728},
  abstract = {One of the major challenges facing the field of Affective Computing is the reusability of datasets. Existing affective-related datasets are not consistent with each other, they store a variety of information in different forms, different formats, and the terms used to describe them are not unified. This paper proposes a Recording Ontology for Affective-related Datasets (ROAD) as a solution to this problem, by formally describing the datasets and unifying the terms used. The developed ontology allows information about the origin and meaning of the data to be modeled, i.e., time series, representing both emotional states and features derived from biosignals. Furthermore, the ROAD ontology is extensible and not application-oriented, thus it can be used to store data from a wide range of Affective Computing experiments. The ontology was validated by modeling data obtained from one experiment on AMIGOS dataset (A dataset for Multimodal research of affect, personality traits and mood on Individuals and GrOupS). The approach proposed in the paper can be used both by researchers who create new datasets or want to reuse existing ones, and for those who want to process data from experiments in a more automated way.},
  keywords = {Affective computing,Computational modeling,conceptualization,dataset,emotion,Emotion recognition,Interviews,Ontologies,ontology,ontology development,Roads,time series,Time series analysis},
  note = {Recording Ontology for multimodal Affective-related Datasets (ROAD)},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\MJCHBZ6Q\\Zawadzka et al. - 2021 - Ontological Model for Contextual Data Defining Tim.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\7J9DPGUN\\9635802.html}
}

@article{zenLearningPersonalizedModels2016,
  title = {Learning {{Personalized Models}} for {{Facial Expression Analysis}} and {{Gesture Recognition}}},
  author = {Zen, Gloria and Porzi, Lorenzo and Sangineto, Enver and Ricci, Elisa and Sebe, Nicu},
  year = {2016},
  month = apr,
  journal = {IEEE Transactions on Multimedia},
  volume = {18},
  number = {4},
  pages = {775--788},
  issn = {1941-0077},
  doi = {10.1109/TMM.2016.2523421},
  abstract = {Facial expression and gesture recognition algorithms are key enabling technologies for human-computer interaction (HCI) systems. State of the art approaches for automatic detection of body movements and analyzing emotions from facial features heavily rely on advanced machine learning algorithms. Most of these methods are designed for the average user, but the assumption ``one-size-fits-all'' ignores diversity in cultural background, gender, ethnicity, and personal behavior, and limits their applicability in real-world scenarios. A possible solution is to build personalized interfaces, which practically implies learning person-specific classifiers and usually collecting a significant amount of labeled samples for each novel user. As data annotation is a tedious and time-consuming process, in this paper we present a framework for personalizing classification models which does not require labeled target data. Personalization is achieved by devising a novel transfer learning approach. Specifically, we propose a regression framework which exploits auxiliary (source) annotated data to learn the relation between person-specific sample distributions and parameters of the corresponding classifiers. Then, when considering a new target user, the classification model is computed by simply feeding the associated (unlabeled) sample distribution into the learned regression function. We evaluate the proposed approach in different applications: pain recognition and action unit detection using visual data and gestures classification using inertial measurements, demonstrating the generality of our method with respect to different input data types and basic classifiers. We also show the advantages of our approach in terms of accuracy and computational time both with respect to user-independent approaches and to previous personalization techniques.},
  keywords = {Data models,Face recognition,Facial expression analysis,Facial Expression Analysis,gesture recognition,Gesture recognition,Gesture Recognition,Human computer interaction,Pain,personalization,Personalization,ransductive Parameter Transfer,Training,transductive parameter transfer,transfer learning,Transfer Learning,Visualization},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\SDT3PBA6\\Zen et al. - 2016 - Learning Personalized Models for Facial Expression.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\XKJKFXNA\\7394167.html}
}

@article{zhangAlignedreidSurpassingHumanlevel2017,
  title = {Alignedreid: {{Surpassing}} Human-Level Performance in Person Re-Identification},
  shorttitle = {Alignedreid},
  author = {Zhang, Xuan and Luo, Hao and Fan, Xing and Xiang, Weilai and Sun, Yixiao and Xiao, Qiqi and Jiang, Wei and Zhang, Chi and Sun, Jian},
  year = {2017},
  journal = {arXiv preprint arXiv:1711.08184},
  eprint = {1711.08184},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\83CC9VP5\\Zhang et al. - 2017 - Alignedreid Surpassing human-level performance in.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\6HQA2PLP\\1711.html}
}

@article{zhangDeepCrossCorpusSpeech2021,
  title = {Deep {{Cross-Corpus Speech Emotion Recognition}}: {{Recent Advances}} and {{Perspectives}}},
  shorttitle = {Deep {{Cross-Corpus Speech Emotion Recognition}}},
  author = {Zhang, Shiqing and Liu, Ruixin and Tao, Xin and Zhao, Xiaoming},
  year = {2021},
  month = nov,
  journal = {Frontiers in Neurorobotics},
  volume = {15},
  pages = {784514},
  issn = {1662-5218},
  doi = {10.3389/fnbot.2021.784514},
  abstract = {Automatic speech emotion recognition (SER) is a challenging component of human-computer interaction (HCI). Existing literatures mainly focus on evaluating the SER performance by means of training and testing on a single corpus with a single language setting. However, in many practical applications, there are great differences between the training corpus and testing corpus. Due to the diversity of different speech emotional corpus or languages, most previous SER methods do not perform well when applied in real-world cross-corpus or cross-language scenarios. Inspired by the powerful feature learning ability of recently-emerged deep learning techniques, various advanced deep learning models have increasingly been adopted for cross-corpus SER. This paper aims to provide an up-to-date and comprehensive survey of cross-corpus SER, especially for various deep learning techniques associated with supervised, unsupervised and semi-supervised learning in this area. In addition, this paper also highlights different challenges and opportunities on cross-corpus SER tasks, and points out its future trends.},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\5X8IN8ZC\\Zhang et al. - 2021 - Deep Cross-Corpus Speech Emotion Recognition Rece.pdf}
}

@inproceedings{zhangExceptionHandlingExperiencebased2013,
  title = {Exception Handling for Experience-Based Mobile Cognitive Systems in Restaurant Environments Exemplified by Guest Detection},
  booktitle = {2013 {{IEEE International Conference}} on {{Information}} and {{Automation}} ({{ICIA}})},
  author = {Zhang, Liwei and Rockel, Sebastian and Zhang, Jianwei},
  year = {2013},
  month = aug,
  pages = {970--975},
  doi = {10.1109/ICInfA.2013.6720435},
  abstract = {Recording and exploiting past experiences to handle an unforeseen situation is an important asset of human beings. However, in current robot architectures, experience-based learning has mainly been realized at sub-symbolic levels. In this paper, we present how to handle exception for an experience-based artificial cognitive system which is able to fulfill unforeseen situations in restaurant environments. Guest detection is employed to demonstrate the exception handling functionality. Experiments were executed to validate and evaluate the effectiveness of this artificial cognitive system.},
  keywords = {Cognitive System,Exception Handling,Experience Learning,Face,Guest Detection,Mobile communication,Planning,Robot sensing systems,Target tracking},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\IRERFJQX\\Zhang et al. - 2013 - Exception handling for experience-based mobile cog.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\CBX2AHWZ\\6720435.html}
}

@article{zhangGANSERSelfsupervisedData2022,
  title = {{{GANSER}}: {{A Self-supervised Data Augmentation Framework}} for {{EEG-based Emotion Recognition}}},
  shorttitle = {{{GANSER}}},
  author = {Zhang, Zhi and Zhong, Sheng-hua and Liu, Yan},
  year = {2022},
  journal = {IEEE Transactions on Affective Computing},
  pages = {1--1},
  issn = {1949-3045},
  doi = {10.1109/TAFFC.2022.3170369},
  abstract = {Electroencephalography (EEG)-based affective computing has a scarcity problem. As a result, it is difficult to build effective, highly accurate and stable models using machine learning algorithms, especially deep learning models. Data augmentation has recently shown performance improvements in deep learning models with increased accuracy, stability and reduced overfitting. In this paper, we propose a novel data augmentation framework, named the generative adversarial network-based self-supervised data augmentation (GANSER). As the first to combine adversarial training with self-supervised learning for EEG-based emotion recognition, the proposed framework generates high-quality and high-diversity simulated EEG samples. In particular, we utilize adversarial training to learn an EEG generator and force the generated EEG signals to approximate the distribution of real samples, ensuring the quality of the augmented samples. A transformation operation is employed to mask parts of the EEG signals and force the generator to synthesize potential EEG signals based on the unmasked parts to produce a wide variety of samples. A masking possibility during transformation is introduced as prior knowledge to generalize the classifier for the augmented sample space. Finally, numerous experiments demonstrate that our proposed method can improve emotion recognition with an increase in performance and achieve state-of-the-art results.},
  keywords = {Brain modeling,data augmentation,Deep learning,EEG-based emotion recognition,Electrodes,Electroencephalography,Emotion recognition,generative adversarial networks,Generative adversarial networks,Training},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\3KKX8MVP\\9763358.html}
}

@inproceedings{zhangNewRealtimeEye2006,
  title = {A New Real-Time Eye Tracking for Driver Fatigue Detection},
  booktitle = {2006 6th {{International Conference}} on {{ITS Telecommunications}}},
  author = {Zhang, Zutao and Zhang, Jiashu},
  year = {2006},
  pages = {8--11},
  publisher = {{IEEE}},
  keywords = {Change detection algorithms,Eyes,Face detection,Face recognition,Fatigue,Glass,Hair,Road accidents,Robustness,Target tracking},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\2VD6A89N\\Zhang und Zhang - 2006 - A New Real-Time Eye Tracking for Driver Fatigue De.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\C7F2CYKW\\4068517.html;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\ILAF8ZY3\\4068517.html}
}

@inproceedings{zhangResearchPersonReidentification2021,
  title = {Research on {{Person Re-identification Based}} on {{Pose-Attention-GAN}}},
  booktitle = {2021 {{International Conference}} on {{Computer Information Science}} and {{Artificial Intelligence}} ({{CISAI}})},
  author = {Zhang, YuXia and Che, Jin and He, YuTing},
  year = {2021},
  month = sep,
  pages = {261--267},
  doi = {10.1109/CISAI54367.2021.00056},
  abstract = {Person re-identification (reID) is a challenging task, with the purpose of matching pedestrian images with the same identity across multiple cameras. With the wide usage of deep learning methods, reID performances by different algorithms increase rapidly. However, in the presence of large pose variations, it faces two major challenges: the lack of cross-view paired training data and learning discriminative identity-sensitive and view-invariant features. To solve the above challenges, this paper proposes an image generation and classification framework for pedestrian arbitrary pose based on the combination of Pose-attention-Generative Adversarial Network(PAGAN) and Resnet50 network. Firstly, the human pose network is used to extract the pose corresponding to the pedestrian image, and the generator and discriminator of the PAGAN network are trained with the pedestrian image and its pose. The generator of the network comprises 9 pose attention transfer blocks that each transfers certain regions it attends to, generating arbitrary pose and high-quality fake images progressively, and discriminator distinguishes whether the input image is real or not. Secondly, the fake images of arbitrary pose are increased as training samples. Finally, the expanded training set is used to train the classification and recognition network to learn features related to identity but not pose, effectively improving the generalization ability of the model. The Rank-1/mAP on the Market1501 and DukeMTMC-reID datasets reach 94.1\%/85.2\% and 84.7\%/72.2\% respectively. Experimental results show that the proposed algorithm has certain advantages.},
  keywords = {Computational modeling,generative adversarial network,Generators,identity feature,Image synthesis,Information science,Person re-identification,pose attention,pose feature,Robustness,Training,Training data},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\HZI2M44S\\Zhang et al. - 2021 - Research on Person Re-identification Based on Pose.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\RX24V48T\\9719245.html}
}

@article{zhangSSGNNMacroMicrofacial2022,
  title = {{{SSGNN}}: {{A Macro}} and {{Microfacial Expression Recognition Graph Neural Network Combining Spatial}} and {{Spectral Domain Features}}},
  shorttitle = {{{SSGNN}}},
  author = {Zhang, Junjie and Sun, Guangmin and Zheng, Kun and Mazhar, Sarah and Fu, Xiaohui and Li, Yu and Yu, Hui},
  year = {2022},
  journal = {IEEE Transactions on Human-Machine Systems},
  pages = {1--30},
  issn = {2168-2305},
  doi = {10.1109/THMS.2022.3163211},
  abstract = {Emotion recognition from macroexpression and microexpression has been widely used in applications such as human\textendash computer interaction, learning status evaluation, and mental disorder diagnosis. However, due to the complexity of human macroexpressions, recognizing macroexpressions with high accuracy is a challenging task. Moreover, the short duration and low movement intensity of microexpressions make its recognition more difficult. For MM-FER (macro and microfacial expression recognition), the key information can be more efficiently expressed by a graph. In this article, a novel framework based on graph neural network named SSGNN (spatial and spectral domain features based on a graph neural network) is designed to extract spatial and spectral domain features from facial images for MM-FER, which can efficiently recognize both macroexpressions and microexpressions under the same model. SSGNN consists of two parts, SPAGNN and SPEGNN, which are used to extract spectral and spatial domain features, respectively. Experiments proved that jointly using the spectral and spatial information extracted by SSGNN can largely improve the performance of MM-FER when the training sample is limited. First, the influences of different neighbors and samples to the model performance was analyzed. Then, the contribution of SPAGNN and SPEGNN were evaluated. It was discovered that fusing the result of SPAGNN and SPEGNN at decision level further improved the performance of MM-FER. Experiment proved that SSGNN can recognize microexpression acquired by various sensors with higher accuracy under different image resolutions and image formats than the compared state-of-the-art methods in most cases. A cross-dataset experiment demonstrated the generalization ability of SSGNN.},
  keywords = {Convolutional neural networks,Cross-dataset,Face recognition,facial expression recognition,Feature extraction,microexpression,spatial and spectral domain graph neural network (SSGNN),spatial domain,Spectral analysis,spectral domain,Task analysis,Training data,Trajectory},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\IC3U6Q3H\\Zhang et al. - 2022 - SSGNN A Macro and Microfacial Expression Recognit.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\VYQRRVRX\\9761968.html}
}

@article{zhangUnderstandingDeepLearning2016,
  title = {Understanding Deep Learning Requires Rethinking Generalization},
  author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  year = {2016},
  month = nov,
  abstract = {Through extensive systematic experiments, we show how the traditional approaches fail to explain why large neural networks generalize well in practice, and why understanding deep learning requires...},
  langid = {english},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\ALMTX6FW\\Zhang et al. - 2016 - Understanding deep learning requires rethinking ge.pdf}
}

@inproceedings{zhaoApplicationDeepLearning2021,
  title = {The {{Application}} of {{Deep Learning}} in {{Micro-Expression Recognition}}},
  booktitle = {2021 3rd {{International Conference}} on {{Machine Learning}}, {{Big Data}} and {{Business Intelligence}} ({{MLBDBI}})},
  author = {Zhao, Xinyuan and Sun, Furong and Shen, Xunbing},
  year = {2021},
  month = dec,
  pages = {176--179},
  doi = {10.1109/MLBDBI54094.2021.00041},
  abstract = {With the advent of the era of big data, the application of artificial intelligence and big data technology has led to widespread interest in facial expression recognition. In addition to the common macro-expressions in daily life, facial expressions also have an imperceptible subtle expression called micro-expressions. Micro-expression is a very fast expression, lasting only 1/25 seconds to 1/5 seconds, expressing the real emotions that people try to suppress and hide. Micro-expressions have important applications in public safety, judicial criminal investigation, clinical medicine, etc. Therefore, the research on micro-expression recognition has been increasing in recent years. In the year of 2006, Hinton's proposal of deep learning in an article in \guillemotleft Science\guillemotright{} made it appear in front of the world as a new field of machine learning, which set off a wave of deep learning. Deep learning is currently one of the hottest research directions in artificial intelligence and machine learning, and it is widely used in speech recognition, natural language processing, image recognition and other fields. Because of the characteristics of micro-expressions such as short time and subtle changes, traditional machine learning algorithms have poor robustness. Therefore, this paper summarizes the research on micro-expression recognition based on deep learning methods, mainly including some mainstream algorithms such as DBN, CNN, and discusses the development problems and trends of deep learning on micro-expression recognition, hoping to provide reference for the subsequent research in this field.},
  keywords = {artificial intelligence,Big Data,Convolutional Neural Network(CNN),Deep Belief Network(DBN),deep learning,Deep learning,machine learning,Machine learning algorithms,Market research,micro-expression,Neural networks,Robustness,Speech recognition},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\NMGLBILW\\Zhao et al. - 2021 - The Application of Deep Learning in Micro-Expressi.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\FJ3ZSI58\\9731067.html}
}

@inproceedings{zhaoEmotionRecognitionUsing2016,
  title = {Emotion Recognition Using Wireless Signals},
  booktitle = {Proceedings of the 22nd {{Annual International Conference}} on {{Mobile Computing}} and {{Networking}}},
  author = {Zhao, Mingmin and Adib, Fadel and Katabi, Dina},
  year = {2016},
  pages = {95--108},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\EE9ACCTT\\EmotionRecognitionUsingWirelessSignals.pdf}
}

@article{zhaoSmartCityResearch2021,
  title = {Smart City Research: {{A}} Holistic and State-of-the-Art Literature Review},
  shorttitle = {Smart City Research},
  author = {Zhao, Fang and Fashola, Olushola I. and Olarewaju, Tolulope I. and Onwumere, Ijeoma},
  year = {2021},
  month = dec,
  journal = {Cities},
  volume = {119},
  pages = {103406},
  issn = {0264-2751},
  doi = {10.1016/j.cities.2021.103406},
  abstract = {This literature review provides a systematic review of smart city research between 2000 and 2019. The aim of the review is to provide a comprehensive picture of the state-of-the-art of research in smart cities by addressing major issues and identifying gaps and areas for future research. The analysis of 191 publications drawn from high-quality journals and the most influential or highly cited smart city literature highlights four major challenges for small city research: (a) smart city research is often fragmented and technology-driven; (b) many studies are on the perceived benefits of smart cities and fewer on the downsides of technologies and failed projects; (c) there is a need to build new theories for smart city research; and (d) there is a lack of empirical testing of the conceptual frameworks developed in smart city research. The research insights of this literature review may encourage practitioners, including city councillors, urban planners, and business managers, to consider smart city strategies in a holistic way when building vibrant, sustainable, and resilient cities of the future.},
  langid = {english},
  keywords = {Entrepreneurship,Literature review,Smart city,Strategy,Technology diffusion,Urban planning and governance},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\SK47F6CV\\S026427512100305X.html}
}

@article{zhouEmotionIntensityIts2022,
  title = {Emotion {{Intensity}} and Its {{Control}} for {{Emotional Voice Conversion}}},
  author = {Zhou, Kun and Sisman, Berrak and Rana, Rajib and Schuller, Bj{\"o}rn W. and Li, Haizhou},
  year = {2022},
  journal = {IEEE Transactions on Affective Computing},
  pages = {1--1},
  issn = {1949-3045},
  doi = {10.1109/TAFFC.2022.3175578},
  abstract = {Emotional voice conversion (EVC) seeks to convert the emotional state of an utterance while preserving the linguistic content and speaker identity. In EVC, emotions are usually treated as discrete categories overlooking the fact that speech also conveys emotions with various intensity levels that the listener can perceive. In this work, we aim to explicitly characterize and control the intensity of emotion. We propose to disentangle the speaker style from linguistic content and encode the speaker style into a style embedding in a continuous space that forms the prototype of emotion embedding. We further learn the actual emotion encoder from an emotion-labelled database and study the use of relative attributes to represent fine-grained emotion intensity. To ensure emotional intelligibility, we incorporate emotion classification loss and emotion embedding similarity loss into the training of the EVC network. As desired, the proposed network controls the fine-grained emotion intensity in the output speech. Through both objective and subjective evaluations, we validate the effectiveness of the proposed network for emotional expressiveness and emotion intensity control.},
  keywords = {Computational modeling,Databases,emotion intensity,Emotion recognition,Emotional voice conversion,Hidden Markov models,limited data,perceptual loss,relative attribute,sequence-to-sequence,Speech recognition,Speech synthesis,Training},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\ALR6RWKU\\Zhou et al. - 2022 - Emotion Intensity and its Control for Emotional Vo.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\V994NE79\\9778970.html}
}

@article{zhouLearningGeneralisableOmniScale2021,
  title = {Learning {{Generalisable Omni-Scale Representations}} for {{Person Re-Identification}}},
  author = {Zhou, Kaiyang and Yang, Yongxin and Cavallaro, Andrea and Xiang, Tao},
  year = {2021},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  pages = {1--1},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2021.3069237},
  abstract = {An effective person re-identification (re-ID) model should learn feature representations that are both discriminative, for distinguishing similar-looking people, and generalisable, for deployment across datasets without any adaptation. In this paper, we develop novel CNN architectures to address both challenges. First, we present a re-ID CNN termed omni-scale network (OSNet) to learn features that not only capture different spatial scales but also encapsulate a synergistic combination of multiple scales, namely omni-scale features. The basic building block consists of multiple convolutional streams, each detecting features at a certain scale. For omni-scale feature learning, a unified aggregation gate is introduced to dynamically fuse multi-scale features with channel-wise weights. OSNet is lightweight as its building blocks comprise factorised convolutions. Second, to improve generalisable feature learning, we introduce instance normalisation (IN) layers into OSNet to cope with cross-dataset discrepancies. Further, to determine the optimal placements of these IN layers in the architecture, we formulate an efficient differentiable architecture search algorithm. Extensive experiments show that, in the conventional same-dataset setting, OSNet achieves state-of-the-art performance, despite being much smaller than existing re-ID models. In the more challenging yet practical cross-dataset setting, OSNet beats most recent unsupervised domain adaptation methods without using any target data.},
  keywords = {Adaptation models,Architecture,Cameras,Convolution,Convolutional codes,Cross-Domain Re-ID,Data models,Feature extraction,Lightweight Network,Neural Architecture Search,Omni-Scale Learning,Person Re-Identification},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\7C3MZESV\\Zhou et al. - 2021 - Learning Generalisable Omni-Scale Representations .pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\FIA2LTUG\\9387612.html}
}

@article{zhouOmniScaleFeatureLearning2019,
  title = {Omni-{{Scale Feature Learning}} for {{Person Re-Identification}}},
  author = {Zhou, Kaiyang and Yang, Yongxin and Cavallaro, Andrea and Xiang, Tao},
  year = {2019},
  month = dec,
  journal = {arXiv:1905.00953 [cs]},
  eprint = {1905.00953},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {As an instance-level recognition problem, person re-identification (ReID) relies on discriminative features, which not only capture different spatial scales but also encapsulate an arbitrary combination of multiple scales. We call features of both homogeneous and heterogeneous scales omni-scale features. In this paper, a novel deep ReID CNN is designed, termed Omni-Scale Network (OSNet), for omni-scale feature learning. This is achieved by designing a residual block composed of multiple convolutional streams, each detecting features at a certain scale. Importantly, a novel unified aggregation gate is introduced to dynamically fuse multi-scale features with input-dependent channel-wise weights. To efficiently learn spatial-channel correlations and avoid overfitting, the building block uses pointwise and depthwise convolutions. By stacking such block layer-by-layer, our OSNet is extremely lightweight and can be trained from scratch on existing ReID benchmarks. Despite its small model size, OSNet achieves state-of-the-art performance on six person ReID datasets, outperforming most large-sized models, often by a clear margin. Code and models are available at: \textbackslash url\{https://github.com/KaiyangZhou/deep-person-reid\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: ICCV 2019; This version adds additional training recipes for practitioners},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\A6ASLHYK\\Zhou et al. - 2019 - Omni-Scale Feature Learning for Person Re-Identifi.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\WSW8397M\\1905.html}
}

@article{zhouTorchreidLibraryDeep2019,
  title = {Torchreid: {{A Library}} for {{Deep Learning Person Re-Identification}} in {{Pytorch}}},
  shorttitle = {Torchreid},
  author = {Zhou, Kaiyang and Xiang, Tao},
  year = {2019},
  month = oct,
  journal = {arXiv:1910.10093 [cs]},
  eprint = {1910.10093},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Person re-identification (re-ID), which aims to re-identify people across different camera views, has been significantly advanced by deep learning in recent years, particularly with convolutional neural networks (CNNs). In this paper, we present Torchreid, a software library built on PyTorch that allows fast development and end-to-end training and evaluation of deep re-ID models. As a general-purpose framework for person re-ID research, Torchreid provides (1) unified data loaders that support 15 commonly used re-ID benchmark datasets covering both image and video domains, (2) streamlined pipelines for quick development and benchmarking of deep re-ID models, and (3) implementations of the latest re-ID CNN architectures along with their pre-trained models to facilitate reproducibility as well as future research. With a high-level modularity in its design, Torchreid offers a great flexibility to allow easy extension to new datasets, CNN models and loss functions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Tech report},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\VVC7Y6T3\\Zhou und Xiang - 2019 - Torchreid A Library for Deep Learning Person Re-I.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\CV22WP6I\\1910.html}
}

@inproceedings{zhuApproximateOntologyReasoning2021,
  title = {Approximate {{Ontology Reasoning}} for {{Domain-Specific Knowledge Graph}} Based on {{Deep Learning}}},
  booktitle = {2021 7th {{International Conference}} on {{Big Data}} and {{Information Analytics}} ({{BigDIA}})},
  author = {Zhu, Xixi and Liu, Bin and Ding, Zhaoyun and Zhu, Cheng and Yao, Li},
  year = {2021},
  month = oct,
  pages = {172--179},
  doi = {10.1109/BigDIA53151.2021.9619694},
  abstract = {Ontology reasoning has great potential in application of domain-specific knowledge graph. However, the traditional ontology reasoner is difficult to perform rapidly reasoning on domain-specific knowledge graph as its complex logical expression and large-scale assertion set. On the other hand, the knowledge graph reasoning method based on presentation learning has better scalability, but it is hard to incorporate expert knowledge in the schema-level information effectively, so it can't implement complex reasoning. The above restricts the application of reasoning in domain-specific knowledge graph. In order to solve the problem, researchers try to combine ontology reasoning with deep learning to achieve approximate ontology reasoning. However, the state-of-the-art method has shortcomings of model design and evaluation which make it lose generalization ability. This paper proposes a novel method to transform ontology reasoning into a graph-to-graph relational mapping learning model, and realizes approximate ontology reasoning based on deep learning. Besides, this paper also defines new knowledge graph reasoning evaluation criterion, and synthesizes new test data. On this basis, relevant comparative experiments are carried out. The results show that the proposed method achieves good performance on LUBM synthetic dataset, even though the data mode that not appear in the training can also be inferred effectively, which can make the model applicable to a wider range of application scenarios.},
  keywords = {Approximate Ontology Reasoning,Big Data,Cognition,Deep learning,Deep Learning,Domain-specific Knowledge Graph,Knowledge Reasoning,Ontologies,Scalability,Training,Transforms},
  file = {C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\UKHRJ27E\\Zhu et al. - 2021 - Approximate Ontology Reasoning for Domain-Specific.pdf;C\:\\Users\\gebele\\LRZ Sync+Share\\Forschung\\Literaturverwaltung\\Zotero\\storage\\D7EUJY3T\\9619694.html}
}


